{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ab4b36ac7ef4acba9c9d63292daf3ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94515028ddc14639a075905852d1bc53",
              "IPY_MODEL_c1d7fc34d60a45f2ba5a049fde6dffa7",
              "IPY_MODEL_3dd7da60e5d24542b77ea2cfed6a9f44"
            ],
            "layout": "IPY_MODEL_49ed97b609b8413fa81e12cb0ecef740"
          }
        },
        "94515028ddc14639a075905852d1bc53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee9bfe366fc349a6b5eb5e4269557c6b",
            "placeholder": "​",
            "style": "IPY_MODEL_cdc847f498cb41da99404a88a47471e8",
            "value": "100%"
          }
        },
        "c1d7fc34d60a45f2ba5a049fde6dffa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84da082fa4cf419bbf3bc635e1f22ac5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_be08623fcfa64046bbc16ab5f42cbcac",
            "value": 1
          }
        },
        "3dd7da60e5d24542b77ea2cfed6a9f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b6d7d92d2984ba78caeddcba3841ec8",
            "placeholder": "​",
            "style": "IPY_MODEL_8fdde1941f9e47a0ae5a2f2a06ba53c1",
            "value": " 1/1 [00:04&lt;00:00,  4.33s/it]"
          }
        },
        "49ed97b609b8413fa81e12cb0ecef740": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee9bfe366fc349a6b5eb5e4269557c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc847f498cb41da99404a88a47471e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84da082fa4cf419bbf3bc635e1f22ac5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be08623fcfa64046bbc16ab5f42cbcac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b6d7d92d2984ba78caeddcba3841ec8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fdde1941f9e47a0ae5a2f2a06ba53c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2024c4050a864f568def28b83cd8dc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52862ce702ba445eb632a1b0099d244e",
              "IPY_MODEL_3ab4d4f91d6d49369321a979fed6aa2e",
              "IPY_MODEL_6f6064b52a114dc896b61a27852bd637"
            ],
            "layout": "IPY_MODEL_52826b0a472b4103be644e26613f5855"
          }
        },
        "52862ce702ba445eb632a1b0099d244e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91e02f226614436984bd0e878b2f03c7",
            "placeholder": "​",
            "style": "IPY_MODEL_3fb30f4a6de349dd86eb806dcb0e6577",
            "value": "100%"
          }
        },
        "3ab4d4f91d6d49369321a979fed6aa2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b84211ab9c7241a5b1283fd018944e4d",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd486dfca5d24a73be3fa034266bd7d8",
            "value": 15
          }
        },
        "6f6064b52a114dc896b61a27852bd637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6300381c167a4b62a3394e395c0ea099",
            "placeholder": "​",
            "style": "IPY_MODEL_6e8c7bf5b31d43319ac06c3e26468700",
            "value": " 15/15 [00:01&lt;00:00,  8.06it/s]"
          }
        },
        "52826b0a472b4103be644e26613f5855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91e02f226614436984bd0e878b2f03c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fb30f4a6de349dd86eb806dcb0e6577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b84211ab9c7241a5b1283fd018944e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd486dfca5d24a73be3fa034266bd7d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6300381c167a4b62a3394e395c0ea099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e8c7bf5b31d43319ac06c3e26468700": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7617e2754854406c93b89a0ccfe761e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_415b1ce3cbb244278c780e7049e3bd1d",
              "IPY_MODEL_3e46b93014ae47e79ae3c143fac23035",
              "IPY_MODEL_70b5beb037db47909725c4fe67c4b65a"
            ],
            "layout": "IPY_MODEL_ae825d5206f34149803e4f1e8a010b7c"
          }
        },
        "415b1ce3cbb244278c780e7049e3bd1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef6a96802f344423af3d552c82eff14b",
            "placeholder": "​",
            "style": "IPY_MODEL_55a8d7e66608429d9aa988c31737032d",
            "value": "100%"
          }
        },
        "3e46b93014ae47e79ae3c143fac23035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34de9f2a1a124c93826c771af7d560f5",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc41a3e332204eaeb3f92c608f683bfb",
            "value": 15
          }
        },
        "70b5beb037db47909725c4fe67c4b65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_316a608f2d534277a108ece0f2232534",
            "placeholder": "​",
            "style": "IPY_MODEL_0c6f781c738742b5b7f36fde2dea9236",
            "value": " 15/15 [00:02&lt;00:00,  6.94it/s]"
          }
        },
        "ae825d5206f34149803e4f1e8a010b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef6a96802f344423af3d552c82eff14b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55a8d7e66608429d9aa988c31737032d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34de9f2a1a124c93826c771af7d560f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc41a3e332204eaeb3f92c608f683bfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "316a608f2d534277a108ece0f2232534": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c6f781c738742b5b7f36fde2dea9236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "970fdebd8c97472ea546cf6814565814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c25756e975b945459f2c7be10d20b52b",
              "IPY_MODEL_cbab6e083480469bab9ea0429f6684b7",
              "IPY_MODEL_b02163d07bd44e5c8d0a15dda6e2ee12"
            ],
            "layout": "IPY_MODEL_f45b21e5d5c14fbbafacc1cbc48c8ded"
          }
        },
        "c25756e975b945459f2c7be10d20b52b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27be3bcca20e45f7b2e4c37c1f462a72",
            "placeholder": "​",
            "style": "IPY_MODEL_cddadd5971c24697bc647ef13b196934",
            "value": "100%"
          }
        },
        "cbab6e083480469bab9ea0429f6684b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7bc8dcc079d645bc83ebad8fe5b8405b",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_578eff3e1f214ce49f9b5f5f33c39c0d",
            "value": 20
          }
        },
        "b02163d07bd44e5c8d0a15dda6e2ee12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0520f5422e264a459d6c9d272d828426",
            "placeholder": "​",
            "style": "IPY_MODEL_6f3062f0431847ee90ece49acda1886f",
            "value": " 20/20 [00:01&lt;00:00, 17.60it/s]"
          }
        },
        "f45b21e5d5c14fbbafacc1cbc48c8ded": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27be3bcca20e45f7b2e4c37c1f462a72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cddadd5971c24697bc647ef13b196934": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bc8dcc079d645bc83ebad8fe5b8405b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578eff3e1f214ce49f9b5f5f33c39c0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0520f5422e264a459d6c9d272d828426": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f3062f0431847ee90ece49acda1886f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc14693a844a4bf0b2b3c4479d1654a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_af35d405064d43d1b27edfd96b07a5a0",
              "IPY_MODEL_e07a5ca0357f43b0a51e169fbe807aa6",
              "IPY_MODEL_68c36f6960354ba9954733ec123b816b"
            ],
            "layout": "IPY_MODEL_2bfa71b6ce6f4c5d80f1550a6a8e22a3"
          }
        },
        "af35d405064d43d1b27edfd96b07a5a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5cc7c6798e3247168a8d9580eab5f431",
            "placeholder": "​",
            "style": "IPY_MODEL_20e80f0cb572480cafb51da8eafed5c2",
            "value": "100%"
          }
        },
        "e07a5ca0357f43b0a51e169fbe807aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d385d264f724374be582d0109f7ca69",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66854292d1bf4f768c2fbf6199b2593f",
            "value": 20
          }
        },
        "68c36f6960354ba9954733ec123b816b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b09e5c508a2d46e99a9450c933190da8",
            "placeholder": "​",
            "style": "IPY_MODEL_c5f2f973ee844d5b8de67f3473f067c2",
            "value": " 20/20 [00:01&lt;00:00, 13.59it/s]"
          }
        },
        "2bfa71b6ce6f4c5d80f1550a6a8e22a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cc7c6798e3247168a8d9580eab5f431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20e80f0cb572480cafb51da8eafed5c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d385d264f724374be582d0109f7ca69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66854292d1bf4f768c2fbf6199b2593f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b09e5c508a2d46e99a9450c933190da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f2f973ee844d5b8de67f3473f067c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJk9JtVzY-p0",
        "outputId": "0010389b-f56c-469e-e5bb-359d1008cf6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEbRmKd6aw0V",
        "outputId": "04921829-663a-4690-ddf4-757f391d1e82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/My Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "id": "SSzYNID1bZpd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2de01b03-0bfe-4578-ec39-fb6fe805bbdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m204.8/275.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.11.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.19.3)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.2.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.2.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (1.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (23.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283835 sha256=2d3383cc445d7aad6f6b302a8caa3be150feb812850bc60286783891311fb03d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap"
      ],
      "metadata": {
        "id": "hn_uTXvkbiHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35821f63-22d6-4bc8-b182-a9411ae64619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.44.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/535.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.5/535.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.7/535.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.44.1 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dice_ml"
      ],
      "metadata": {
        "id": "bUDMS6WEboyf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cce129e-d120-45c9-f541-fad6ae7113a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dice_ml\n",
            "  Downloading dice_ml-0.11-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from dice_ml) (4.19.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dice_ml) (1.25.2)\n",
            "Requirement already satisfied: pandas<2.0.0 in /usr/local/lib/python3.10/dist-packages (from dice_ml) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from dice_ml) (1.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dice_ml) (4.66.2)\n",
            "Collecting raiutils>=0.4.0 (from dice_ml)\n",
            "  Downloading raiutils-0.4.1-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0->dice_ml) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.0.0->dice_ml) (2023.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from raiutils>=0.4.0->dice_ml) (2.31.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from raiutils>=0.4.0->dice_ml) (1.11.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->dice_ml) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->dice_ml) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->dice_ml) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->dice_ml) (0.18.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->dice_ml) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->dice_ml) (3.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2.0.0->dice_ml) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->raiutils>=0.4.0->dice_ml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->raiutils>=0.4.0->dice_ml) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->raiutils>=0.4.0->dice_ml) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->raiutils>=0.4.0->dice_ml) (2024.2.2)\n",
            "Installing collected packages: raiutils, dice_ml\n",
            "Successfully installed dice_ml-0.11 raiutils-0.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install interpret"
      ],
      "metadata": {
        "id": "JiZvRVAGb2YV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72137b68-4aeb-4242-9fbf-6dd58cc608f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting interpret\n",
            "  Downloading interpret-0.5.1-py3-none-any.whl (1.4 kB)\n",
            "Collecting interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1 (from interpret)\n",
            "  Downloading interpret_core-0.5.1-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (1.11.4)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (1.5.3)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (1.3.2)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (5.9.5)\n",
            "Collecting dash>=1.0.0 (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading dash-2.15.0-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dash-core-components>=1.0.0 (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-html-components>=1.0.0 (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-table>=4.1.0 (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Collecting dash-cytoscape>=0.1.1 (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading dash_cytoscape-1.0.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gevent>=1.3.6 (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading gevent-24.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2.31.0)\n",
            "Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.44.1)\n",
            "Collecting dill>=0.2.5 (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (5.5.6)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (7.34.0)\n",
            "Collecting SALib>=1.3.3 (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading salib-1.4.8-py3-none-any.whl (778 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.7/778.7 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (5.15.0)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (4.9.0)\n",
            "Collecting retrying (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (67.7.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (7.0.1)\n",
            "Collecting zope.event (from gevent>=1.3.6->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface (from gevent>=1.3.6->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading zope.interface-6.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent>=1.3.6->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.0.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (6.3.2)\n",
            "Collecting jedi>=0.16 (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19.2->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2023.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=3.8.1->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=3.8.1->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2024.2.2)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.7.1)\n",
            "Collecting multiprocess (from SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.1->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap>=0.28.5->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (4.66.2)\n",
            "Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.10/dist-packages (from shap>=0.28.5->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.0.7)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap>=0.28.5->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap>=0.28.5->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2.2.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (8.1.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.8.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->SALib>=1.3.3->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.1.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=0.19.2->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug<3.1->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (2.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=1.0.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (3.17.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (5.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client->ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (23.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap>=0.28.5->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (0.41.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client->ipykernel>=4.10.0->interpret-core[dash,debug,linear,notebook,plotly,sensitivity,shap]==0.5.1->interpret) (4.2.0)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, zope.interface, zope.event, retrying, jedi, dill, multiprocess, gevent, SALib, interpret-core, dash, dash-cytoscape, interpret\n",
            "Successfully installed SALib-1.4.8 dash-2.15.0 dash-core-components-2.0.0 dash-cytoscape-1.0.0 dash-html-components-2.0.0 dash-table-5.0.0 dill-0.3.8 gevent-24.2.1 interpret-0.5.1 interpret-core-0.5.1 jedi-0.19.1 multiprocess-0.70.16 retrying-1.3.4 zope.event-5.0 zope.interface-6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import lime\n",
        "import shap\n",
        "import dice_ml\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from IPython.display import display\n",
        "from lime import lime_tabular\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from interpret.blackbox import LimeTabular\n",
        "from interpret import show\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "fCWJ_Ltnb-3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file with the specified delimiter\n",
        "data = pd.read_csv('bank-additional-full.csv', delimiter=';')"
      ],
      "metadata": {
        "id": "6UZqKJpEccSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "i8OrZUhpcgn0",
        "outputId": "3ba64809-ab75-4587-b2c8-2c1b611c62dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   age          job  marital            education  default housing loan  \\\n",
              "0   56    housemaid  married             basic.4y       no      no   no   \n",
              "1   57     services  married          high.school  unknown      no   no   \n",
              "2   37     services  married          high.school       no     yes   no   \n",
              "3   40       admin.  married             basic.6y       no      no   no   \n",
              "4   56     services  married          high.school       no      no  yes   \n",
              "5   45     services  married             basic.9y  unknown      no   no   \n",
              "6   59       admin.  married  professional.course       no      no   no   \n",
              "7   41  blue-collar  married              unknown  unknown      no   no   \n",
              "8   24   technician   single  professional.course       no     yes   no   \n",
              "9   25     services   single          high.school       no     yes   no   \n",
              "\n",
              "     contact month day_of_week  ...  campaign  pdays  previous     poutcome  \\\n",
              "0  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "1  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "2  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "3  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "4  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "5  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "6  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "7  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "8  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "9  telephone   may         mon  ...         1    999         0  nonexistent   \n",
              "\n",
              "  emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
              "0          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "1          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "2          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "3          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "4          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "5          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "6          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "7          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "8          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "9          1.1          93.994          -36.4      4.857       5191.0  no  \n",
              "\n",
              "[10 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d2dd49a-5685-41ba-87c0-00d489c37333\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>month</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>...</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>emp.var.rate</th>\n",
              "      <th>cons.price.idx</th>\n",
              "      <th>cons.conf.idx</th>\n",
              "      <th>euribor3m</th>\n",
              "      <th>nr.employed</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56</td>\n",
              "      <td>housemaid</td>\n",
              "      <td>married</td>\n",
              "      <td>basic.4y</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>57</td>\n",
              "      <td>services</td>\n",
              "      <td>married</td>\n",
              "      <td>high.school</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37</td>\n",
              "      <td>services</td>\n",
              "      <td>married</td>\n",
              "      <td>high.school</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>40</td>\n",
              "      <td>admin.</td>\n",
              "      <td>married</td>\n",
              "      <td>basic.6y</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56</td>\n",
              "      <td>services</td>\n",
              "      <td>married</td>\n",
              "      <td>high.school</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>45</td>\n",
              "      <td>services</td>\n",
              "      <td>married</td>\n",
              "      <td>basic.9y</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>59</td>\n",
              "      <td>admin.</td>\n",
              "      <td>married</td>\n",
              "      <td>professional.course</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>41</td>\n",
              "      <td>blue-collar</td>\n",
              "      <td>married</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>24</td>\n",
              "      <td>technician</td>\n",
              "      <td>single</td>\n",
              "      <td>professional.course</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>25</td>\n",
              "      <td>services</td>\n",
              "      <td>single</td>\n",
              "      <td>high.school</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>telephone</td>\n",
              "      <td>may</td>\n",
              "      <td>mon</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.1</td>\n",
              "      <td>93.994</td>\n",
              "      <td>-36.4</td>\n",
              "      <td>4.857</td>\n",
              "      <td>5191.0</td>\n",
              "      <td>no</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d2dd49a-5685-41ba-87c0-00d489c37333')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3d2dd49a-5685-41ba-87c0-00d489c37333 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3d2dd49a-5685-41ba-87c0-00d489c37333');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-60c48080-4390-443c-9891-a03508391bea\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-60c48080-4390-443c-9891-a03508391bea')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-60c48080-4390-443c-9891-a03508391bea button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop any rows with missing values\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "O_N1DaEZckHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features (X) and target (y)\n",
        "x = data.drop('y', axis=1)  # Features\n",
        "y = data['y']  # Target"
      ],
      "metadata": {
        "id": "ipPicMFccoED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the target column to numerical values\n",
        "y = y.map({'no': 0, 'yes': 1})"
      ],
      "metadata": {
        "id": "DkSumlkWcrZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical variables to numerical using one-hot encoding\n",
        "categorical_columns = list(x.select_dtypes(include=\"object\").columns)\n",
        "# x = pd.get_dummies(x, columns=categorical_columns)"
      ],
      "metadata": {
        "id": "wL5a_yQudRmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the numerical columns using StandardScaler\n",
        "numerical_columns = list(x.select_dtypes(exclude=\"object\").columns)"
      ],
      "metadata": {
        "id": "n5Gwm-eccvXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform oversampling on the features (x) and target (y)\n",
        "oversampler = RandomOverSampler()\n",
        "x, y = oversampler.fit_resample(x, y)"
      ],
      "metadata": {
        "id": "G1pNqhmOc3OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into train and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "7iiXqhJ2c6u3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for numericals\n",
        "#The first step is an imputer that fills missing values in numerical columns using the mean strategy.\n",
        "#The SimpleImputer class from scikit-learn is used for this purpose.\n",
        "#The second step is a scaler that standardizes the numerical columns by subtracting the mean and dividing by the standard deviation.\n",
        "#The StandardScaler class from scikit-learn is used for this purpose.\n",
        "# for categorical\n",
        "# The first step is an imputer that fills missing values in categorical columns using the most frequent strategy.\n",
        "#The SimpleImputer class is used, similar to the numerical transformer.\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "encoder = ColumnTransformer([(\"numerical\", numerical_transformer, numerical_columns),\n",
        "                             (\"categorical\", categorical_transformer, categorical_columns)])"
      ],
      "metadata": {
        "id": "P1Kk5qbCc-xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Training a Deep Neural Network\n",
        "\n",
        "mlp_model = Pipeline(\n",
        "    [\n",
        "        (\"preprocessor\", encoder),\n",
        "        (\"model\", MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=200))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "mlp_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_mlp = mlp_model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
        "precision_mlp = precision_score(y_test, y_pred_mlp)\n",
        "recall_mlp = recall_score(y_test, y_pred_mlp)\n",
        "f1_mlp = f1_score(y_test, y_pred_mlp)"
      ],
      "metadata": {
        "id": "8kXyGIU0dVNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "281379ca-86bc-4d8e-881e-808847e59846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Training a Random Forest Classifier\n",
        "\n",
        "rf_model = Pipeline(\n",
        "    [\n",
        "        (\"preprocessor\", encoder),\n",
        "        (\"model\", RandomForestClassifier(n_estimators=100))\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_rf = rf_model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_rf = precision_score(y_test, y_pred_rf)\n",
        "recall_rf = recall_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf)"
      ],
      "metadata": {
        "id": "aqU509dudt8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Training an SVM Classifier\n",
        "\n",
        "svm_model = Pipeline(\n",
        "    [\n",
        "        (\"preprocessor\", encoder),\n",
        "        (\"model\", svm.SVC(probability=True))\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "svm_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "precision_svm = precision_score(y_test, y_pred_svm)\n",
        "recall_svm = recall_score(y_test, y_pred_svm)\n",
        "f1_svm = f1_score(y_test, y_pred_svm)"
      ],
      "metadata": {
        "id": "L9GIU8PvgAsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the evaluation metrics for each model\n",
        "print(\"Deep Neural Network:\")\n",
        "print(\"Accuracy:\", accuracy_mlp)\n",
        "print(\"Precision:\", precision_mlp)\n",
        "print(\"Recall:\", recall_mlp)\n",
        "print(\"F1 Score:\", f1_mlp)\n",
        "print()\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(\"Accuracy:\", accuracy_rf)\n",
        "print(\"Precision:\", precision_rf)\n",
        "print(\"Recall:\", recall_rf)\n",
        "print(\"F1 Score:\", f1_rf)\n",
        "print()\n",
        "\n",
        "print(\"SVM:\")\n",
        "print(\"Accuracy:\", accuracy_svm)\n",
        "print(\"Precision:\", precision_svm)\n",
        "print(\"Recall:\", recall_svm)\n",
        "print(\"F1 Score:\", f1_svm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7elMuI9lj26",
        "outputId": "1af205fe-23e8-4d40-f471-7aa1f3d9fac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Neural Network:\n",
            "Accuracy: 0.9600547195622435\n",
            "Precision: 0.9276601173768818\n",
            "Recall: 0.9976673984632273\n",
            "F1 Score: 0.9613909824143859\n",
            "\n",
            "Random Forest:\n",
            "Accuracy: 0.9705882352941176\n",
            "Precision: 0.944286084477844\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9713447954151673\n",
            "\n",
            "SVM:\n",
            "Accuracy: 0.9002735978112175\n",
            "Precision: 0.8621118012422361\n",
            "Recall: 0.9522502744237102\n",
            "F1 Score: 0.9049419741817707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counterfactual Explanations"
      ],
      "metadata": {
        "id": "Zw9BENoEl6Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CounterfactualExplainer:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.df = data\n",
        "        self.X_train, self.X_test = x_train, x_test\n",
        "        self.cat_features, self.num_features = categorical_columns, numerical_columns\n",
        "        self.preprocessor = self.model.named_steps[\"preprocessor\"]\n",
        "        self.ohe_categories = self.preprocessor.named_transformers_[\"categorical\"][\"onehot\"].categories_\n",
        "        self.new_ohe_features = [f\"{col}__{val}\" for col, vals in zip(self.cat_features, self.ohe_categories) for val in\n",
        "                                 vals]\n",
        "        self.all_features = self.num_features + self.new_ohe_features\n",
        "\n",
        "    def cf_explain(self, sample):\n",
        "        data_dice = dice_ml.Data(dataframe=self.df,\n",
        "                                 continuous_features=self.num_features,\n",
        "                                 outcome_name='y')\n",
        "        model_dice = dice_ml.Model(model=self.model,\n",
        "                                   backend=\"sklearn\")\n",
        "        explainer = dice_ml.Dice(data_dice,\n",
        "                                 model_dice,\n",
        "                                 method=\"random\")\n",
        "        feature_weights = {'age': 10}\n",
        "        dice_exp = explainer.generate_counterfactuals(sample, total_CFs=4, desired_class=\"opposite\",\n",
        "                                                      proximity_weight=feature_weights)\n",
        "        json_data = dice_exp.to_json()\n",
        "        counterfactual_samples = self.generate_counterfactuals(json_data)\n",
        "        cf_text = \"\"\n",
        "        for index, sample in enumerate(counterfactual_samples):\n",
        "            cf_text += f\"\\nCounterfactual Result {index + 1}:\\n\" + sample + \"\\n\"\n",
        "\n",
        "\n",
        "        return cf_text\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_counterfactuals(json_data, precision=2):\n",
        "        data = json.loads(json_data)\n",
        "\n",
        "        test_data = data['test_data'][0][0]\n",
        "        cfs_list = data['cfs_list'][0]\n",
        "\n",
        "        feature_names = data['feature_names']\n",
        "\n",
        "        counterfactuals = []\n",
        "\n",
        "        for cf in cfs_list:\n",
        "            counterfactual_text = \"\"\n",
        "            for i, (original_value, cf_value) in enumerate(zip(test_data, cf)):\n",
        "                if i < len(feature_names):\n",
        "                    feature_name = feature_names[i]\n",
        "                    if isinstance(original_value, float) and isinstance(cf_value, float):\n",
        "                        original_value = round(original_value, precision)\n",
        "                        cf_value = round(cf_value, precision)\n",
        "                    if original_value != cf_value:\n",
        "                        counterfactual_text += f\"   {feature_name}: {original_value} -> {cf_value}\\n\"\n",
        "\n",
        "            counterfactuals.append(counterfactual_text)\n",
        "\n",
        "        return counterfactuals\n",
        "\n",
        "def cf_explanation(sample, model):\n",
        "    cf_explainer = CounterfactualExplainer(model)\n",
        "    return cf_explainer.cf_explain(sample)"
      ],
      "metadata": {
        "id": "xRQkAMTylnd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.iloc[[67], :]"
      ],
      "metadata": {
        "id": "HcdQ65DMmwsT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "51cb08d7-341a-497a-8667-7b47b133909d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       age     job marital    education default housing loan   contact month  \\\n",
              "72788   24  admin.  single  high.school      no     yes   no  cellular   jul   \n",
              "\n",
              "      day_of_week  duration  campaign  pdays  previous     poutcome  \\\n",
              "72788         thu       725         3    999         0  nonexistent   \n",
              "\n",
              "       emp.var.rate  cons.price.idx  cons.conf.idx  euribor3m  nr.employed  \n",
              "72788           1.4          93.918          -42.7      4.968       5228.1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b170fb84-93f9-4720-a69f-aade157d75d6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>job</th>\n",
              "      <th>marital</th>\n",
              "      <th>education</th>\n",
              "      <th>default</th>\n",
              "      <th>housing</th>\n",
              "      <th>loan</th>\n",
              "      <th>contact</th>\n",
              "      <th>month</th>\n",
              "      <th>day_of_week</th>\n",
              "      <th>duration</th>\n",
              "      <th>campaign</th>\n",
              "      <th>pdays</th>\n",
              "      <th>previous</th>\n",
              "      <th>poutcome</th>\n",
              "      <th>emp.var.rate</th>\n",
              "      <th>cons.price.idx</th>\n",
              "      <th>cons.conf.idx</th>\n",
              "      <th>euribor3m</th>\n",
              "      <th>nr.employed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>72788</th>\n",
              "      <td>24</td>\n",
              "      <td>admin.</td>\n",
              "      <td>single</td>\n",
              "      <td>high.school</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>cellular</td>\n",
              "      <td>jul</td>\n",
              "      <td>thu</td>\n",
              "      <td>725</td>\n",
              "      <td>3</td>\n",
              "      <td>999</td>\n",
              "      <td>0</td>\n",
              "      <td>nonexistent</td>\n",
              "      <td>1.4</td>\n",
              "      <td>93.918</td>\n",
              "      <td>-42.7</td>\n",
              "      <td>4.968</td>\n",
              "      <td>5228.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b170fb84-93f9-4720-a69f-aade157d75d6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b170fb84-93f9-4720-a69f-aade157d75d6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b170fb84-93f9-4720-a69f-aade157d75d6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"x_test\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 24,\n        \"max\": 24,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          24\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"job\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"admin.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"marital\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"single\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"education\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"high.school\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"default\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"no\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"housing\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loan\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"no\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"contact\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"cellular\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"month\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"jul\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"day_of_week\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"thu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"duration\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 725,\n        \"max\": 725,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          725\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"campaign\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pdays\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 999,\n        \"max\": 999,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"previous\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"poutcome\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"nonexistent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emp.var.rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.4,\n        \"max\": 1.4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cons.price.idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 93.918,\n        \"max\": 93.918,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          93.918\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cons.conf.idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": -42.7,\n        \"max\": -42.7,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          -42.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"euribor3m\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 4.968,\n        \"max\": 4.968,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4.968\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"nr.employed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 5228.1,\n        \"max\": 5228.1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5228.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train2 = x_train.copy()\n",
        "x_test2 = x_test.copy()\n",
        "y_train2 = y_train.copy()\n",
        "y_test2 = y_test.copy()"
      ],
      "metadata": {
        "id": "cy39aEGMm0LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical columns\n",
        "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']"
      ],
      "metadata": {
        "id": "TjBun5I8nWHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply label encoding to categorical columns\n",
        "label_encoder = LabelEncoder()\n",
        "for feature in categorical_cols:\n",
        "    x_train2[feature] = label_encoder.fit_transform(x_train2[feature])\n",
        "    x_test2[feature] = label_encoder.transform(x_test2[feature])"
      ],
      "metadata": {
        "id": "nFtczOSqnZdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_model2 = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "mlp_model2.fit(x_train2, y_train2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "4Rd2AsDancuV",
        "outputId": "76ec207e-bafe-4cdc-f8a4-5355ae5e1fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(100, 50))"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(100, 50))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(100, 50))</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rf_model2 = RandomForestClassifier(n_estimators=100)\n",
        "\n",
        "# Train the model\n",
        "rf_model2.fit(x_train2, y_train2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "dU8oI8kPoYO4",
        "outputId": "dd4caea9-1e9c-4392-8ddd-0166d17181e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm_model2 = svm.SVC(probability=True)\n",
        "\n",
        "# Train the model\n",
        "svm_model2.fit(x_train2, y_train2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "kClRIOBTr3gR",
        "outputId": "f8a33441-2d46-4101-c7ba-ed91a2ca6978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(probability=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Interface for LIME, Local SHAP and Counterfactual Explanations"
      ],
      "metadata": {
        "id": "WLc9HEHDPVVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Machine Learning Model Explanation Interface')\n",
        "\n",
        "# Select test data\n",
        "n = int(input('Select Index of Test Data:'))\n",
        "\n",
        "print(x_test.iloc[n])\n",
        "\n",
        "# Select model\n",
        "selected_model = input('Select Model (MLP/RF/SVM):')\n",
        "\n",
        "# Get the selected model\n",
        "if selected_model == 'MLP':\n",
        "    model = mlp_model\n",
        "    model2 = mlp_model2\n",
        "elif selected_model == 'RF':\n",
        "    model = rf_model\n",
        "    model2 = rf_model2\n",
        "else:\n",
        "    model = svm_model\n",
        "    model2 = svm_model2\n",
        "\n",
        "# Explain using Lime\n",
        "lime = LimeTabular(model2, data=x_train2,random_state=1)\n",
        "lime_local = lime.explain_local(x_test2.iloc[n:n+1],y_test.iloc[n:n+1],name='LIME')\n",
        "show(lime_local)\n",
        "\n",
        "# Explain using SHAP\n",
        "explainer = shap.KernelExplainer(model2.predict_proba, x_train2[:100])\n",
        "shap_values = explainer.shap_values(x_test2.iloc[n].values.reshape(1, -1))\n",
        "shap.force_plot(explainer.expected_value[0], shap_values[0], x_test2.iloc[n].values.reshape(1, -1), feature_names=x_test2.columns, matplotlib=True)\n",
        "\n",
        "# Explain using countefactual\n",
        "print(cf_explanation(x_test.iloc[[n], :], model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6ab4b36ac7ef4acba9c9d63292daf3ce",
            "94515028ddc14639a075905852d1bc53",
            "c1d7fc34d60a45f2ba5a049fde6dffa7",
            "3dd7da60e5d24542b77ea2cfed6a9f44",
            "49ed97b609b8413fa81e12cb0ecef740",
            "ee9bfe366fc349a6b5eb5e4269557c6b",
            "cdc847f498cb41da99404a88a47471e8",
            "84da082fa4cf419bbf3bc635e1f22ac5",
            "be08623fcfa64046bbc16ab5f42cbcac",
            "9b6d7d92d2984ba78caeddcba3841ec8",
            "8fdde1941f9e47a0ae5a2f2a06ba53c1"
          ]
        },
        "id": "shiOYHLHvjSx",
        "outputId": "b20c2153-4b1a-4dc3-b30d-e29cab332c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Machine Learning Model Explanation Interface\n",
            "Select Index of Test Data:27\n",
            "age                        37\n",
            "job               blue-collar\n",
            "marital               married\n",
            "education         high.school\n",
            "default               unknown\n",
            "housing                   yes\n",
            "loan                       no\n",
            "contact              cellular\n",
            "month                     may\n",
            "day_of_week               fri\n",
            "duration                  181\n",
            "campaign                    3\n",
            "pdays                     999\n",
            "previous                    0\n",
            "poutcome          nonexistent\n",
            "emp.var.rate             -1.8\n",
            "cons.price.idx         92.893\n",
            "cons.conf.idx           -46.2\n",
            "euribor3m               1.313\n",
            "nr.employed            5099.1\n",
            "Name: 32114, dtype: object\n",
            "Select Model (MLP/RF/SVM):MLP\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <script type=\"text/javascript\" src=\"https://unpkg.com/@interpretml/interpret-inline@0.5.1/dist/interpret-inline.js\"></script>\n",
              "        \n",
              "    <div id=\"_interpret-viz-20c78721-af1a-44e7-adc4-dd315c28a817\"></div>\n",
              "    <script defer type=\"text/javascript\">\n",
              "\n",
              "    (function universalLoad(root, callback) {\n",
              "      if(typeof exports === 'object' && typeof module === 'object') {\n",
              "        // CommonJS2\n",
              "        console.log(\"CommonJS2\");\n",
              "        var interpretInline = require('interpret-inline');\n",
              "        callback(interpretInline);\n",
              "      } else if(typeof define === 'function' && define.amd) {\n",
              "        // AMD\n",
              "        console.log(\"AMD\");\n",
              "        require(['interpret-inline'], function(interpretInline) {\n",
              "          callback(interpretInline);\n",
              "        });\n",
              "      } else if(typeof exports === 'object') {\n",
              "        // CommonJS\n",
              "        console.log(\"CommonJS\");\n",
              "        var interpretInline = require('interpret-inline');\n",
              "        callback(interpretInline);\n",
              "      } else {\n",
              "        // Browser\n",
              "        console.log(\"Browser\");\n",
              "        callback(root['interpret-inline']);\n",
              "      }\n",
              "    })(this, function(interpretInline) {\n",
              "        interpretInline.RenderApp(\"_interpret-viz-20c78721-af1a-44e7-adc4-dd315c28a817\", {\"name\": \"LIME\", \"overall\": {\"type\": \"none\", \"figure\": \"null\", \"help\": {}}, \"specific\": [{\"type\": \"plotly\", \"figure\": {\"data\": [{\"marker\": {\"color\": [\"#1f77b4\", \"#1f77b4\", \"#1f77b4\", \"#1f77b4\", \"#ff7f0e\", \"#ff7f0e\", \"#1f77b4\", \"#1f77b4\", \"#ff7f0e\", \"#1f77b4\", \"#808080\"]}, \"orientation\": \"h\", \"x\": [-0.020496953133739265, -0.024318362471895724, -0.024721029661093453, -0.02594189832570633, 0.03269825380664991, 0.04475714112116643, -0.04947897202021646, -0.07282683407287915, 0.16818191633693794, -0.2556846900326324, 0.412753002309128], \"y\": [\"cons.price.idx (92.89)\", \"marital (1.00)\", \"day_of_week (0.00)\", \"month (6.00)\", \"contact (0.00)\", \"euribor3m (1.31)\", \"poutcome (1.00)\", \"default (1.00)\", \"emp.var.rate (-1.80)\", \"duration (181.00)\", \"Intercept\"], \"type\": \"bar\"}], \"layout\": {\"title\": {\"text\": \"Actual: 0 | Predicted: 0.124\"}, \"xaxis\": {\"range\": [-0.412753002309128, 0.412753002309128], \"title\": {\"text\": \"\"}}, \"yaxis\": {\"automargin\": true, \"dtick\": 1, \"title\": {\"text\": \"\"}}, \"template\": {\"data\": {\"histogram2dcontour\": [{\"type\": \"histogram2dcontour\", \"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}], \"choropleth\": [{\"type\": \"choropleth\", \"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}], \"histogram2d\": [{\"type\": \"histogram2d\", \"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}], \"heatmap\": [{\"type\": \"heatmap\", \"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}], \"heatmapgl\": [{\"type\": \"heatmapgl\", \"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}], \"contourcarpet\": [{\"type\": \"contourcarpet\", \"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}], \"contour\": [{\"type\": \"contour\", \"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}], \"surface\": [{\"type\": \"surface\", \"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}], \"mesh3d\": [{\"type\": \"mesh3d\", \"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}], \"scatter\": [{\"fillpattern\": {\"fillmode\": \"overlay\", \"size\": 10, \"solidity\": 0.2}, \"type\": \"scatter\"}], \"parcoords\": [{\"type\": \"parcoords\", \"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}}], \"scatterpolargl\": [{\"type\": \"scatterpolargl\", \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}}], \"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}, \"pattern\": {\"fillmode\": \"overlay\", \"size\": 10, \"solidity\": 0.2}}, \"type\": \"bar\"}], \"scattergeo\": [{\"type\": \"scattergeo\", \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}}], \"scatterpolar\": [{\"type\": \"scatterpolar\", \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}}], \"histogram\": [{\"marker\": {\"pattern\": {\"fillmode\": \"overlay\", \"size\": 10, \"solidity\": 0.2}}, \"type\": \"histogram\"}], \"scattergl\": [{\"type\": \"scattergl\", \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}}], \"scatter3d\": [{\"type\": \"scatter3d\", \"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}}], \"scattermapbox\": [{\"type\": \"scattermapbox\", \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}}], \"scatterternary\": [{\"type\": \"scatterternary\", \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}}], \"scattercarpet\": [{\"type\": \"scattercarpet\", \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}, \"pattern\": {\"fillmode\": \"overlay\", \"size\": 10, \"solidity\": 0.2}}, \"type\": \"barpolar\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}]}, \"layout\": {\"autotypenumbers\": \"strict\", \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"hovermode\": \"closest\", \"hoverlabel\": {\"align\": \"left\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"bgcolor\": \"#E5ECF6\", \"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"ternary\": {\"bgcolor\": \"#E5ECF6\", \"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]]}, \"xaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"automargin\": true, \"zerolinewidth\": 2}, \"yaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"automargin\": true, \"zerolinewidth\": 2}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\", \"gridwidth\": 2}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\", \"gridwidth\": 2}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\", \"gridwidth\": 2}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"geo\": {\"bgcolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"subunitcolor\": \"white\", \"showland\": true, \"showlakes\": true, \"lakecolor\": \"white\"}, \"title\": {\"x\": 0.05}, \"mapbox\": {\"style\": \"light\"}}}}}, \"help\": {}}], \"selector\": {\"columns\": [\"Actual\", \"Predicted\", \"Resid\", \"AbsResid\"], \"data\": [{\"Actual\": 0, \"Predicted\": 0.124, \"Resid\": -0.124, \"AbsResid\": 0.124}]}}, -1);\n",
              "    });\n",
              "\n",
              "    </script>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ab4b36ac7ef4acba9c9d63292daf3ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Counterfactual Result 1:\n",
            "   education: high.school -> basic.9y\n",
            "   previous: 0 -> 4\n",
            "\n",
            "\n",
            "Counterfactual Result 2:\n",
            "   duration: 181 -> 2514\n",
            "\n",
            "\n",
            "Counterfactual Result 3:\n",
            "   duration: 181 -> 2580\n",
            "\n",
            "\n",
            "Counterfactual Result 4:\n",
            "   duration: 181 -> 2368\n",
            "   cons.conf.idx: -46.2 -> -26.8\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Interface for Global SHAP based on the group of samples one after the other(Continuously)"
      ],
      "metadata": {
        "id": "rb9XM1Aegup2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interface\n",
        "print('Machine Learning Model Explanation Interface')\n",
        "\n",
        "# Select test data\n",
        "n = int(input('Select Index of Test Data:'))\n",
        "\n",
        "# Select model\n",
        "selected_model = input('Select Model (MLP/RF/SVM):')\n",
        "\n",
        "# Get the selected model\n",
        "if selected_model == 'MLP':\n",
        "    model2 = mlp_model2\n",
        "elif selected_model == 'RF':\n",
        "    model2 = rf_model2\n",
        "else:\n",
        "    model2 = svm_model2\n",
        "\n",
        "\n",
        "\n",
        "explainer = shap.KernelExplainer(model2.predict_proba, x_train2[:n])\n",
        "# Compute SHAP values for global explanation for class 0\n",
        "shap_values_global_class0 = explainer.shap_values(x_train2[:n], nsamples=n)\n",
        "\n",
        "# Calculate the mean absolute SHAP values across all instances for class 0\n",
        "mean_shap_values_class0 = np.abs(shap_values_global_class0).mean(axis=0)\n",
        "\n",
        "# Compute SHAP values for global explanation for class 1\n",
        "shap_values_global_class1 = explainer.shap_values(x_train2[:n], nsamples=n, output_inds=1)\n",
        "\n",
        "# Calculate the mean absolute SHAP values across all instances for class 1\n",
        "mean_shap_values_class1 = np.abs(shap_values_global_class1).mean(axis=0)\n",
        "\n",
        "# Plot the global feature importance for both classes using a bar chart\n",
        "shap.summary_plot([mean_shap_values_class0, mean_shap_values_class1],\n",
        "                  x_train2[:n],\n",
        "                  feature_names=x_test2.columns,\n",
        "                  class_names=['Class 0', 'Class 1'],\n",
        "                  plot_type=\"bar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2024c4050a864f568def28b83cd8dc7b",
            "52862ce702ba445eb632a1b0099d244e",
            "3ab4d4f91d6d49369321a979fed6aa2e",
            "6f6064b52a114dc896b61a27852bd637",
            "52826b0a472b4103be644e26613f5855",
            "91e02f226614436984bd0e878b2f03c7",
            "3fb30f4a6de349dd86eb806dcb0e6577",
            "b84211ab9c7241a5b1283fd018944e4d",
            "cd486dfca5d24a73be3fa034266bd7d8",
            "6300381c167a4b62a3394e395c0ea099",
            "6e8c7bf5b31d43319ac06c3e26468700",
            "7617e2754854406c93b89a0ccfe761e6",
            "415b1ce3cbb244278c780e7049e3bd1d",
            "3e46b93014ae47e79ae3c143fac23035",
            "70b5beb037db47909725c4fe67c4b65a",
            "ae825d5206f34149803e4f1e8a010b7c",
            "ef6a96802f344423af3d552c82eff14b",
            "55a8d7e66608429d9aa988c31737032d",
            "34de9f2a1a124c93826c771af7d560f5",
            "fc41a3e332204eaeb3f92c608f683bfb",
            "316a608f2d534277a108ece0f2232534",
            "0c6f781c738742b5b7f36fde2dea9236"
          ]
        },
        "id": "mDqfW1ydjerr",
        "outputId": "612c34e3-f794-42d1-bbc9-82ebc4d6a2a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Machine Learning Model Explanation Interface\n",
            "Select Index of Test Data:15\n",
            "Select Model (MLP/RF/SVM):RF\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2024c4050a864f568def28b83cd8dc7b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.854e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.518e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.518e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.508e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=3.518e-02, previous alpha=3.470e-02, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.854e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.518e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.518e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.508e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=3.518e-02, previous alpha=3.470e-02, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.047e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.047e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.523e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.523e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.408e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.293e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.095e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=4.523e-02, previous alpha=3.525e-02, with an active set of 5 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.047e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=9.047e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.523e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.523e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.408e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.293e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.095e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=4.523e-02, previous alpha=3.525e-02, with an active set of 5 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.035e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.000e-02, previous alpha=9.477e-03, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.259e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=8.472e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=5.903e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=5.672e-03, previous alpha=5.472e-03, with an active set of 13 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.823e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=3.438e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.719e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.716e-02, previous alpha=1.716e-02, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=6.823e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.241e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.241e-03, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=4.514e-03, previous alpha=4.514e-03, with an active set of 10 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.052e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.026e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.026e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.026e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=3.026e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=3.026e-04, previous alpha=3.026e-04, with an active set of 10 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.052e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.026e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=3.026e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=3.026e-04, previous alpha=3.026e-04, with an active set of 12 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.687e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.957e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.823e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.823e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.323e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.306e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.306e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.284e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.253e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.769e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.922e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.816e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.849e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.687e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.957e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.823e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.823e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.323e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.306e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.306e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.284e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.253e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=5.769e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.922e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=3.816e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.849e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.539e-01, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.696e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=6.073e-02, previous alpha=6.034e-02, with an active set of 5 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=1.539e-01, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.696e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=6.073e-02, previous alpha=6.034e-02, with an active set of 5 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.501e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=1.501e-02, previous alpha=1.499e-02, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.184e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=8.184e-04, previous alpha=8.160e-04, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.186e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=6.186e-04, previous alpha=6.168e-04, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.765e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.823e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=8.823e-03, previous alpha=8.793e-03, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.765e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.823e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=8.823e-03, previous alpha=8.793e-03, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.088e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.500e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.483e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.451e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.130e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.481e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.516e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.377e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.145e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.750e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=9.130e-03, previous alpha=3.279e-03, with an active set of 10 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.088e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.500e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.483e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.451e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.130e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.481e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.516e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.377e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=5.145e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.750e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=8.089e-03, previous alpha=3.279e-03, with an active set of 10 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.131e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.654e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.844e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.791e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.238e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=3.400e-02, previous alpha=1.711e-02, with an active set of 10 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.131e-01, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.654e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.844e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=2.791e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=2.238e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=2.788e-02, previous alpha=1.711e-02, with an active set of 10 regressors.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7617e2754854406c93b89a0ccfe761e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.744e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.872e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.872e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.841e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.623e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=2.702e-02, previous alpha=2.623e-02, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=7.744e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.872e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.872e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.841e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.623e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=2.702e-02, previous alpha=2.623e-02, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.475e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.639e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.737e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.737e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.869e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.869e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.476e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.448e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.201e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.035e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.006e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.807e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.726e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.717e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.639e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.564e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=9.496e-03, previous alpha=8.209e-03, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.726e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.717e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.639e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.564e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=9.235e-03, previous alpha=8.209e-03, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.326e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.163e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.163e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.081e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.076e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=1.034e-02, previous alpha=1.034e-02, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.326e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.163e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.163e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.081e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.076e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=1.034e-02, previous alpha=1.034e-02, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.618e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.782e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.809e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.809e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.391e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.391e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=1.392e-02, previous alpha=1.387e-02, with an active set of 7 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=3.618e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.782e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.809e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.809e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.391e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.391e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=1.392e-02, previous alpha=1.387e-02, with an active set of 7 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=5.122e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.561e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.561e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.281e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.204e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.192e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.131e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.085e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.071e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.067e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.023e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.232e-02, previous alpha=5.793e-04, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=5.122e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.561e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.561e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.281e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.204e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.192e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.131e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.085e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.071e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=1.067e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=9.023e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=1.232e-02, previous alpha=5.793e-04, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.125e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.072e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.311e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.993e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=2.667e-03, previous alpha=1.700e-03, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.125e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=5.072e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.311e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.993e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=2.667e-03, previous alpha=1.700e-03, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.015e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.015e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.008e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.008e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.831e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.827e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=3.003e-02, previous alpha=2.803e-02, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.015e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=6.015e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.008e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.008e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=2.831e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.827e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=3.003e-02, previous alpha=2.803e-02, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.339e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.652e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.838e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.834e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.509e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=2.509e-02, previous alpha=2.503e-02, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.339e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.652e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.838e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=3.834e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.509e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=2.509e-02, previous alpha=2.503e-02, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=6.955e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.477e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.196e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.175e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.082e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=3.130e-02, previous alpha=2.990e-02, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=6.955e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.477e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.196e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.175e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.082e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=3.130e-02, previous alpha=2.990e-02, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.631e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.499e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.499e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.448e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.317e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=1.621e-02, previous alpha=1.255e-02, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.631e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.499e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.499e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.448e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.317e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.161e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.627e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.591e-03, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.634e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.817e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.817e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.750e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.392e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.112e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.052e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.003e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.922e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.185e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.103e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=3.590e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=7.120e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.111e-04, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=5.634e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.817e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.817e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.749e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.381e-02, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.054e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.004e-02, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.804e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=7.804e-03, with an active set of 13 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.725e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=6.091e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.380e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=5.380e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.707e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=1.307e-03, with an active set of 14 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.131e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=1.131e-02, previous alpha=1.127e-02, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.131e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=1.131e-02, previous alpha=1.127e-02, with an active set of 6 regressors.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x950 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# User Interface for Global SHAP based on the user inputs"
      ],
      "metadata": {
        "id": "cXigKT2Kgng9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Machine Learning Model Explanation Interface')\n",
        "\n",
        "# Select the number of users for SHAP global explanations\n",
        "num_users = int(input('Enter the number of users for SHAP global explanations: '))\n",
        "\n",
        "# Select the indices of the users for SHAP global explanations\n",
        "user_indices = []\n",
        "for i in range(num_users):\n",
        "    index = int(input(f'Enter the index of user {i+1} (between 0 and {len(x_train2)-1}): '))\n",
        "    user_indices.append(index)\n",
        "\n",
        "# Select the model\n",
        "selected_model = input('Select Model (MLP/RF/SVM): ')\n",
        "\n",
        "# Get the selected model\n",
        "if selected_model == 'MLP':\n",
        "    model2 = mlp_model2\n",
        "elif selected_model == 'Random Forest':\n",
        "    model2 = rf_model2\n",
        "else:\n",
        "    model2 = svm_model2\n",
        "\n",
        "# Select the data instances based on the user-provided indices\n",
        "selected_data = x_train2.iloc[user_indices]\n",
        "\n",
        "explainer = shap.KernelExplainer(model2.predict_proba, selected_data)\n",
        "\n",
        "# Compute SHAP values for global explanation for class 0\n",
        "shap_values_global_class0 = explainer.shap_values(selected_data, nsamples=num_users)\n",
        "\n",
        "# Calculate the mean absolute SHAP values across all instances for class 0\n",
        "mean_shap_values_class0 = np.abs(shap_values_global_class0).mean(axis=0)\n",
        "\n",
        "# Compute SHAP values for global explanation for class 1\n",
        "shap_values_global_class1 = explainer.shap_values(selected_data, nsamples=num_users, output_inds=1)\n",
        "\n",
        "# Calculate the mean absolute SHAP values across all instances for class 1\n",
        "mean_shap_values_class1 = np.abs(shap_values_global_class1).mean(axis=0)\n",
        "\n",
        "# Plot the global feature importance for both classes using a bar chart\n",
        "shap.summary_plot([mean_shap_values_class0, mean_shap_values_class1],\n",
        "                  selected_data,\n",
        "                  feature_names=x_test2.columns,\n",
        "                  class_names=['Class 0', 'Class 1'],\n",
        "                  plot_type=\"bar\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "970fdebd8c97472ea546cf6814565814",
            "c25756e975b945459f2c7be10d20b52b",
            "cbab6e083480469bab9ea0429f6684b7",
            "b02163d07bd44e5c8d0a15dda6e2ee12",
            "f45b21e5d5c14fbbafacc1cbc48c8ded",
            "27be3bcca20e45f7b2e4c37c1f462a72",
            "cddadd5971c24697bc647ef13b196934",
            "7bc8dcc079d645bc83ebad8fe5b8405b",
            "578eff3e1f214ce49f9b5f5f33c39c0d",
            "0520f5422e264a459d6c9d272d828426",
            "6f3062f0431847ee90ece49acda1886f",
            "cc14693a844a4bf0b2b3c4479d1654a0",
            "af35d405064d43d1b27edfd96b07a5a0",
            "e07a5ca0357f43b0a51e169fbe807aa6",
            "68c36f6960354ba9954733ec123b816b",
            "2bfa71b6ce6f4c5d80f1550a6a8e22a3",
            "5cc7c6798e3247168a8d9580eab5f431",
            "20e80f0cb572480cafb51da8eafed5c2",
            "8d385d264f724374be582d0109f7ca69",
            "66854292d1bf4f768c2fbf6199b2593f",
            "b09e5c508a2d46e99a9450c933190da8",
            "c5f2f973ee844d5b8de67f3473f067c2"
          ]
        },
        "id": "MEUwfmXogm2-",
        "outputId": "c0f1b256-a96b-46ac-fce1-f3571f786ce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Machine Learning Model Explanation Interface\n",
            "Enter the number of users for SHAP global explanations: 20\n",
            "Enter the index of user 1 (between 0 and 58475): 2\n",
            "Enter the index of user 2 (between 0 and 58475): 5\n",
            "Enter the index of user 3 (between 0 and 58475): 7\n",
            "Enter the index of user 4 (between 0 and 58475): 9\n",
            "Enter the index of user 5 (between 0 and 58475): 20\n",
            "Enter the index of user 6 (between 0 and 58475): 56\n",
            "Enter the index of user 7 (between 0 and 58475): 890\n",
            "Enter the index of user 8 (between 0 and 58475): 900\n",
            "Enter the index of user 9 (between 0 and 58475): 1000\n",
            "Enter the index of user 10 (between 0 and 58475): 1200\n",
            "Enter the index of user 11 (between 0 and 58475): 1250\n",
            "Enter the index of user 12 (between 0 and 58475): 1400\n",
            "Enter the index of user 13 (between 0 and 58475): 1700\n",
            "Enter the index of user 14 (between 0 and 58475): 1900\n",
            "Enter the index of user 15 (between 0 and 58475): 2100\n",
            "Enter the index of user 16 (between 0 and 58475): 2200\n",
            "Enter the index of user 17 (between 0 and 58475): 2400\n",
            "Enter the index of user 18 (between 0 and 58475): 2500\n",
            "Enter the index of user 19 (between 0 and 58475): 5000\n",
            "Enter the index of user 20 (between 0 and 58475): 5700\n",
            "Select Model (MLP/RF/SVM): MLP\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "970fdebd8c97472ea546cf6814565814"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.068e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.034e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.034e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=9.944e-04, previous alpha=6.246e-04, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=2.068e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.034e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.034e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=9.944e-04, previous alpha=6.246e-04, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.891e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.411e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.405e-02, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=2.411e-02, previous alpha=2.402e-02, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=7.891e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=2.414e-02, previous alpha=2.414e-02, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.706e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.453e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.453e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.151e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.151e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=2.857e-03, previous alpha=1.532e-03, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.706e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.453e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.453e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.151e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=3.151e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=2.857e-03, previous alpha=1.532e-03, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.347e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.946e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.646e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=1.277e-02, previous alpha=6.409e-03, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.347e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.946e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.646e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=1.277e-02, previous alpha=6.409e-03, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.290e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.818e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 20 iterations, alpha=1.795e-02, previous alpha=1.718e-02, with an active set of 11 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=7.290e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.818e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=2.818e-02, previous alpha=2.818e-02, with an active set of 5 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.725e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=9.153e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.804e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=7.529e-03, previous alpha=7.205e-03, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.747e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.675e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=8.675e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=7.176e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 8 iterations, alpha=6.744e-03, previous alpha=5.586e-03, with an active set of 7 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.041e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.009e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.963e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.175e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.884e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=4.354e-03, previous alpha=3.350e-03, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.041e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.009e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=4.963e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.175e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.884e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=4.354e-03, previous alpha=3.350e-03, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.943e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.376e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.846e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.612e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=4.331e-03, previous alpha=1.221e-03, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=5.943e-03, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=5.376e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.846e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.612e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=4.331e-03, previous alpha=1.221e-03, with an active set of 9 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.406e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.203e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.203e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.133e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.133e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=4.407e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=5.751e-03, previous alpha=4.099e-03, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.406e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.180e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.203e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.203e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.002e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=8.696e-03, previous alpha=8.639e-03, with an active set of 7 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.035e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.652e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.652e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.554e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.110e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.948e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.934e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.241e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.154e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.072e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.932e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.965e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.449e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.503e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.557e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.035e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.652e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.652e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.554e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=1.110e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.948e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=7.934e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=6.241e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.154e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.072e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=2.932e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=1.965e-04, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=9.449e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.503e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.557e-05, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.083e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.041e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.040e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.039e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.016e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.690e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=8.686e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.172e-04, previous alpha=6.305e-06, with an active set of 12 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.035e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.035e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.014e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.014e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 5 iterations, alpha=1.014e-02, previous alpha=1.005e-02, with an active set of 4 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.035e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=2.035e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.014e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.014e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 5 iterations, alpha=1.014e-02, previous alpha=1.005e-02, with an active set of 4 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.945e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.473e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=2.473e-02, previous alpha=2.462e-02, with an active set of 5 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.945e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.473e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 6 iterations, alpha=2.473e-02, previous alpha=2.462e-02, with an active set of 5 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=4.013e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.613e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.063e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.443e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=7.329e-03, previous alpha=6.586e-03, with an active set of 7 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=4.013e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.613e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.298e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.298e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.063e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=7.443e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=7.329e-03, previous alpha=6.586e-03, with an active set of 7 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.069e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=2.526e-02, previous alpha=2.526e-02, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.069e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 26 iterations, alpha=1.169e-02, previous alpha=7.758e-03, with an active set of 17 regressors.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc14693a844a4bf0b2b3c4479d1654a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.254e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.244e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.244e-05, previous alpha=5.222e-05, with an active set of 11 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.254e-05, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=5.244e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.244e-05, previous alpha=5.222e-05, with an active set of 11 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.909e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=1.610e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=1.608e-03, previous alpha=1.278e-03, with an active set of 10 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=4.909e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=2.604e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.037e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=2.026e-03, previous alpha=1.897e-03, with an active set of 10 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.003e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.015e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.015e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=5.015e-03, previous alpha=4.829e-03, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.003e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.015e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=5.015e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 7 iterations, alpha=5.015e-03, previous alpha=4.829e-03, with an active set of 6 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.434e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.142e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.950e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=4.979e-03, previous alpha=2.697e-03, with an active set of 11 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.434e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.142e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=3.950e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=4.979e-03, previous alpha=2.697e-03, with an active set of 11 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.052e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.052e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=4.047e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=4.052e-03, previous alpha=4.047e-03, with an active set of 10 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.095e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.928e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=3.900e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.913e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 10 iterations, alpha=2.913e-03, previous alpha=2.912e-03, with an active set of 7 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=1.095e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=7.928e-03, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=2.913e-03, previous alpha=2.913e-03, with an active set of 7 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=4.438e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=4.383e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 11 iterations, alpha=4.383e-03, previous alpha=4.336e-03, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.976e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.333e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.167e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.241e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.131e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.762e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.806e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.679e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.083e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.047e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.041e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.438e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.211e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.412e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.296e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=1.817e-03, previous alpha=1.888e-04, with an active set of 11 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.976e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=8.333e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.167e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.241e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=3.131e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=2.762e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.806e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=1.679e-03, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.083e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.047e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.041e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=5.438e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=4.211e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.412e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=3.296e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 14 iterations, alpha=1.817e-03, previous alpha=1.888e-04, with an active set of 11 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=9.839e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=4.919e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.443e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.824e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.824e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 9 iterations, alpha=1.756e-02, previous alpha=1.679e-02, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=3.043e-02, with an active set of 3 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=2.214e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.695e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.678e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.668e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=8.501e-03, previous alpha=8.212e-03, with an active set of 10 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.644e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.097e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.743e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.729e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.724e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 12 iterations, alpha=2.531e-02, previous alpha=2.459e-02, with an active set of 7 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 1 iterations, i.e. alpha=8.644e-02, with an active set of 1 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=8.097e-02, with an active set of 2 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=3.694e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.743e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.729e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.724e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.082e-02, with an active set of 7 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.098e-02, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=2.498e-02, previous alpha=6.164e-03, with an active set of 8 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=2.802e-03, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.401e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.392e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.746e-04, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=3.066e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=2.883e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.373e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.190e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.701e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.608e-03, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.501e-04, with an active set of 9 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=2.361e-04, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=5.648e-05, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=2.470e-05, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.308e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.302e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=2.302e-04, previous alpha=2.296e-04, with an active set of 12 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=2.308e-04, with an active set of 11 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=2.302e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 15 iterations, alpha=2.302e-04, previous alpha=2.296e-04, with an active set of 12 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.423e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.291e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.291e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=7.333e-03, previous alpha=6.062e-03, with an active set of 11 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=5.423e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.291e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.291e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 24 iterations, alpha=7.708e-03, previous alpha=6.106e-03, with an active set of 11 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.679e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.818e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.237e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.649e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=6.537e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=5.081e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.746e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.549e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.072e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=5.219e-04, previous alpha=2.921e-05, with an active set of 18 regressors.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=7.679e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.818e-03, with an active set of 8 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=1.237e-03, with an active set of 10 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=9.649e-04, with an active set of 12 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=7.171e-04, with an active set of 15 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.756e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=6.695e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:648: ConvergenceWarning:\n",
            "\n",
            "Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=1.686e-04, with an active set of 16 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_least_angle.py:678: ConvergenceWarning:\n",
            "\n",
            "Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 25 iterations, alpha=8.637e-04, previous alpha=9.065e-05, with an active set of 18 regressors.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x950 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAOsCAYAAADX7yC0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADMDklEQVR4nOzdeXxN1/7/8ffJRIQk5koipqA19IvGWCWqisQYMbVUQ1ExtIq2V4dL6bd1r5qnRM2q0mhIYyy30eJWJHGvDlqthiJBBZHJlOT8/vBzvk5PEHY4Da/n49HHlbXXXuuzd+4f55211z4ms9lsFgAAAAAY4GDvAgAAAAAUfwQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECxgWERGhq1ev2rsMAAAA2BHBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhpnMZrPZ3kWgeDNNz7V3CQBgxTwhxN4lAEDRM2+wdwW3xIoFAAAAAMMIFgAAAAAMI1gAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGBxg65du2rYsGH2LuO2ikudAAAAeHgQLP6iwsPDtXPnTnuXAQAAABSKk70LQMEWL16sLl26KCAgwObY559/LpPJdP+LAgAAAG6CYHGPXbp0SU5OTnJyKrpb7eLiUmRjAQAAAEXhoXwU6tSpU3rzzTfVtm1btW3bVmPHjtWJEyds+qWmpsrf31/h4eE2x8LDw+Xv76/U1FRL26RJk+Tv76/z589r8uTJevbZZ/XUU0/pjz/+kCRFRUVp5MiR6ty5s1q0aKGOHTvqnXfesRrj+pyStHHjRvn7+1v+u+5meyx27typwYMHq3Xr1nrqqac0ePDgAh+nun7+0aNH9corr6hNmzZq27atXn/9daWlpRX+RgIAAAD/30O3YpGZmalhw4bp9OnTCg4OVs2aNbV//34NHz5cly9fLpI5Ro4cqfLly2vIkCG6ePGiSpUqJUlavXq1GjRooL59+8rDw0O//fabNmzYoISEBK1du1aenp4qW7as3nvvPb377rtq3LixevbsWag5o6KiNG3aNFWvXl0vvfSSpGvBZPz48Zo4caKCg4Ot+p85c0bDhw9XQECAxowZo19//VXR0dHKzs7W/Pnzi+Q+AAAA4OHx0AWLlStXKjU1Ve+++666desmSerdu7c++ugjffrpp0UyR61atTRlyhSb9rVr18rV1dWqrU2bNgoLC1NMTIwGDRokV1dXBQYG6t1335W3t7cCAwNvO19GRobmzJkjHx8fLV++XKVLl5YkhYSE6Pnnn9esWbPUoUMHlSlTxnLO8ePH9cEHH6hDhw6WNgcHB0VFReno0aOqXr36XV49AAAAHkYP3aNQO3fuVPny5RUUFGTVPmjQoCKbY8CAAQW2Xw8V+fn5ysrKUnp6uurUqaPSpUvrhx9+uOv54uPjdfHiRfXr188SKiSpdOnS6tevn3JychQfH291TsWKFa1ChSTL41bHjx+/61oAAADwcHroVixSUlJUr149OTo6WrVXqFDB6i/6RlSrVq3A9oSEBC1evFg//vijzWNXmZmZdz1fSkqKJKlmzZo2x663Xe9znbe3t01fDw8PSdKFCxfuuhYAAAA8nB66YHEnbvVK17y8vJseK1mypE3bjz/+qFGjRsnHx0ejRo2Sl5eXSpQoIZPJpIkTJyo/P79Iai4sB4ebL1aZzeb7WAkAAAAeBA9dsPD29tbx48eVl5dntWqRlpZms2rg7u4u6doehj/78wrA7WzdulV5eXmaM2eO1WrBxYsXDa1WSJKPj48kKTk5Wc2aNbM6duTIEUkFr1AAAAAAReWh22PRtm1bnT17Vps2bbJqX7FihU1fNzc3lS9fXgkJCVZ/xT9x4sQdfyv29RDz59WApUuXFrhaUapUqUI/ktS8eXO5uroqMjJS2dnZlvbs7GxFRkaqVKlSatGixR3VCwAAANyJh27F4oUXXtDWrVv1/vvv66efflKtWrWUlJSk7777Tp6enjb9+/Tpo4ULF2rMmDFq27at0tLS9Pnnn6tWrVo6ePBgoecNCAjQmjVr9Morr6hnz55ydnZWfHy8Dh8+XOC8DRo00L59+7R8+XI98sgjMplM6tixY4FjlylTRmPGjNG0adP04osvqkuXLpKuvW72+PHjmjhxotWmbgAAAKCoPXTBwt3dXR9//LFmzJihzZs3S5KaNGmi8PBwjRgxwqb/oEGDlJWVpc2bNyspKUk1atTQO++8o59++umOgkWjRo30j3/8Qx9//LEWLVqkEiVKqFmzZoqIiNDQoUNt+r/55puaNm2ali1bZlmFuFmwkK69MrdChQpatWqVFi9eLEmqU6eOpk+froCAgELXCQAAANwNk5mdujDIND3X3iUAgBXzhBB7lwAARc+8wd4V3NJDt8cCAAAAQNEjWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCM183CsIiICIWGhsrZ2dnepQAAAMBOWLEAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIaZzGaz2d5FoHgzTc+1dwl4gJgnhNi7BDxozBvsXQEAPBRYsQAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIYRLAAAAAAYRrB4QCUmJio8PFyZmZn2LgUAAAAPAYLFAyopKUmLFy8mWAAAAOC+IFjYQXZ29j3tDwAAANxvTvdi0CtXrmj16tXaunWrTpw4IRcXFzVu3FjDhw/Xo48+aumXmJiol19+WX//+9916dIlffrppzp16pSqVq2qUaNG6amnntLhw4c1e/Zsfffdd3JyclKnTp00duxYOTn9X+nDhg3TyZMntXDhQs2YMUNJSUmSpKZNm+rVV1+Vj4/PLetdt26dPvzwQ3300Udq27at1bH8/Hx16dJFnp6eWrNmjSRp7969iomJ0cGDB5WWliZnZ2fVr19fgwcP1hNPPGF1/o21zZkzR4mJicrIyFBiYqJNHampqerWrZuGDh2qGjVqaOXKlTpy5Ig6dOigSZMm6ejRo1q7dq3279+vU6dOKS8vTzVq1FBISIh69OhhGWfSpEnauHGjJKlbt26W9qFDh2r48OGSpKysLC1dulRfffWVTp8+LTc3NzVr1kxhYWG3vV8AAADAnxV5sMjNzdXo0aP13XffKTAwUH369FFWVpbWr1+vIUOGaPHixapXr57VOVFRUcrIyFCPHj3k4uKiyMhIjR8/XtOmTdPUqVPVsWNHtW3bVvHx8YqMjFTZsmX10ksvWY1x8eJFDR8+XA0aNNCoUaN07NgxrVu3Tt9//70++eQTVahQ4aY1P/vss5oxY4Y2bdpkEyz27dunP/74Q88//7ylLTY2VhcuXFBgYKAqV66sP/74QzExMQoLC9OiRYvUuHFjqzFycnI0fPhwPf744woLC9O5c+dueQ+//vprRUZGqlevXurVq5fc3NwkXQti+/fvV+vWreXl5aVLly5px44dmjp1qs6fP6/Q0FBJUnBwsLKzsxUXF6fXXntNnp6ekqTatWtLuhYqBg8erFOnTqlbt26qWbOm0tLStG7dOr344otatWqVqlSpcssaAQAAgBsVebCIjIxUUlKS5s6dq5YtW1raQ0JC1LdvX82aNUsRERFW55w5c0ZRUVEqXbq0pGsrDf3799eECRM0bdo0Pf3005YxBgwYoKioKJtgkZ6erv79+2vcuHGWtiZNmmjChAmKiIjQxIkTb1qzu7u7nnrqKe3atUsZGRlyd3e3HNu0aZMcHR3VuXNnS9vbb78tV1dXqzF69eqlPn36aNmyZTbB4sKFC+rVq5fCwsJuee+u++2337R27VrVqFHDqj0oKEghISFWbc8995xefvllLV++XAMHDpSTk5Mef/xx+fn5KS4uTgEBAfLy8rI6Z9GiRUpJSdGyZctUp04dS3vXrl3Vr18/hYeHa9KkSYWqFQAAAJDuwR6LLVu2qHr16nrssceUnp5u+S83N1fNmzfXgQMHdOnSJatzunTpYgkV0rW/rLu5ualixYqWUHFdo0aNdPbsWeXk5NjMPWjQIKuf27Vrp2rVqunrr7++bd1dunTRlStX9OWXX1racnJytHPnTrVq1UrlypWztN8YKnJycpSeni5HR0c1aNBAP/74Y4HjDxw48LY1XNe6dWubUPHneS9fvqz09HRlZGSoRYsWys7O1tGjR287ttls1pYtW9S4cWNVqlTJ6nfk6uqqBg0aaO/evYWuFQAAAJDuwYrFkSNHdPnyZT3zzDM37ZOenq5HHnnE8rO3t7dNH3d3d1WuXNmmvUyZMpKurQKUKlXKqr2gx51q1KihnTt36uLFizarDDdq2bKlypUrp82bN1tWBb766itdvHhRQUFBVn1PnDih+fPna+/evTZvXTKZTDZjly1b1lJ3Yfj6+hbYnpOTo4iICG3fvl2nT5+2OZ6RkXHbsc+fP68LFy5o7969N/0dOTiwpx8AAAB35p5s3vbz89PYsWNverxs2bJWPzs6OhbY71YfcM1m890VdxNOTk7q2LGjPv30Ux0/flxVq1bVpk2b5O7urjZt2lj65eTkaOjQobp48aL69+8vPz8/ubm5yWQyafny5UpISLAZu2TJkndUy836v/XWW9q9e7d69uypJk2ayMPDQw4ODtqzZ4/WrFmj/Pz82459/b41a9bMZoUHAAAAuFtFHiyqVq2q8+fPq2nTpvf1L9+ZmZlKS0uzWbU4cuSIypUrd8vViuu6dOmiTz/9VJs2bVKPHj2UlJSknj17ysXFxdJn3759OnPmjN59912rNy5J0sKFC4vmYgqQmZmp3bt3KzAw0Ga/yL59+2z6F7RyIv3f6kl2draaN29+T2oFAADAw6fIP/kHBQXp7Nmz+uSTTwo8fvbs2aKe0mLFihVWP8fFxen333+3edPT0aNHdeLECZvz69atq9q1a2vLli3avHmz5VWzN7q+uvLnFZO9e/fqhx9+KHStaWlpOnr0qM1+k5u5HtL+PG9aWpo2bNhg0//6Y2J/fjzKwcFBnTp10o8//qgdO3YUONft3loFAAAA/FmRr1j0799f8fHxmj17thISEtS0aVO5ubnp1KlTSkhIkIuLi8LDw4t6Wnl6euqrr77SmTNn9MQTT1heN1u+fHnLdzdcFxISoipVqig2NtZmnKCgIM2aNUsrVqyQr6+vGjZsaHW8UaNGKl++vGbNmqWTJ0+qUqVK+uWXX7R582b5+fnp8OHDhap33rx52rhxoxYtWiR/f//b9ndzc1OLFi20ZcsWlShRQvXr19fJkycVHR0tb29vXbhwwap/gwYNJElz5sxR586d5eLiolq1asnPz08jR47UgQMH9Le//U3/+te/1LBhQzk7O+vkyZPas2ePHnvsMd4KBQAAgDtS5MHCyclJs2bN0rp167R582ZLiKhYsaLq169vswJQVFxdXS1fkDdv3jyZzWa1bNlSY8eOveV3WPxZ586dNXfuXGVnZ+uFF16wOV6mTBnNmzdPc+bMUWRkpPLy8vToo49q9uzZiomJKXSwuBtTpkzR3LlztWvXLm3atElVq1ZVWFiYnJycNHnyZKu+jRo10ujRoxUdHa2pU6cqLy9PQ4cOlZ+fn0qXLq2lS5dq9erV2r59u7755hs5OjqqUqVKatSokdWX7QEAAACFYTIX9S5oO7j+7dYFrUDg3jNNz7V3CXiAmCeE3L4TcCfMG+xdAQA8FHivKAAAAADDCBYAAAAADCNYAAAAADDsnnxB3v0WERFh7xIAAACAhxorFgAAAAAMI1gAAAAAMIxgAQAAAMCwB+J7LGBfERERCg0NlbOzs71LAQAAgJ2wYgEAAADAMIIFAAAAAMMIFgAAAAAMI1gAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADDOZzWazvYtA8WaanmvvEgrFPCHE3iXcH+YN9q4AAAA8hFixAAAAAGAYwQIAAACAYQQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhhEsJA0bNkxdu3a1dxmFFhsbK39/fyUmJtq7FAAAAEASweIvKzExUeHh4crMzLR3KQAAAMBtESz+opKSkrR48eICg0VgYKD27NmjJk2a2KEyAAAAwJaTvQvAnXN0dJSjo6O9ywAAAAAsiv2KxZUrV7R06VL16dNHrVq1UkBAgMaOHauff/7Zpm9GRoamTp2q9u3bq3Xr1ho2bJh++umnAsf19/fXpEmTbNpvtr8hKytL8+fPV0hIiFq1aqX27dtryJAh2rZtm6XP0aNH9eGHH6pPnz5q06aNnnzySQ0YMEAbNmywGmvSpElavHixJKlbt27y9/eXv7+/wsPDb1lDenq6pk2bpqCgILVo0UJBQUGaNm2a0tPTC7yGhIQErVq1St27d1fLli0VHBysjRs3Fng/AAAAgFsp1isWubm5Gj16tL777jsFBgaqT58+ysrK0vr16zVkyBAtXrxY9erVs/QdNWqUDh48qMDAQDVs2FC//PKLwsLC5OHhYaiOzMxMDRkyRMnJyWrfvr1CQkKUl5enQ4cOaffu3erYsaOka/sm9u/fr9atW8vLy0uXLl3Sjh07NHXqVJ0/f16hoaGSpODgYGVnZysuLk6vvfaaPD09JUm1a9e+aQ1ZWVkaPHiwjh8/rm7duunRRx/VoUOHtG7dOiUkJGjFihVyc3OzOmf+/Pm6fPmygoOD5eLionXr1mnSpEny8fFRo0aNDN0TAAAAPFyKdbCIjIxUUlKS5s6dq5YtW1raQ0JC1LdvX82aNUsRERGSpC+++EIHDx7U0KFDNXz4cEvfGjVqaMaMGapSpcpd1zF//nwlJydr4sSJCg4OtjqWn59v+XdQUJBCQkKsjj/33HN6+eWXtXz5cg0cOFBOTk56/PHH5efnp7i4OAUEBMjLy+u2NaxYsULHjh3TG2+8od69e1va69Spo3/84x9auXKlRowYYXXOlStXtHLlSjk7O0uS2rdvr+7du+uzzz4jWAAAAOCOFOtHobZs2aLq1avrscceU3p6uuW/3NxcNW/eXAcOHNClS5ckSTt37pSjo6Oef/55qzFCQkJs/pJ/J/Lz8/Xll1+qRo0aNqFCkhwc/u8Wu7q6Wv59+fJlpaenKyMjQy1atFB2draOHj1613Xs3LlTZcuWVc+ePa3ag4ODVbZsWcXFxdmc07t3b0uokKRKlSrJ19dXx48fv+s6AAAA8HAq1isWR44c0eXLl/XMM8/ctE96eroeeeQRpaSkqEKFCipdurTVcRcXF3l7e9/1a12vh4MbV0xuJicnRxEREdq+fbtOnz5tczwjI+OuapCk1NRUPfbYY3Jysv6VOjk5ydfXt8A9J97e3jZtHh4eOnXq1F3XAQAAgIdTsQ4WkuTn56exY8fe9HjZsmWLdL68vLy7Pvett97S7t271bNnTzVp0kQeHh5ycHDQnj17tGbNGqvHpu6HG1dTbmQ2m+9rHQAAACj+inWwqFq1qs6fP6+mTZve9EPydd7e3oqPj1dWVpbVqsWVK1eUkpIid3d3q/4eHh66cOGCzTgpKSlWP3t6esrd3V2//vrrLefPzMzU7t27FRgYqIkTJ1od27dvn01/k8l0y/H+zNvbW7///rtyc3OtVi1yc3N17NixAlcnAAAAgKJSrPdYBAUF6ezZs/rkk08KPH727FnLv9u2bau8vDybvuvWrVN2drbNub6+vvr+++8tezSka48qffHFF1b9HBwc1LFjRyUnJ9u8Nlb6v7/+Xw8+f14NSEtLK/C8UqVKWeYsjLZt2+r8+fM2Y23YsEHnz59Xu3btCjUOAAAAcDeK9YpF//79FR8fr9mzZyshIUFNmzaVm5ubTp06pYSEBLm4uFi++6Fbt25av369Fi9erJSUFD3++OM6dOiQduzYIR8fH5tHnPr06aN33nlHL7/8sgIDA5WZmakNGzaoSpUqVoFFkkaMGKGEhARNnTpV8fHx+p//+R9J0qFDh5Sbm6spU6bIzc1NLVq00JYtW1SiRAnVr19fJ0+eVHR0tLy9vW1WRxo0aCBJmjNnjjp37iwXFxfVqlVLfn5+Bd6LQYMG6V//+pf+8Y9/6NChQ6pbt64OHTqkmJgYVatWTS+88EKR3HMAAACgIMU6WDg5OWnWrFlat26dNm/ebAkRFStWVP369dWlSxdLX2dnZ82fP1+zZ8/W119/ra+++kr16tXT/PnzNWvWLJ08edJq7M6dO+vMmTP67LPPNHPmTHl7e+ull16Sg4ODfvjhB6u+7u7uWrZsmZYuXaq4uDjFxcXJzc1NNWrUUN++fS39pkyZorlz52rXrl3atGmTqlatqrCwMDk5OWny5MlWYzZq1EijR49WdHS0pk6dqry8PA0dOvSmwaJ06dJasmSJwsPD9c033+iLL75Q+fLl1atXLw0fPtzQm68AAACA2zGZ2akLg0zTc+1dQqGYJ4TcvtODwLzB3hUAAICHULHeYwEAAADgr4FgAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGABAAAAwDBeNwvDIiIiFBoaKmdnZ3uXAgAAADthxQIAAACAYQQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGCYyWw2m+1dBIo30/Rce5dQIPOEEHuXUPTMG+xdAQAAQIFYsQAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQSLeyg1NVX+/v4KDw+3dymF1rVrVw0bNszeZQAAAKCYIVgAAAAAMIxgAQAAAMCwYhEssrOz7V0CAAAAgFu4L8EiNjZW/v7+SkhI0KpVq9S9e3e1bNlSwcHB2rhxo1Vff39/TZo0Sfv27dOQIUP01FNPaezYsTcd+8qVK1q6dKn69OmjVq1aKSAgQGPHjtXPP/9s1S8xMVH+/v6KjY1VVFSUgoOD1apVK/Xt21e7du2SJB0+fFijR49W27Zt1b59e/3zn/9Ubm6u1TjDhg1T165ddeLECb322mtq27at2rZtq/Hjx+vEiROFuh+5ublavny5evfurVatWql9+/YaP368Dh8+bOlz7tw5tWjRQm+//XaBY0ybNk1NmzZVamqqpS0rK0tz5sxRjx491LJlSz3zzDOaOHFigXWdOnVKb775pqX+sWPHFrp+AAAA4M+c7udk8+fP1+XLlxUcHCwXFxetW7dOkyZNko+Pjxo1amTpd/DgQX311Vfq0aOHunTpctPxcnNzNXr0aH333XcKDAxUnz59lJWVpfXr12vIkCFavHix6tWrZ3VOVFSUMjIy1KNHD7m4uCgyMlLjx4/XtGnTNHXqVHXs2FFt27ZVfHy8IiMjVbZsWb300ktWY1y8eFHDhw9XgwYNNGrUKB07dkzr1q3T999/r08++UQVKlS45X145513tH37djVv3ly9evXS2bNnFRUVpdDQUC1evFiPPvqoypUrpzZt2iguLk6ZmZkqU6aM5fzLly9r69atatasmby8vCRdCxWDBw/WqVOn1K1bN9WsWVNpaWlat26dXnzxRa1atUpVqlSRJGVmZmrYsGE6ffq0goODVbNmTe3fv1/Dhw/X5cuXC/W7BAAAAG50X4PFlStXtHLlSjk7O0uS2rdvr+7du+uzzz6zChbJycmaP3++mjdvfsvxIiMjlZSUpLlz56ply5aW9pCQEPXt21ezZs1SRESE1TlnzpxRVFSUSpcuLUlq2rSp+vfvrwkTJmjatGl6+umnLWMMGDBAUVFRNsEiPT1d/fv317hx4yxtTZo00YQJExQREaGJEyfetOa9e/dq+/bt6tChg/73f/9XJpNJktShQwcNHDhQ06dP18cffyxJCg4O1ldffaWtW7eqd+/eljG++uorZWZmqkePHpa2RYsWKSUlRcuWLVOdOnUs7V27dlW/fv0UHh6uSZMmSZJWrlyp1NRUvfvuu+rWrZskqXfv3vroo4/06aef3vKeAwAAAAW5r3ssevfubQkVklSpUiX5+vrq+PHjVv3q1Klz21AhSVu2bFH16tX12GOPKT093fJfbm6umjdvrgMHDujSpUtW53Tp0sUSKiSpdu3acnNzU8WKFS2h4rpGjRrp7NmzysnJsZl70KBBVj+3a9dO1apV09dff33Lmnfu3ClJGjx4sCVUXL/mp556Sv/97391/vx5SVLz5s3l7e2tmJgYqzFiYmLk4eGhgIAASZLZbNaWLVvUuHFjVapUyepeuLq6qkGDBtq7d69VDeXLl1dQUNAtrwkAAAAorPu6YuHt7W3T5uHhoVOnTlm1+fr6Fmq8I0eO6PLly3rmmWdu2ic9PV2PPPLILWtwd3dX5cqVbdqvP3504cIFlSpVyqq9oMedatSooZ07d+rixYtydXUtsJ7U1FQ5ODioRo0aNsdq1qypnTt3KiUlRWXLlpXJZFL37t21YMECHTp0SHXr1tWJEyeUlJSkfv36WULa+fPndeHCBe3du/em98LB4f8yZEpKiurVqydHR0erPhUqVLB65AoAAAAorPsaLG78cHsjs9ls9XPJkiULPaafn98tN3eXLVvW6uc/f5i+XW0F1Xc/devWTeHh4YqJidHrr7+uL774Qmaz2eoxqOv1NWvWjFUHAAAA2MV9DRZFrWrVqjp//ryaNm16y2BQ1DIzM5WWlmazanHkyBGVK1fupqsV0rUVk/z8fB05ckS1a9e2Of96n+sqVKigNm3aaOvWrRo9erQ2btyoBg0aqFatWpY+ZcuWVZkyZZSdnV2oR8i8vb11/Phx5eXlWQWttLQ0ZWZm3vZ8AAAA4M+KxfdYSNKJEyd09OhRq7agoCCdPXtWn3zySYHnnD179p7Vs2LFCquf4+Li9Pvvv6tt27a3PO/68WXLllmthBw+fFjffPONGjVqZLPK0qNHD2VkZOh///d/9ccff1itVkjXVls6deqkH3/8UTt27Chw3nPnzlnVcPbsWW3atOmW1wQAAAAUVrFZsRgxYoROnjypxMRES1v//v0VHx+v2bNnKyEhQU2bNpWbm5tOnTqlhIQEubi4KDw8vMhr8fT01FdffaUzZ87oiSeesLxutnz58ho+fPgtz23RooU6dOigL7/8UpmZmWrdurXldbMuLi4aP368zTktW7ZUlSpVtGXLFpUqVUrPPvusTZ+RI0fqwIED+tvf/qZ//etfatiwoZydnXXy5Ent2bNHjz32mOWtUC+88IK2bt2q999/Xz/99JNq1aqlpKQkfffdd/L09CyKWwQAAICHTLEJFgVxcnLSrFmztG7dOm3evNkSIipWrKj69evf8jswjHB1ddXChQs1Y8YMzZs3T2azWS1bttTYsWNv+x0WkjRlyhTVrVtXGzdu1KxZs+Tq6qomTZpoxIgR8vPzs+nv4OCg7t27a9GiRXrmmWesNpJfV7p0aS1dulSrV6/W9u3b9c0338jR0VGVKlVSo0aNrFY53N3d9fHHH2vGjBnavHmzpGuvyw0PD9eIESPu/sYAAADgoWUy23NncjE0bNgwnTx5UrGxsfd13hUrVmju3LlaunSpHn/88fs69+2YpufevpMdmCeE2LuEomfeYO8KAAAAClRs9lg8zHJzcxUdHS0/P7+/XKgAAAAApGL+KNSDLiUlRd9//72+/vprpaSk6P3337d3SQAAAECBCBZ/Yfv379fkyZPl6empoUOHqmPHjvYuCQAAACgQeyxgGHss7iP2WAAAgL8o9lgAAAAAMIxgAQAAAMAwggUAAAAAw9hjAcMiIiIUGhoqZ2dne5cCAAAAO2HFAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIYRLAAAAAAYZjKbzWZ7F4HizTQ9194lSJLME0LsXULRM2+wdwUAAACFwooFAAAAAMMIFgAAAAAMI1gAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGBhUNeuXTVs2DBDY0RFRalXr15q2bKl/P39lZqaWkTVFczf31+TJk26p3MAAADg4eJk7wIedomJiZo2bZratm2rQYMGycnJSWXLlr3vdYSHh6tu3boKCAi473MDAACg+CNY2Fl8fLwk6d1335WHh4fd6li8eLG6dOlCsAAAAMBd4VEoO0tLS5Mku4YKAAAAwChWLArp1KlTmjVrlr799ltJUpMmTTRu3Lib9o+Pj9fKlSv1448/6sqVK/L19VVISIhCQkIkSampqerWrZulv7+/v2XciIgIHT16VGvXrtX+/ft16tQp5eXlqUaNGgoJCVGPHj2s5po0aZI2btyoxMREmzr8/f3VpUuXm+6puLGOjRs3auPGjZZjBY0HAAAAFIRgUQiZmZkaNmyYTp8+reDgYNWsWVP79+/X8OHDdfnyZZv+0dHR+uCDD9SwYUMNHjxYrq6uio+P14cffqiUlBS98sorKlu2rN577z2tX79e//nPf/Tee+9JksqVKyfp2of6/fv3q3Xr1vLy8tKlS5e0Y8cOTZ06VefPn1doaGiRXNv1Ot599101btxYPXv2LJJxAQAA8HAhWBTCypUrlZqaqnfffdfy1/3evXvro48+0qeffmrVNy0tTdOnT9ezzz6r999/39Leu3dvTZ8+XZ988ol69eolHx8fBQYGat++ffrPf/6jwMBAq3GCgoIsqxvXPffcc3r55Ze1fPlyDRw4UE5Oxn99rq6uCgwM1Lvvvitvb2+bOgAAAIDCYI9FIezcuVPly5dXUFCQVfugQYNs+u7YsUNXrlxR9+7dlZ6ebvXfU089pfz8fO3bt++2c7q6ulr+ffnyZaWnpysjI0MtWrRQdna2jh49avi6AAAAgKLCikUhpKSkqF69enJ0dLRqr1ChgsqUKWPVdv0Df1hY2E3HO3fu3G3nzMnJUUREhLZv367Tp0/bHM/IyChE5QAAAMD9QbAoYmazWZI0efJkVahQocA+3t7etx3nrbfe0u7du9WzZ081adJEHh4ecnBw0J49e7RmzRrl5+db+ppMpgLHyM3NvYsrAAAAAO4cwaIQvL29dfz4ceXl5VmtWqSlpSkzM9Oqb9WqVSVJnp6eat68+V3Nl5mZqd27dyswMFATJ060OlbQY1Tu7u6SpAsXLli9tjYlJeWu5gcAAADuFHssCqFt27Y6e/asNm3aZNW+YsUKm74dOnSQi4uLwsPDdenSJZvjWVlZunLlyi3nc3C49mu5vvpxXVpamjZs2GDT39fXV5Jt6Fi9evUt57lRqVKldOHChUL3BwAAAG7EikUhvPDCC9q6davef/99/fTTT6pVq5aSkpL03XffydPT06pv5cqV9eabb2rq1Knq3bu3AgMDVaVKFZ0/f16HDx/Wzp07FRUVJS8vr5vO5+bmphYtWmjLli0qUaKE6tevr5MnTyo6Olre3t42AaBjx45asGCB3n//fR09elTu7u769ttvlZ6eXuhrbNCggfbt26fly5frkUcekclkUseOHe/kNgEAAOAhRrAoBHd3d3388ceaMWOGNm/eLOnaF9mFh4drxIgRNv27desmX19frV69WtHR0crMzJSnp6eqVaumESNGqHz58redc8qUKZo7d6527dqlTZs2qWrVqgoLC5OTk5MmT55s1bd06dKaPXu2ZsyYoWXLlsnV1VVPP/20pkyZonbt2hXqGt98801NmzZNy5YtU3Z2tiQRLAAAAFBoJvOfn7cB7pBp+l9jk7h5QsjtOxU35g32rgAAAKBQ2GMBAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCMYAEAAADAMIIFAAAAAMN43SwMi4iIUGhoqJydne1dCgAAAOyEFQsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhJrPZbLZ3ESjeTNNz7V3CfWGeEHKfJ9xwf+cDAAAwgBULAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsLiNxMRE+fv7KzY29pZtAAAAwMPMyd4FoPDOnTunuXPn6qefftIff/yhS5cuqVKlSmrSpIlCQ0NVtWpVe5cIAACAhxTB4jaaNGmiPXv2yMnJ/rcqIyNDv//+u1q0aKFHHnlEJUuW1LFjx/TFF1/oX//6l5YtW6aaNWvau0wAAAA8hOz/afkvKjs7W25ubnJwcFCJEiX+ErVUr15dS5cutTnevn17DRo0SJ999pnefPNNO1QIAACAh12x2mNx5coVLV26VH369FGrVq0UEBCgsWPH6ueff7bqFxsbK39/fyUmJtqMMWzYMHXt2tWqrWvXrho2bJh+/vlnjRo1Sm3btlX//v0l3X4/xdq1axUcHKxWrVopODhYa9euLbDf/v37FRYWprZt2+rJJ5/U888/rw0bNty0vhMnTuj111/X008/rbZt297yvlSpUkXStRWNgsZKTU3V+PHjFRAQoHbt2mnSpEnKyclRfn6+li5dqm7duqlVq1Z6/vnn9d///veWcwEAAAAFKTYrFrm5uRo9erS+++47BQYGqk+fPsrKytL69es1ZMgQLV68WPXq1bvr8U+fPq0RI0bomWee0dNPP62cnJzbnhMZGamzZ88qODhYpUqV0rZt2zR9+nRlZGRo2LBhln7ffPONJkyYoPLly2vAgAEqVaqUvvzyS02dOlUpKSkaOXKk1bg5OTkaPny4Hn/8cYWFhencuXM29yIrK0u5ubk6fvy4IiIiJElPPvmkTY0XL17UiBEj1KRJE40aNUoHDx7UF198ocuXL8vT01M//PCD+vTpo9zcXK1evVqvvfaaYmNj5ebmdje3EQAAAA+pYhMsIiMjlZSUpLlz56ply5aW9pCQEPXt21ezZs2yfMC+GykpKXr77bfVo0ePQp9z7NgxRUVFqXLlypKkPn36aMiQIVqyZIm6d++uypUrKy8vT//4xz/k6uqqFStWqGLFipa+w4cP14oVK9S1a1f5+vpaxr1w4YJ69eqlsLCwAuf99ttvNXbsWMvP5cuX16uvvqqgoCCbvunp6XrhhRf0wgsvWNoyMzO1Y8cOPfroo1q2bJll/0iNGjU0btw4bd26Vb169Sr0fQAAAACKzaNQW7ZsUfXq1fXYY48pPT3d8l9ubq6aN2+uAwcO6NKlS3c9voeHh80jUrfTqVMnS6iQJGdnZz333HPKy8vTrl27JEk//fSTTp06pW7dullCxfW+L7zwgvLz8/X111/bjD1w4MCbztuwYUPNnz9fM2bM0KhRo1S+fHllZmYqNzfXpq+jo6P69u1r1daoUSOZzWb16tXLalN648aNJUnHjx8v5B0AAAAArik2KxZHjhzR5cuX9cwzz9y0T3p6uh555JG7Gt/b21uOjo53dE6NGjVs2q6/lSklJUWSlJqaatV+o1q1aln1va5s2bIqU6bMTef19PRU8+bNJUlt2rRRUFCQ+vXrp3Pnzumtt96y6luhQgWbzefu7u6SJC8vrwLbL1y4cNO5AQAAgIIUm2AhSX5+flaPAP1Z2bJlJUkmk+mmffLy8gpsL1mypLHiitCd1lKxYkU1a9ZMX3zxhSZMmCAXFxfLMQeHmy9K3eyY2Wy+o/kBAACAYhMsqlatqvPnz6tp06a3/LAs/d9f3v/8liTp2gpCUX0nxZEjR2zakpOTJV1bAbnxf6+336qvEZcvX1ZeXp6ys7OtggUAAABwPxSbPRZBQUE6e/asPvnkkwKPnz171vLv6xuh9+3bZ9Vn69atOnPmTJHVtHXrVp0+fdry89WrV7VmzRo5OjqqdevWkqRHH31UjzzyiGJjY5WWlmbpm5ubq1WrVslkMt32dbLX3XiNN0pOTlZCQoJ8fHwsqzYAAADA/VRsViz69++v+Ph4zZ49WwkJCWratKnc3Nx06tQpJSQkyMXFReHh4ZKk6tWrq1mzZoqOjpbZbFadOnX0yy+/aOfOnapatWqBm5zvhq+vr1588UX16tVLpUqV0tatW3Xw4EG99NJLlr0ejo6Oev311zVhwgQNGjRIPXv2VKlSpbR9+3Z9//33Cg0NtXoj1K0sX75c8fHxevLJJ+Xl5SWz2azffvtNmzdvVm5urt54440iuS4AAADgThWbYOHk5KRZs2Zp3bp12rx5syVEVKxYUfXr11eXLl2s+r/33nv65z//qa1bt2rz5s1q3LixFi1apA8++EAnT54skpr69u2r7OxsRUZG6tSpU3rkkUc0btw4y5frXdemTRstWLBAS5Ys0apVq3T16lVVr179jl9v27p1a50+fVo7duzQuXPnlJ+fr0qVKumZZ57RgAEDLJvBAQAAgPvNZGanLgwyTS+aFaC/OvOEkPs84Yb7Ox8AAIABxWaPBQAAAIC/LoIFAAAAAMMIFgAAAAAMI1gAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAzjeyxgWEREhEJDQ+Xs7GzvUgAAAGAnrFgAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCMYAEAAADAMIIFAAAAAMNMZrPZbO8iULyZpufau4T7wjwhxOAAG4qkDgAAgL8iViwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBItiJDU1Vf7+/goPD78n43ft2lXDhg27J2MDAADgwUawKOYyMzMVHh6uxMREe5cCAACAh5iTvQtA4VWpUkV79uyRo6OjpS0zM1OLFy+WJPn7+9urNAAAADzkWLEoBrKzsyVJJpNJJUqUkJMTeRAAAAB/LXxCvY3Y2FhNnjxZCxYs0IEDBxQTE6Pz58/Lz89P48ePV8OGDZWUlKQFCxbo0KFDcnNzU+/evfXSSy9Zxti7d69iYmJ08OBBpaWlydnZWfXr19fgwYP1xBNPWM03bNgwnTx5UgsXLtScOXOUmJiojIwMJSYmKjU1Vd26ddPQoUM1fPhwJSYm6uWXX5YkLV682LJyUaVKFcXGxkqSoqKitHPnTiUnJ+v8+fPy8PBQs2bNNGLECHl5ed2nuwgAAIAHHcGikObNm6e8vDz169dPubm5Wr16tUaNGqXJkydrypQp6tmzpzp37qzt27dr0aJF8vLyUmBgoKRr4eTChQsKDAxU5cqV9ccffygmJkZhYWFatGiRGjdubDVXTk6Ohg8frscff1xhYWE6d+5cgTXVqFFDr732mmbMmKF27dqpXbt2kqRSpUpZ+qxevVoNGjRQ37595eHhod9++00bNmxQQkKC1q5dK09Pz3tzwwAAAPBQIVgUUl5enpYvXy5nZ2dJ1z7Ujxs3Tm+88YaWLVumevXqSZK6d++uLl26KCoqyhIs3n77bbm6ulqN16tXL/Xp00fLli2zCRYXLlxQr169FBYWdsuaypcvr4CAAM2YMUN+fn6W+W60du1am7nbtGmjsLAwxcTEaNCgQXd2IwAAAIACECwKKSQkxBIqJFnCQIMGDSyhQpLlMacDBw5Y2m78YJ+Tk6MrV67I0dFRDRo00A8//FDgfAMHDiySuq/PnZ+fr5ycHOXm5qpOnToqXbr0TecGAAAA7hTBopC8vb2tfnZ3d5ekAvcpuLu768KFC5afT5w4ofnz52vv3r3KzMy06msymWzOL1u2rMqUKVMUZSshIUGLFy/Wjz/+qMuXL1sd+3MtAAAAwN0iWBSSg0PBL9C68dWvBcnJydHQoUN18eJF9e/fX35+fnJzc5PJZNLy5cuVkJBgc07JkiWLpOYff/xRo0aNko+Pj0aNGiUvLy+VKFFCJpNJEydOVH5+fpHMAwAAABAs7rF9+/bpzJkzevfdd9WtWzerYwsXLjQ8fkErHtdt3bpVeXl5mjNnjtWKy8WLF1mtAAAAQJHieyzusesrGmaz2ap97969RbLH4foeioyMjELPvXTpUlYrAAAAUKRYsbjHGjVqpPLly2vWrFk6efKkKlWqpF9++UWbN2+Wn5+fDh8+bGh8T09PVa1aVV9++aV8fHxUrlw5ubq6qk2bNgoICNCaNWv0yiuvqGfPnnJ2dlZ8fLwOHz7Ma2YBAABQpFixuMfKlCmjefPmqUGDBoqMjNSsWbOUnJys2bNn69FHHy2SOaZMmaKqVatq/vz5euutt/TPf/5T0rVQ849//EOurq5atGiRIiIiVKJECUVERNi8ghYAAAAwwmT+83MywB0yTc+1dwn3hXlCiMEBNhRJHQAAAH9FrFgAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGABAAAAwDBeNwvDIiIiFBoaKmdnZ3uXAgAAADthxQIAAACAYQQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGCYyWw2m+1dBIo30/RcQ+ebJ4QUUSV2Yt5g7woAAADsjhULAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsCimEhMT5e/vr9jY2Pt6LgAAAFAQggUAAAAAw5zsXQDuTpMmTbRnzx45OfErBAAAgP3xqbSYcnBwUIkSJexdBgAAACCJR6GKrYL2SVy8eFHz5s1T9+7d1bJlS3Xs2FHvvvuuTp48edNx1q5dq+DgYLVq1UrBwcFau3bt/SgfAAAADxhWLB4Qubm5GjVqlA4cOKD27dtrwIABOnbsmD7//HPFx8dr5cqVqly5stU5kZGROnv2rIKDg1WqVClt27ZN06dPV0ZGhoYNG2anKwEAAEBxRLB4QMTGxurAgQMaOHCgXnnlFUt78+bN9eqrr2revHmaMmWK1TnHjh1TVFSUJXD06dNHQ4YM0ZIlS9S9e3ebIAIAAADcDI9CPSDi4uLk4OCg0NBQq/bWrVurTp06+uabb5Sfn291rFOnTlbhwdnZWc8995zy8vK0a9eu+1I3AAAAHgwEiwdEamqqKlasKHd3d5tjtWrVUnZ2ttLT063aa9SoYdO3Zs2akqSUlJR7UicAAAAeTAQLAAAAAIYRLB4Q3t7eOnPmjDIzM22OJScny83NTZ6enlbtR44cKbDv9fEAAACAwiJYPCACAgKUn5+v5cuXW7Xv2bNHhw4dUps2beTgYP3r3rp1q06fPm35+erVq1qzZo0cHR3VunXr+1E2AAAAHhC8FeoB0bVrV23cuFErVqxQamqqmjRpouPHj2vdunUqX768Ro4caXOOr6+vXnzxRfXq1UulSpXS1q1bdfDgQb300kt65JFH7HAVAAAAKK4IFg8IJycnzZs3T0uWLNH27dsVFxenMmXKqH379goLCyswKPTt21fZ2dmKjIzUqVOn9Mgjj2jcuHHq37+/Ha4AAAAAxRnBopi6/upYR0dHS5urq6tGjRqlUaNG3fJcf39/JSYmWn7u16/fvSkSAAAADw32WBRTZ86ckSSVK1fOzpUAAAAArFgUO2fPnlVcXJw+/fRTubm5qWHDhvYuCQAAAGDForg5cuSIZs6cKVdXV82YMUNubm72LgkAAABgxaK48ff31549e+xdBgAAAGCFFQsAAAAAhhEsAAAAABhGsAAAAABgmMlsNpvtXQSKt4iICIWGhsrZ2dnepQAAAMBOWLEAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIaZzGaz2d5FoHgzTc+1dwm3ZJ4Qcg8H33DvxgYAAChGWLEAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIY52bsA3Fx2drZWrFih+Ph4nThxQjk5OapcubLat2+voUOHqmTJkpa+6enpmj17tr755htduXJF9evX16uvvqoZM2bo5MmTio2NtRr74MGDWrp0qf7zn/8oJydHVapUUVBQkAYNGiQnJ/5vAQAAgDvDJ8i/sDNnzigmJkZPP/20OnXqJEdHR+3fv18rV67UoUOHNG/ePEnSlStXFBYWpl9++UVdu3ZV/fr19euvv2rkyJFyd3e3GXf37t2aMGGCqlatqgEDBsjd3V3ff/+9wsPD9csvv2jatGn3+1IBAABQzBEs/sK8vb21adMmqxWEPn36aOHChVqyZIl++OEHNWjQQDExMfrll180YsQIDRkyxNLXz89P06ZNU5UqVSxtly9f1pQpU9SgQQMtXLjQMnavXr1Uu3ZtzZw5U4mJifL3979/FwoAAIBijz0Wf2HOzs6WD/65ubnKyMhQenq6mjVrJkn64YcfJEm7du2So6Oj+vfvb3V+jx49VLp0aau2+Ph4nT17Vl27dlVWVpbS09Mt/z355JOWPgAAAMCdYMXiLy4qKkqff/65kpOTlZ+fb3UsMzNTkpSSkqIKFSqoVKlSVsednZ3l5eVl6SdJR44ckSS99957N53z7NmzRVU+AAAAHhIEi7+w1atXa9asWWrRooX69eunChUqyNnZWWfOnNGkSZNsgkZhmM1mSdIrr7yiOnXqFNinYsWKhuoGAADAw4dg8Re2efNmeXl5ac6cOXJw+L+n1v79739b9fPy8tK+ffuUk5NjtWqRm5ur1NRUlSlTxtLm6+srSXJ1dVXz5s3v8RUAAADgYcEei78wR0dHmUwmyyqDdC0sLF++3KrfU089pby8PH366adW7evXr1dWVpZVW8uWLVWuXDktX75cFy5csJnz0qVLys7OLrqLAAAAwEOBFYu/sPbt22vevHkaM2aM2rVrp+zsbG3bts3meyZ69Oih6OhoLVy4UCdOnLC8bnbHjh2qWrWq8vLyLH1dXV01efJkjR8/Xr169VK3bt1UtWpVZWZm6ujRo4qLi9M///lP3goFAACAO0Kw+AsbOHCgzGazYmJi9NFHH6l8+fLq0KGDunXrpt69e1v6ubi4aOHChZo9e7a+/vprbd++XQ0aNNCCBQs0depUXbp0yWrcli1basWKFVqxYoW2bNmi8+fPy93dXT4+Pnr++edVu3bt+32pAAAAKOZM5hufs8EDJS8vT88884waNGiguXPn3rN5TNNz79nYRcE8IeQeDr7h3o0NAABQjLDH4gHx51UJSfr888+VmZnJJm0AAADcczwK9YB4//33dfnyZT3++ONycXHR999/r61bt6pq1arq2bOnvcsDAADAA45g8YBo3ry5oqKitGTJEuXk5Kh8+fLq0aOHXn75Zbm5udm7PAAAADzg2GMBw9hjAQAAAPZYAAAAADCMYAEAAADAMB6FgmEREREKDQ2Vs7OzvUsBAACAnbBiAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCMYAEAAADAMIIFAAAAAMMIFgAAAAAMI1gAAAAAMMxkNpvN9i4CxZtpeu5dn2ueEFKElRQh8wZ7VwAAAFCssGIBAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCMYAEAAADAMIIFAAAAAMMIFv9fbGys/P39lZiYaO9Sbik1NVX+/v4KDw+3dykAAACABcECAAAAgGFO9i4Ad6ZKlSras2ePHB0d7V0KAAAAYEGwKGZMJpNKlChh7zIAAAAAKzwK9Sdms1mrVq1S9+7d1bJlSwUHB2vjxo02/TZs2KDnn39eTz75pNq2bauRI0fqv//9r1WfW+2HCA8Pl7+/v1JTUy1tp06d0uTJk9WlSxe1bNlSHTp00ODBg63mL2jMG9t27dqlF154Qa1atVLHjh01e/Zs5ebm2sz/r3/9S/3791erVq0UFBSkiIgIxcfHy9/fX7GxsXdz6wAAAPAQY8XiT+bPn6/Lly8rODhYLi4uWrdunSZNmiQfHx81atRIkjRnzhytXLlS9evXV1hYmHJycrR+/XoNHz5cH330kVq3bn3H8+bm5mrkyJE6c+aMQkJC5Ovrq6ysLB0+fFj/+c9/1KVLl9uOsWfPHq1bt069evVSt27d9PXXX2vVqlUqU6aMBg8ebOn35Zdf6q233pKPj4+GDh0qR0dHbdy4Ubt27brjugEAAACJYGHjypUrWrlypZydnSVJ7du3V/fu3fXZZ5+pUaNGOnr0qFatWqX/+Z//0aJFiyz9evTood69e2vatGlq2bLlHe+BOHLkiH7//XeNHj1agwYNuqvak5OT9dlnn8nLy0uS1KtXL/Xt21eRkZGWYJGbm6uZM2eqbNmyWrFihdzd3SVJISEh6t+//13NCwAAAPAo1J/07t3bEhYkqVKlSvL19dXx48clSV9//bXMZrNeeOEFq34VK1ZU165ddfLkSR06dOiO5y1durQkKSkpSefOnbur2gMCAiyhQrq2H8Pf319nz55VTk6OJOnnn3/WmTNn1KVLF0uokKRSpUopODj4ruYFAAAACBZ/4u3tbdPm4eGhCxcuSJJlT0StWrVs+l1vS0lJueN5q1SposGDB2vv3r3q1KmTBgwYoNmzZ+vHH380XLskS/3Xa6tWrZpN34LaAAAAgMIgWPyJg0PBt8RsNt/xWCaT6abH8vLybNrCwsIUHR2t1157TT4+PoqJidGgQYM0Z86cQs13s9qlu6sfAAAAKCyCxR26virw22+/2RxLTk626nP9UaOMjAybvjdb1fDx8VG/fv304YcfasuWLWrSpIlWrlx5149H/dn1R6V+//13m2MFtQEAAACFQbC4Q23atJHJZNKqVausXuOalpam2NhYValSRXXr1pUkubm5qXz58kpISLBaMThx4oR27txpNW5WVpbNa2FLlCih6tWrSyo4nNyNxx57TBUqVNDGjRutxszJyVF0dHSRzAEAAICHD2+FukPVq1fXwIEDtXLlSg0dOlQdOnSwvG42JydHU6ZMsXojVJ8+fbRw4UKNGTNGbdu2VVpamj7//HPVqlVLBw8etPRLTEzU+++/r6efflrVqlVTqVKl9NNPPykmJkYNGjSwBAyjnJyc9Oqrr+rtt9/WoEGD1L17dzk6Oio2NlYeHh5KSUm55SNcAAAAQEEIFndhzJgxqlq1qqKiojRv3jw5Ozurfv36mjp1qho3bmzVd9CgQcrKytLmzZuVlJSkGjVq6J133tFPP/1kFSxq166tdu3aKSkpSVu3blVeXp4eeeQRhYaGasCAAUVaf6dOneTk5KSPP/5Y4eHhKleunLp3767atWtrwoQJfLM3AAAA7pjJzK5e/H+rV6/WrFmztGzZMjVs2LDQ55mm236zd2GZJ4Tc9bn3lHmDvSsAAAAoVthj8RC6evWqzVupcnJyFBUVJQ8PDz366KN2qgwAAADFFY9CPYRSUlI0ZswYPfvss/Ly8lJaWpo2bdqklJQUvfnmm1Zf/AcAAAAUBsHiIeTp6akGDRpoy5YtOn/+vBwdHeXn56dRo0apQ4cO9i4PAAAAxRB7LGAYeywAAADAHgsAAAAAhhEsAAAAABhGsAAAAABgGHssYFhERIRCQ0N5mxQAAMBDjBULAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGCYyWw2m+1dBIo30/Tc+zqfeUJIEQ20oWjGAQAAACsWAAAAAIwjWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCMYAEAAADAMILFQyg8PFz+/v5KTU21dykAAAB4QBAsHlCJiYkKDw9XZmamvUsBAADAQ4Bg8YBKSkrS4sWLCRYAAAC4LwgWAAAAAAwjWBgQGxsrf39/7du3T4sXL1aXLl305JNPatCgQfr+++8lXVs5GDJkiFq3bq2OHTvq448/thln586dGjx4sFq3bq2nnnpKgwcP1s6dO236de3aVcOGDdPRo0f1yiuvqE2bNmrbtq1ef/11paWlWfpNmjRJixcvliR169ZN/v7+8vf3V3h4uNV4V65c0fz58xUYGKiWLVuqf//+2r17dxHeIQAAADwsnOxdwINg3rx5ysvLU79+/ZSbm6vVq1dr1KhRmjx5sqZMmaKePXuqc+fO2r59uxYtWiQvLy8FBgZKkqKiojRt2jRVr15dL730kiRp48aNGj9+vCZOnKjg4GCruc6cOaPhw4crICBAY8aM0a+//qro6GhlZ2dr/vz5kqTg4GBlZ2crLi5Or732mjw9PSVJtWvXthpr0qRJcnJy0oABA3T16lV9+umnGj9+vKKjo+Xl5XWP7xoAAAAeJASLIpCXl6fly5fL2dlZklSjRg2NGzdOb7zxhpYtW6Z69epJkrp3764uXbooKipKgYGBysjI0Jw5c+Tj46Ply5erdOnSkqSQkBA9//zzmjVrljp06KAyZcpY5jp+/Lg++OADdejQwdLm4OCgqKgoHT16VNWrV9fjjz8uPz8/xcXFKSAg4KYhwdPTUzNnzpTJZJIk+fv7a9CgQYqOjtaoUaPuyb0CAADAg4lHoYpASEiIJVRIUuPGjSVJDRo0sIQKSXJ2dlb9+vV17NgxSVJ8fLwuXryofv36WUKFJJUuXVr9+vVTTk6O4uPjreaqWLGiVaiQrgUC6VrouBP9+vWzhApJql+/vkqVKmWpDwAAACgsgkUR8Pb2tvrZ3d1dkgpcKXB3d9eFCxckSSkpKZKkmjVr2vS73na9z83mkiQPDw9JsoxbWD4+PgWOdafjAAAAAASLIuDgUPBtdHR0vG9zSZLZbC6Sse50HAAAAIBgYUfXVwySk5Ntjh05ckRSwSsUhXHjI04AAADAvUawsKPmzZvL1dVVkZGRys7OtrRnZ2crMjJSpUqVUosWLe5q7FKlSkmSMjIyiqRWAAAA4FZ4K5QdlSlTRmPGjNG0adP04osvqkuXLpKuvW72+PHjmjhxotWm7jvRoEEDSdKcOXPUuXNnubi4qFatWvLz8yuy+gEAAIDrCBZ21rt3b1WoUEGrVq2yfKldnTp1NH36dAUEBNz1uI0aNdLo0aMVHR2tqVOnKi8vT0OHDiVYAAAA4J4wmdmpC4NM03Pv63zmCSFFNNCGohkHAAAA7LEAAAAAYBzBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGG8bhaGRUREKDQ0VM7OzvYuBQAAAHbCigUAAAAAwwgWAAAAAAwjWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCMYAEAAADAMIIFAAAAAMMIFgAAAAAMI1gAAAAAMIxgAQAAAMAwk9lsNtu7CBRvpum5RTaWeUJIkY1180k23Ps5AAAAHjKsWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCMYAEAAADAMIIFAAAAAMMIFn9BmZmZCg8PV2Ji4gM5HwAAAB48BIu/oMzMTC1evFhJSUkP5HwAAAB48BAsAAAAABhmMpvNZnsX8Vd09epVrVmzRtu2bdPvv/8uJycn+fr6qkuXLurbt6+lX2pqqhYuXKj4+HhlZmaqUqVKevbZZzVkyBCVLFnS0i88PFyLFy/WunXrtGnTJm3atEnnz59X9erVNXLkSLVu3VqSlJiYqJdfftmmnipVqig2NlaSFBUVpZ07dyo5OVnnz5+Xh4eHmjVrphEjRsjLy8vm3MTERK1atUo//PCDLl68qIoVK+qJJ57QmDFjdPjw4dvOdzum6bmF6lcY5gkhRTbWzSfZcO/nAAAAeMg42buAv6KrV69q1KhRSkpKUosWLdS5c2e5uLjo8OHDiouLswSLkydPatCgQcrKylJISIh8fX2VlJSkZcuW6cCBA1qwYIGcnKxv8aRJk+Tk5KQBAwbo6tWr+vTTTzV+/HhFR0fLy8tLNWrU0GuvvaYZM2aoXbt2ateunSSpVKlSljFWr16tBg0aqG/fvvLw8NBvv/2mDRs2KCEhQWvXrpWnp6el7+eff64PP/xQlSpVUq9evVSlShWdOnVKu3bt0unTpws1HwAAAHA7BIsCrFmzRklJSQoNDdXIkSOtjuXn51v+PX/+fJ0/f16zZs2yrDj07t1bs2fP1qpVq7Rx40b16NHD6nxPT0/NnDlTJpNJkuTv769BgwYpOjpao0aNUvny5RUQEKAZM2bIz89PgYGBNvWtXbtWrq6uVm1t2rRRWFiYYmJiNGjQIEnS6dOnNX36dFWvXl1Lly5VmTJlLP1HjBih/Px8OTg43HY+AAAA4HbYY1GArVu3yt3dXS+99JLNMQeHa7csPz9f33zzjerWrWsJFde9+OKLcnBw0M6dO23O79evnyVUSFL9+vVVqlQpHTt2rND1XQ8V+fn5ysrKUnp6uurUqaPSpUvrhx9+sPTbsWOHrl69qqFDh1qFij9fCwAAAGAUKxYFOHbsmOrWrasSJUrctM/58+eVk5OjmjVr2hzz8PBQhQoVlJKSYnPMx8enwP4XLlwodH0JCQlavHixfvzxR12+fNnqWGZmpuXfx48flyTVrVu30GMDAAAAd4NgcZ/dbJWgsHvof/zxR40aNUo+Pj4aNWqUvLy8VKJECZlMJk2cONHqUS0AAADgfiFYFKBatWo6evSorly5IhcXlwL7lC1bVm5ubkpOTrY5lpGRobS0NNWpU+eu5r/xUak/27p1q/Ly8jRnzhx5e3tb2i9evGi1WiFJvr6+kqRffvlF1apVu6v5AAAAgMLgIfsCdOrUSRkZGVqyZInNsesrCw4ODnrqqad06NAh/fvf/7bqs3z5cuXn5ysgIOCu5r++hyIjI8PmmKOjo1Ud1y1dutRmtaJ9+/ZydnbW4sWLlZWVddNrudV8AAAAQGGwYlGA/v37a9euXVqyZIkOHjyo5s2bq0SJEkpOTtbvv/+uBQsWSJJGjhyp+Ph4jR8/XiEhIapatar279+v7du3q0mTJurSpctdze/p6amqVavqyy+/lI+Pj8qVKydXV1e1adNGAQEBWrNmjV555RX17NlTzs7Oio+P1+HDh61eMytJlStX1rhx4zRt2jT169dPQUFBqlKliv744w99/fXXevfdd1W3bt1bzgcAAAAUBsGiAM7Ozpo3b55Wr16tbdu2acGCBXJxcZGvr6+6du1q6VelShUtX75cixYt0pYtW5SZmanKlSsrNDRUQ4YMsfkOizsxZcoUzZgxQ/Pnz9elS5dUpUoVtWnTRo0aNdI//vEPffzxx1q0aJFKlCihZs2aKSIiQkOHDrUZJyQkRD4+Plq5cqXWrl2rq1evqmLFimratKkqV6582/kAAACAwuCbt2EY37wNAAAA9lgAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGABAAAAwDCCBQAAAADD+B4LGBYREaHQ0FA5OzvbuxQAAADYCSsWAAAAAAwjWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCMYAEAAADAMIIFAAAAAMMIFgAAAAAMI1gAAAAAMIxgAQAAAMAwk9lsNtu7CBRvpum59i7BinlCyE0ObLivdQAAADxMWLEAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESz+YmJjY+Xv76/ExER7lwIAAAAUGsECAAAAgGEECwAAAACGESwAAAAAGOZk7wIeJLGxsZo8ebLmz5+v//73v4qNjdXZs2dVrVo1hYaGqmPHjlb9169fr9WrVys1NVWVK1dWnz59VLp0aZtxz5w5o9WrVyshIUEnT57U5cuX5e3traCgIA0cOFCOjo6SpLi4OE2YMEFvvfWWevbsaTNOnz59dOXKFa1fv14mk0m//fabIiIi9N133yk9PV3u7u6qXr26Bg4cqNatW9+bmwQAAIAHEsHiHpg7d64uXryokJAQSdcCx1tvvaUrV66oa9eukqQ1a9ZoxowZqlOnjkaOHKlLly5p9erVKlu2rM14v/76q+Li4hQQECAfHx/l5ubq22+/1bx585SSkqK33npLkvTUU0+pfPny+uKLL2yCxffff6/k5GSFhYXJZDIpPT1dI0aMkCT16tVLjzzyiNLT0/XTTz/phx9+IFgAAADgjhAs7oH09HStXbvWsvoQEhKifv36aebMmerQoYOuXr2qBQsWqEaNGlq6dKlKliwpSeratasljNyoSZMmiomJkclksrQ999xzeueddxQTE6Phw4erQoUKcnJyUrdu3bRs2TIlJyerZs2alv4xMTFydHS0BJsDBw7o3Llz+uCDD9ShQ4d7eTsAAADwEGCPxT0QEhJi9UhT6dKl1atXL2VkZCgpKUl79+7VpUuX1Lt3b0uokKTKlSurU6dONuOVLFnSEiquXr2qCxcuKD09XS1btlR+fr4OHjxo6dujRw+ZTCbFxMRY2i5evKjt27erVatWqlixoqUmSfr3v/+trKysor0BAAAAeOiwYnEPVK9e3aatRo0akqSUlBTl5OTctN+NqwzX5ebmavny5dq8ebOOHz8us9lsdTwjI8Pyb29vbzVr1kybN2/W6NGj5eTkpO3btys7O1vdu3e39HviiScUFBSk2NhYbdmyRfXq1VPz5s3VoUOHAmsAAAAAboUVi2Jg5syZWrRokerWrau///3vmj17tubPn6/Ro0dLkk3Q6Nmzp86fP6+vv/5a0rXHoMqXL2+zb2Ly5Mlau3atwsLC5OHhodWrV6t///6KjIy8PxcGAACABwYrFvfA0aNHbdqOHDki6dqKwvUVi6NHj6pZs2ZW/ZKTk23O3bx5s5o0aaIPPvjAqv348eMFzh8QEKBy5copJiZGtWrV0oEDBzRo0CA5Odn+uv38/OTn56cXXnhBmZmZGjRokObNm6c+ffpY7ekAAAAAboUVi3tg3bp1VvsWsrKy9Pnnn6tMmTJ64okn1Lx5c5UoUUJRUVG6dOmSpd/p06e1bds2m/EcHBxsViUuXryoNWvWFDi/k5OTunTpor1792rx4sWSZPUYlCRduHBB+fn5Vm1lypSRt7e3Ll26pMuXL9/ZRQMAAOChxorFPeDp6alBgwZZ3sAUGxurU6dO6e2331bJkiVVsmRJjRgxQrNmzdLgwYMVGBioS5cuKTo6WlWrVtWhQ4esxmvfvr2io6P1t7/9Tc2aNdPZs2cVGxsrDw+Pm9bQs2dPrVq1Stu2bVOTJk3k6+trdXzTpk1as2aN2rVrJx8fHzk5OWn//v369ttv1aFDB6tN5QAAAMDtECzugdGjR+u///2voqKidO7cOfn6+mrq1KlWb3waMGCAXF1d9cknn2j+/PmqXLmyBgwYoNKlS+u9996zGu+1116Tm5ubtm/frq+//lqVK1dWz549Va9ePYWFhRVYQ9WqVeXv76+EhASb1Qrp2ubtQ4cOadeuXUpLS5Ojo6O8vLz06quvqk+fPkV7QwAAAPDAM5n//IwN7tr1b95etGiR/P397V2OxowZo++//15btmy5pysQpum592zsu2GeYPtdINcObLivdQAAADxM2GPxgDp+/Lj27t2rzp0781gTAAAA7jkehXrA/PDDDzpy5IjWrl0rZ2dnDRgwwN4lAQAA4CFAsHjArFu3Tps2bZK3t7emTJkiLy8ve5cEAACAhwB7LGAYeywAAADAHgsAAAAAhhEsAAAAABjGo1AwLCIiQqGhoXJ2drZ3KQAAALATViwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhBAsAAAAAhhEsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGmcxms9neRaB4M03PvSfjmieEFNFAG4pmHAAAANwUKxYAAAAADCNYAAAAADCMYAEAAADAMIIFAAAAAMMIFgAAAAAMI1gAAAAAMIxgUcykpqbK399f4eHh9i4FAAAAsCBYAAAAADDMyd4F4M5UqVJFe/bskaOjo71LAQAAACwIFsWMyWRSiRIl7F0GAAAAYKXYB4urV69qzZo12rZtm37//Xc5OTnJ19dXXbp0Ud++fSVJZ86c0erVq5WQkKCTJ0/q8uXL8vb2VlBQkAYOHGj11//Y2FhNnjxZCxYs0IEDBxQTE6Pz58/Lz89P48ePV8OGDZWUlKQFCxbo0KFDcnNzU+/evfXSSy9Z1dW1a1dVqVJFr732mmbNmqUff/xRzs7Oeuqpp/TKK6+oXLlylr7Z2dlasWKF4uPjdeLECeXk5Khy5cpq3769hg4dqpIlS1r6pqamqlu3bho6dKiGDx9uab906ZIWLFigbdu2KSsrS7Vr11ZYWJg2b96sjRs3KjEx0dJ32LBhOnnypJYuXaqZM2fq22+/1ZUrV9S4cWNNmDBB1apVK/LfEwAAAB5sxTpYXL16VaNGjVJSUpJatGihzp07y8XFRYcPH1ZcXJwlWPz666+Ki4tTQECAfHx8lJubq2+//Vbz5s1TSkqK3nrrLZux582bp7y8PPXr10+5ublavXq1Ro0apcmTJ2vKlCnq2bOnOnfurO3bt2vRokXy8vJSYGCg1Rh//PGHRowYoaefflrt27fXzz//rC+++EI//fSTVq5caQkMZ86cUUxMjJ5++ml16tRJjo6O2r9/v1auXKlDhw5p3rx5t70Xb7zxhvbs2aOAgAA1a9ZMqampmjBhgry8vArsf/HiRQ0dOlQNGzbUyJEjlZKSorVr12rcuHGKjIzkUSsAAADckWIdLNasWaOkpCSFhoZq5MiRVsfy8/Mt/27SpIliYmJkMpksbc8995zeeecdxcTEaPjw4apQoYLV+Xl5eVq+fLmcnZ0lSTVq1NC4ceP0xhtvaNmyZapXr54kqXv37urSpYuioqJsgsWJEyf02muv6bnnnrO01axZUzNnztTatWv14osvSpK8vb21adMmOTn936+jT58+WrhwoZYsWaIffvhBDRo0uOl92L17t/bs2aMePXro7bfftrT7+/vr1VdfLfCc9PR0DRw4UIMGDbK0lS1bVnPmzNG+ffvUsmXLm84HAAAA/FmxfivU1q1b5e7ubvMYkiQ5OPzfpZUsWdISKq5evaoLFy4oPT1dLVu2VH5+vg4ePGhzfkhIiCVUSFLjxo0lSQ0aNLCECklydnZW/fr1dezYMZsxrj8mdaPevXvLzc1NcXFxVmNcDxW5ubnKyMhQenq6mjVrJkn64Ycfbnkfdu3aJUl6/vnnrdpbt26tGjVqFHiOg4OD+vXrZ9XWtGlTSSrwWgAAAIBbKdYrFseOHVPdunVvu5k5NzdXy5cv1+bNm3X8+HGZzWar4xkZGTbneHt7W/3s7u4uSQU+WuTu7q4LFy4UOMaN4USSXFxc5O3trZSUFKv2qKgoff7550pOTrZabZGkzMzMW1zdtX0XDg4Oqlq1qs2xatWq6ciRIzbtFStWtLlvHh4eklTgtQAAAAC3UqyDRWHNnDlTkZGR6tChgwYPHqyyZcvKyclJP//8s+bOnWsTNCTrFY8b3Yu9B6tXr9asWbPUokUL9evXTxUqVJCzs7POnDmjSZMm2QSNonCz65NU4P0AAAAAbqVYB4tq1arp6NGjunLlilxcXG7ab/PmzWrSpIk++OADq/bjx4/f0/pSUlJ09epVq1WLK1euKCUlRdWrV7eqz8vLS3PmzLH6wP/vf/+7UPNUqVJF+fn5On78uM2jT7///ruxiwAAAAAKoVjvsejUqZMyMjK0ZMkSm2M3/tXdwcHB5q/wFy9e1Jo1a+5pfdnZ2YqKirJqi4qKUnZ2tgICAixtjo6OMplMVjVef3yrMNq0aSNJNteze/fuAh+DAgAAAIpasV6x6N+/v3bt2qUlS5bo4MGDat68uUqUKKHk5GT9/vvvWrBggSSpffv2io6O1t/+9jc1a9ZMZ8+eVWxsrGVPwb3i4+OjxYsX67ffftNjjz2mn376SV988YWqV69utXG6ffv2mjdvnsaMGaN27dopOztb27Zts3pL1K08+eSTatmypdavX2/Z9J2amqro6GjVrl1bv/766726RAAAAEBSMQ8Wzs7OmjdvnlavXq1t27ZpwYIFcnFxka+vr7p27Wrp99prr8nNzU3bt2/X119/rcqVK6tnz56qV6+ewsLC7ll9lSpV0ocffqhZs2Zp27ZtcnZ2VqdOnfTqq6/K1dXV0m/gwIEym82KiYnRRx99pPLly6tDhw7q1q2bzVulCmIymfSPf/zD8gV5//73v+Xn56fp06crKiqKtzwBAADgnjOZ2al7T1z/5u2IiAi71tG3b1/l5ubq888/v2dzmKbn3pNxzRNCimigDUUzDgAAAG6qWO+xwP+5dOmSTdvu3bv122+/qXnz5naoCAAAAA+TYv0oFP7Pxx9/rEOHDumJJ55Q6dKl9csvv+iLL76Qh4eH1bdrAwAAAPcCweIB0ahRIx04cECrVq1SVlaWPDw89PTTT2vEiBGqXLmyvcsDAADAA449FjCMPRYAAABgjwUAAAAAwwgWAAAAAAwjWAAAAAAwjD0WMCwiIkKhoaFydna2dykAAACwE1YsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIYRLAAAAAAYRrAAAAAAYBjBAgAAAIBhJrPZbLZ3ESjeTNNz7+n45gkhBk7eUGR1AAAA4OZYsQAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwQIAAACAYQQLAAAAAIYRLO5S165dNWzYMLvN7+/vr0mTJhWqb3h4uPz9/ZWamnpviwIAAMBDi2ABAAAAwDAnexdQXH3++ecymUx2m3/Pnj1ydHS02/wAAADAjQgWdyA3N1d5eXkqUaKEXFxc7FpLiRIl7Do/AAAAcKM7DhZXr17VmjVrtG3bNv3+++9ycnKSr6+vunTpor59+1r6paamauHChYqPj1dmZqYqVaqkZ599VkOGDFHJkiUt/cLDw7V48WKtW7dOmzZt0qZNm3T+/HlVr15dI0eOVOvWra3m37hxoz777DMdO3ZMubm5Kl++vBo2bKhx48apbNmyt6zd399fXbp0UefOnbVw4UL9+uuvKl26tDp06KCwsDCVKlXKpq7IyEjFxMRox44dSktL04IFC+Tv76+uXbuqSpUqioiIsJrj559/1rJly/Sf//xHmZmZKleunP7nf/5HYWFh8vHxsfSLj4/XypUr9eOPP+rKlSvy9fVVSEiIQkJCCvV7uH4tN+6zyM/P14oVK7R+/XqlpaXJx8dHoaGhNueePn1azz33nMqXL6+VK1da/T7efvttbdu2TfPmzVPz5s0LVQsAAABwR8Hi6tWrGjVqlJKSktSiRQt17txZLi4uOnz4sOLi4izB4uTJkxo0aJCysrIUEhIiX19fJSUladmyZTpw4IAWLFggJyfrqSdNmiQnJycNGDBAV69e1aeffqrx48crOjpaXl5ekqRNmzZp0qRJaty4sV5++WWVKFFCp0+f1p49e3Tu3LnbBgvp2gf/f/3rX+rRo4eCgoKUmJiotWvX6rffftP8+fPl4GC97eSdd95RiRIl9Pzzz8tkMqlChQo3HXvXrl16/fXX5erqqu7du6tq1ao6e/asvv32Wx0+fNgSLKKjo/XBBx+oYcOGGjx4sFxdXRUfH68PP/xQKSkpeuWVV+7k12Ixc+ZMffrpp2rSpImee+45nTt3TtOmTZO3t7dVv8qVK+vdd9/VuHHj9NFHH+mtt96SJMXExGjr1q168cUXCRUAAAC4I3cULNasWaOkpCSFhoZq5MiRVsfy8/Mt/54/f77Onz+vWbNmWVYcevfurdmzZ2vVqlXauHGjevToYXW+p6enZs6cadm34O/vr0GDBik6OlqjRo2SJO3cuVNubm5auHChVTB5+eWXC30Nhw8f1vTp0xUQEGCpa/r06Vq7dq22b9+ujh07WvUvXbp0gUHozy5duqTJkyerdOnS+uSTT1SpUiXLsaFDh1ruT1pamqZPn65nn31W77//vqXP9To++eQT9erVy2p1ozCOHj2qtWvXqmnTppo3b55l/8XTTz+tgQMH2vRv27at+vbtq8jISDVv3lx+fn765z//qYYNG97R/QQAAACkO3wr1NatW+Xu7q6XXnrJdqD//5f+/Px8ffPNN6pbt67NY0wvvviiHBwctHPnTpvz+/XrZ7UZun79+ipVqpSOHTtmaStdurQuXbqk3bt3y2w230npFtWqVbOEihvrklRgXc8999xtQ4Ukffvtt0pPT9fzzz9vFSquu35/duzYoStXrqh79+5KT0+3+u+pp55Sfn6+9u3bd8fX9fXXX8tsNuv555+32tT96KOP3nT14ZVXXlHdunX1/vvv6/XXX5eTk5Pef//9Ql0vAAAAcKM7+gR57Ngx1a1b95Ybh8+fP6+cnBzVrFnT5piHh4cqVKiglJQUm2MF/YXew8NDFy5csPwcGhqq/fv3a/z48fLw8FCTJk305JNPqkOHDnJzcyvUNdSoUcOmrUKFCipTpkyBdfn6+hZq3OsB6NFHH71lv6NHj0qSwsLCbtrn3LlzhZrzRtdrr169us2xGjVqaO/evTbtLi4uev/999W3b18lJydr6tSplsfOAAAAgDvxl/nT9J/3Nlx348qEr6+voqKitG/fPiUkJGj//v2aOnWqZaP1nT4+VBg3bmwuCtevZ/LkyTfdr/HnPRH30u7du5WXlydJOnTokDp16nTf5gYAAMCD446CRbVq1XT06FFduXLlpq9bLVu2rNzc3JScnGxzLCMjQ2lpaapTp87dVatrf2Vv3bq15TGr3bt369VXX9Unn3yiN95447bnHzlyxKYtLS1NmZmZhj7QV6tWTdK1D+ctWrS4ab+qVatKuranpCg3SF+v/ejRozYBq6BrlqSffvpJ8+fPV/PmzeXp6anVq1erefPmt6wfAAAAKMgd7bHo1KmTMjIytGTJEptj1/8S7+DgoKeeekqHDh3Sv//9b6s+y5cvV35+vs0eh8JKT0+3abv+6NGNj0ylp6fr6NGjysrKsun/+++/2+ylWLFihaRrG5rvVosWLeTp6alPPvlEaWlpNsev358OHTrIxcVF4eHhunTpkk2/rKwsXblyxfJzWlqajh49WmDfG7Vt21Ymk0mffPKJZQVCuvYWrIL2bOTk5GjixIlyd3fXe++9p7/97W/y8vLS3//+97t6FAsAAAAPtztasejfv7927dqlJUuW6ODBg2revLlKlCih5ORk/f7771qwYIEkaeTIkYqPj9f48eMVEhKiqlWrav/+/dq+fbuaNGmiLl263FWxI0eOVJkyZdS4cWNVrlxZmZmZio2NlclkUmBgoKVfZGSkFi9erL///e/q2rWr1Rh+fn5655131KNHD/n6+ioxMVH/+te/1KRJEz377LN3VZd07ZGpd955R2+88Yb69u1red3s+fPntXfvXj333HMKCAhQ5cqV9eabb2rq1Knq3bu3AgMDVaVKFZ0/f16HDx/Wzp07FRUVZdnrMG/ePG3cuFGLFi2Sv7//TeevXr26evfurc8++0wjRozQ008/rXPnzumzzz5T7dq1dejQIav+H3zwgU6cOKG5c+eqfPnykqT3339fL730kiZNmqTZs2fb9ZvFAQAAULzcUbBwdnbWvHnztHr1am3btk0LFiyQi4uLfH19rT7AV6lSRcuXL9eiRYu0ZcsWZWZmqnLlygoNDdWQIUPu+q1DISEh2r59u6Kjo3XhwgV5eHiobt26ev3112/5oftGjz76qMaOHasFCxYoOjpabm5u6tOnj0aOHHnTfR6F1bZtW3388cdatmyZYmJilJOTo3Llyqlx48by8/Oz9OvWrZt8fX21evVqRUdHKzMzU56enqpWrZpGjBhh+aB/p8aPH6/y5ctr/fr1mj17tqpWrao33nhDx44dswoWGzdu1JYtW/TCCy9YPfbUoEEDhYWFac6cOfrkk080YMCAu78ZAAAAeKiYzHf73tZiqKBvq4Zxpum593R884TCfRt5wSdvKLI6AAAAcHPG/kQPAAAAACJYAAAAACgCBAsAAAAAhv1lviDvfkhMTLR3CQAAAMADiRULAAAAAIYRLAAAAAAY9lC9bhb3RkREhEJDQ+Xs7GzvUgAAAGAnrFgAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADCMYAEAAADAMIIFAAAAAMMIFgAAAAAMM5nNZrO9i0DxZpqee8fnmCeE3INK/jzJhns/BwAAACSxYgEAAACgCBAsAAAAABhGsAAAAABgGMECAAAAgGEECwAAAACGESwAAAAAGEawAAAAAGAYwcKgxMREhYeHKzMz096lAAAAAHZDsDAoKSlJixcvJlgAAADgoUawAAAAAGBYsQ4WsbGx8vf3V3x8vMLDw9WlSxe1bNlS/fr107Zt22z679y5U4MHD1br1q311FNPafDgwdq5c6dNP39/f02aNOmm8yUmJkqSJk2apMWLF0uSunXrJn9/f/n7+ys8PNxyTlZWlubPn6+QkBC1atVK7du315AhQ2zq+/XXXzV+/Hi1b99erVq1Uu/evbVixQrl5eVZ9Zs0aZL8/f2Vnp6uSZMmqX379mrTpo3GjRuntLQ0SVJ0dLRlvl69ehV4jZL05ZdfasiQIWrTpo2efPJJDRo0SDt27Ljp/QYAAABuxsneBRSFuXPn6uLFiwoJCZF0LQC89dZbunLlirp27SpJioqK0rRp01S9enW99NJLkqSNGzdq/PjxmjhxooKDg+943uDgYGVnZysuLk6vvfaaPD09JUm1a9eWJGVmZmrIkCFKTk5W+/btFRISory8PB06dEi7d+9Wx44dJUkHDx7UsGHD5OTkpN69e6t8+fLatWuX5s6dq19//VVTp061mXvMmDGqVKmSXn75ZR0/flyRkZGaMGGC2rVrp/Xr16t79+5ycXFRZGSk3njjDUVHR8vb29ty/oIFC7R06VK1atVKL7/8shwcHBQXF6c333xTr7/+uvr06XPH9wMAAAAPrwciWKSnp2vt2rUqXbq0JCkkJET9+vXTzJkz1aFDB125ckVz5syRj4+Pli9fbtXv+eef16xZs9ShQweVKVPmjuZ9/PHH5efnp7i4OAUEBMjLy8vq+Pz585WcnFxgcMnPz7f8e/r06bp69aqWLVtmCSV9+/bV3/72N23dulXdunVTs2bNrM6vX7++3njjDau2NWvW6I8//lBkZKTlGps2bar+/ftr/fr1GjVqlCTp559/1tKlSxUaGqqRI0dazu/Xr5/GjRun+fPnKygoSG5ubnd0PwAAAPDwKtaPQl0XEhJi+SAtSaVLl1avXr2UkZGhpKQkxcfH6+LFi+rXr59Nv379+iknJ0fx8fFFWlN+fr6+/PJL1ahRo8DVEAeHa7f+3Llz+u6779SmTRtLqJAkk8mkwYMHS5Li4uJszu/fv7/Vz40bN5YkBQUFWV1j7dq15ebmpmPHjlnatmzZIpPJpKCgIKWnp1v916ZNG2VnZ+v77783cPUAAAB42DwQKxbVq1e3aatRo4YkKSUlRTk5OZKkmjVr2vS73paSklKkNaWnpysjI0MtW7a8Zb/U1NSb1lajRg05ODgUWNuNjzVJsqy2/HnVRJLc3d114cIFy89HjhyR2Wy2PDpWkLNnz96ybgAAAOBGD0SwuF/+vJHanhwdHe+o3Ww2W/1sMpk0Z84cy8rJn9WqVctYgQAAAHioPBDB4ujRozZtR44ckXTtL/sXL16UJCUnJ9vsVbix33UeHh5Wf+G/rqCVA5PJVGBNnp6ecnd316+//nrL2q+vMCQnJ9scO3r0qPLz821WJ4yqWrWq/v3vf+uRRx6xrOwAAAAARjwQeyzWrVunrKwsy89ZWVn6/PPPVaZMGT3xxBNq3ry5XF1dFRkZqezsbEu/7OxsRUZGqlSpUmrRooWl3dfXV99//70uXbpkacvIyNAXX3xhM3epUqUsx2/k4OCgjh07Kjk5WRs2bLA57/oKQrly5fT444/rm2++0eHDh62OL1u2TJLUrl27O7kdtxUYGCjp2ubyglZheAwKAAAAd+qBWLHw9PTUoEGDLK+WjY2N1alTp/T222+rZMmSKlmypMaMGaNp06bpxRdfVJcuXSRde93s8ePHNXHiRKsNz3369NE777yjl19+WYGBgcrMzNSGDRtUpUoVmw/dDRo0kCTNmTNHnTt3louLi2rVqiU/Pz+NGDFCCQkJmjp1quLj4/U///M/kqRDhw4pNzdXU6ZMkSSNHz9ew4YN09ChQy2vm929e7e+/fZbderUyWaVxaj69etr2LBhioiI0HPPPadnnnlGFStWVFpamn766Sft2bNHe/fuLdI5AQAA8GB7IILF6NGj9d///ldRUVE6d+6cfH19NXXqVHXq1MnSp3fv3qpQoYJWrVpl+VK7OnXqaPr06QoICLAar3Pnzjpz5ow+++wzzZw5U97e3nrppZfk4OCgH374wapvo0aNNHr0aEVHR2vq1KnKy8vT0KFD5efnJ3d3dy1btkxLly5VXFyc4uLi5Obmpho1aqhv376WMerVq6elS5cqPDxc69at08WLF+Xt7a3Ro0drwIAB9+SeDRs2TPXq1dPatWv16aef6uLFiypXrpxq1aql8ePH35M5AQAA8OAymf+8q7cYiY2N1eTJk7Vo0SL5+/vbu5yHlml67h2fY55w8zdSFRnzhns/BwAAACQ9IHssAAAAANgXwQIAAACAYQQLAAAAAIYV6z0W+GtgjwUAAABYsQAAAABgGMECAAAAgGEECwAAAACGsccChkVERCg0NFTOzs72LgUAAAB2wooFAAAAAMMIFgAAAAAMI1gAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAwjWAAAAAAwjGABAAAAwDCCBQAAAADDCBYAAAAADCNYAAAAADDMZDabzfYuAsWbaXruPR3fPCHkDk/YcE/qAAAAwM2xYgEAAADAMIIFAAAAAMMIFgAAAAAMI1gAAAAAMIxgAQAAAMAwggUAAAAAwwgWAAAAAAy7r8Gia9euGjZs2P2csshcunRJ//znPxUUFKRmzZqpa9eu9i7JkGHDhhX7awAAAMBfh5O9CyguVqxYocjISA0cOFB+fn5yc3Ozd0kAAADAXwbBopDi4+Pl5+enV155xd6lAAAAAH857LEopLNnz8rd3d3eZQAAAAB/SfckWJw6dUpvvvmm2rZtq7Zt22rs2LE6ceJEgX2//PJLjR07VkFBQWrZsqXat2+vcePG6ddff7Xq179/fwUFBSk/P99mjB07dsjf318bN268ozpzc3O1fPly9e7dW61atVL79u01fvx4HT582NInNjZW/v7+SklJ0f79++Xv7y9/f3+Fh4cXao6IiAjL+delpaXJ399fTZs21YULFyztR44ckb+/v5YvX241Rnx8vEaOHKmAgAC1atVK/fr107p16wqc7+DBgxo/frzat2+vli1bKjg4WEuWLFFubu5ta01PT1doaKjatm2rffv2Fer6AAAAAOkeBIvMzEwNGzZMcXFxCgwM1KhRo1SyZEkNHz5cFy9etOn/2WefycHBQT179tQbb7yhnj176r///a+GDBmiY8eOWfr16NFDp0+fVnx8vM0YMTExKl26tJ555pk7qvWdd97RvHnzVKlSJY0ZM0bBwcFKTExUaGiofv75Z0lS48aN9d5778nT01PVq1fXe++9p/fee09PP/10oeZo2rSpJCkhIcHStm/fPjk4OMhsNisxMdHSfr3P9XMkKTo6WqNGjdLFixc1ePBgjR07Vj4+Pvrwww81e/Zsq7l2795tuW8DBgzQ+PHj9fjjjys8PFxvvfXWLetMSUnR4MGDdfLkSUVERKhZs2aFuj4AAABAugd7LFauXKnU1FS9++676tatmySpd+/e+uijj/Tpp5/a9J87d65cXV2t2oKCgvTcc89pzZo1evPNNyVJgYGBmjNnjmJiYtSyZUtL31OnTik+Pl7BwcEqWbJkoevcu3evtm/frg4dOuh///d/ZTKZJEkdOnTQwIEDNX36dH388cfy8fGRj4+PFi5cqHLlyikwMPCO7kfDhg1VsmRJJSYmqkePHpKuBYg6dero8uXLSkhIUPv27S3tpUuX1qOPPirp2srG9OnT9eyzz+r999+3jNm7d+//1959x0Vx/P8Dfx3lqAdYUEGkKGDBiigqxI4VsYDdqNgiYtQomtgVa+zEkohRUMRPFOsXiTVC/GgUFSOKNQawV5SmdPb3h7/bj+cdCByICa/n48FDmZ2dnZ2bW+59MzuLVatWITQ0FJ6enrCwsEBWVhYWLVqEhg0b4scff4SW1ruX1tPTE3Z2dli7di0uXboEJycnpTreunULkydPhqGhIbZt2wZzc/NinSMRERERUamPWERFRaFKlSro2bOnQvqIESNU5pcHFYIgID09HcnJyahUqRKsrKwQFxcn5pPJZHBzc8Pvv/+O5ORkMT08PBz5+fno3bt3sesJAKNGjRKDCgCwt7fHF198gStXruD169fFKlMVLS0tNG3aVGFkIiYmBi1atECLFi3EKUeCIODy5ctwdHSEpqYmgHdTvLKzs9G7d28kJycr/HzxxRfIz88X94+OjkZSUhJ69eoltqP8x8XFRczzoejoaHz11VcwNzfH1q1bGVQQERERUYmU+ojFo0eP0KBBA/HDsVzVqlUhk8mU8t+6dQs//fQTYmJilKZK1axZU+H3vn374vDhw/j1118xZMgQCIKA8PBw2Nvbo379+sWq5+PHj6GhoQEbGxulbbVr10ZUVBQePXqESpUqFatcVZycnHD+/HkkJCRAW1sbjx8/RosWLZCVlYU9e/bg+fPneP36NVJSUhSmQSUmJgIAJkyYUGDZr169AvDu/gwA8Pf3LzBvUlKS0r6TJ09G7dq18eOPPxZrxIeIiIiI6H3lutzs06dPMW7cOBgYGGD06NGwtraGrq4uJBIJVq9erRRoNGnSBHXq1MGhQ4cwZMgQXLhwAY8fP8aMGTPK6QyK5v37LKRSKbS0tNCsWTPk5ORAQ0MDFy5cEEdh3g8sBEEAACxcuBBVq1ZVWbY8+JLnnTx5Muzt7VXmNTU1VfjdyMgI9erVw5kzZ3DkyBH07du35CdJRERERBVaqQcWNWvWxIMHD5CXl6cwavHy5UukpaUp5I2MjMTbt2+xZs0apbn/KSkpkEqlSuX37dsXq1atQlxcHA4dOgQdHR107969RPXMz89HQkIC7OzsFLbJv/3/cMSkpOrVqwdDQ0NcvHgR2traaNiwIfT09KCnp4e6devi4sWLSE1NReXKlVGnTh1xv1q1agEATExM4OzsXOgxLC0tAbybWvaxvHJaWlpYuXIlZs6ciaVLlyI3Nxf9+/cv4VkSERERUUVW6vdYtGvXDklJSYiIiFBI3759u/LBNd4dXv5tu9yBAweUpu3I9ejRAzo6OggJCUFUVBQ6duyocopVUeoJAEFBQQrHv3v3Lk6fPo2mTZuWyjQoANDU1ISjoyMuX74s3l8h5+TkhIsXL+Ly5cto3ry5wv0ebm5ukEql2Lx5MzIzM5XKTU9PR3Z2NgCgdevWqFy5MoKDgxWWsJXLzMzEmzdvlNK1tLSwbNkydOrUCd9//73KG+yJiIiIiD6m1Ecshg8fjqNHj2LJkiW4efMm6tSpg5iYGFy9ehUmJiYKeV1cXLB+/XrMmzcPAwYMgEwmQ2xsLP744w9YWFggLy9PqXwjIyN07NgRR44cAYBi37Qt16pVK7i5ueH48eNIS0uDq6srkpKSEBYWBqlUCj8/vxKVW5AWLVrg9OnTAKAwOtOiRQuEhIQopQNA9erV8d1332Hx4sXo378/evToATMzM7x+/Rp3795FVFQUwsLCYG5uDj09PSxcuBB+fn7w9PSEh4cHatWqhbS0NCQmJiIyMhIrV65UuSqUlpYWlixZAi0tLaxevRp5eXkYNmxYqZ4/EREREf27lXpgYWRkhJ9//hlr1qzBr7/+CgBwdHTE5s2b4ePjo5DXwsICP/zwAzZu3IigoCBoaGigSZMm2Lx5M1asWIEnT56oPEa/fv1w5MgR1KpVC82bNy9xXRctWoS6devi8OHDWLduHfT09ODo6AgfHx/Y2tqWuFxV5KMUOjo6aNy4sZjerFkzaGlpITc3V2EkQ87DwwOWlpbYuXMn9u/fj7S0NJiYmMDKygo+Pj6oUqWKmLd169bYvn07tm/fjiNHjuD169cwMjKChYUFhg4dqjTl632amprw9/eHlpYW1q1bh5ycHHh7e5diCxARERHRv5lE+HAe0j9AXFwcRo4cCV9fX374/QxIVn38qd7qEKZ7FXOHg2VSDyIiIiIqWKnfY/Ep7NmzB1paWujVq1d5V4WIiIiIiFDOy80WR0ZGBk6fPo34+HhxaVRVS7C+fPnyo2UZGhqq/cyG9PR0lTdUv09bWxvGxsZqHYeIiIiI6J/gHxNYvH79GrNnz4a+vj46deqESZMmqczXrVu3j5Y1f/58tUc7Vq1ahcOHDxeax9HREYGBgWodh4iIiIjon+AfeY9FYaKjoz+ap06dOgU+cK6o4uPj8eLFi0LzGBkZFfuJ4P9EvMeCiIiIiP4xIxZFVdSHw6mrdu3aqF279ic5FhERERHR5+4fefM2ERERERF9Xv51U6Ho0wsMDIS3tze0tbXLuypEREREVE44YkFERERERGpjYEFERERERGpjYEFERERERGpjYEFERERERGpjYEFERERERGpjYEFERERERGpjYEFERERERGpjYEFERERERGpjYEFERERERGpjYEFERERERGpjYEFERERERGpjYEFERERERGqTCIIglHcl6J9Nsiq3TMoVpnsVMePBMjk+ERERERUdRyyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCw+E5cuXYKTkxPCw8PLuypERERERMXGwIKIiIiIiNTGwIKIiIiIiNTGwIKIiIiIiNSmVd4VoIJlZGRg69atOHHiBJ4/fw4jIyM4OzvDx8cHZmZmYr78/HwEBQXh/PnzuH//PlJSUlClShW4urrCx8cHJiYmYt7Hjx/Dw8MDY8eORYMGDbBlyxbcvXsXMpkMPXr0gK+vL7S02C2IiIiIqHj4CfIzlZubi4kTJyI2NhadOnXCsGHDcP/+fezbtw/R0dHYsWMHqlevDgDIyclBSEgIOnbsiHbt2kFXVxc3btzAoUOHcOXKFezcuRPa2toK5Z89exZ79+6Fp6cnPDw88PvvvyMkJAQymQyjRo0qj1MmIiIion8wBhafqfDwcMTGxuLLL7/E5MmTxXRnZ2dMmTIFGzZswKJFiwAAUqkUR48eha6urkIZjRs3xuLFixEVFQU3NzeFbfHx8dizZw/Mzc0BAJ6enhg4cCB2797NwIKIiIiIio33WHymIiMjoaGhAW9vb4V0V1dX2Nvb4/Tp08jPzwcASCQSMajIy8tDWloakpOT0aJFCwBAXFycUvnt27cXgwp5GU5OTkhKSsLbt2/L6rSIiIiI6F+KIxafqcePH8PU1BRGRkZK2+rUqYM7d+4gOTkZlStXBgCcOHECO3fuxO3bt5Gbm6uQPzU1VamMmjVrKqUZGxsDAFJSUqCvr18ap0FEREREFQQDi3+BU6dOYebMmXBwcICfnx+qV68OqVSK/Px8fP311xAEQWkfDY2CB6tU5SciIiIiKgwDi89UzZo1ce7cOaSlpUEmkylsi4+Ph4GBgbja06+//godHR1s3rxZ4T6LxMTET1hjIiIiIqrIeI/FZ6p9+/bIz89HcHCwQvrZs2dx+/ZttG3bVhx1kP8rv+cCeDfqsHXr1k9WXyIiIiKq2Dhi8Znq1asXDh8+jO3bt+Px48dwdHTEgwcPsHfvXlSpUgW+vr5i3k6dOuHUqVMYP348evbsidzcXPz+++/IzMwsxzMgIiIiooqEgcVnSktLCxs2bBAfkBcZGQmZTIZOnTphwoQJqFGjhpi3a9euePv2LXbt2oWAgADIZDK0bdsWEydORKdOncrxLIiIiIioopAIvFOX1CRZlfvxTCUgTPcqYsaDZXJ8IiIiIio63mNBRERERERqY2BBRERERERqY2BBRERERERqY2BBRERERERqY2BBRERERERqY2BBRERERERqY2BBRERERERq43MsSG2BgYHw9vaGtrZ2eVeFiIiIiMoJRyyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtDCyIiIiIiEhtEkEQhPKuBP2zSVbllnhfYbpXMTIfLPFxiIiIiKhsccSCiIiIiIjUxsCCiIiIiIjUxsCCiIiIiIjUxsCCiIiIiIjUxsCCiIiIiIjUxsCCiIiIiIjUxsCCiIiIiIjUxsDiM3Py5EkMHjwYLi4ucHJywqVLl1Tm69WrF8aNG1ekMsPDwwsti4iIiIhIXVrlXQH6n3v37mH27Nlo3Lgxpk+fDqlUChsbm/KuFhERERHRRzGw+IzExMQgLy8P06ZNQ7169QrNu2/fPkgkkk9UMyIiIiKiwjGw+IwkJSUBAIyMjD6aVyqVlnV1iIiIiIiKrNQDi5ycHOzatQvHjh3DvXv3oKWlBUtLS7i7u2PgwIFivsePH+PHH39EdHQ00tLSUK1aNXTp0gWjR4+Grq6umG/z5s3YsmUL9u7di4iICEREROD169ewtraGr68vXF1dFY5/+PBh7NmzB/fv30dubi6qVKmCRo0aYdq0aahUqdJH63/p0iWEhIQgLi4OGRkZMDU1RfPmzTFp0iSYmJgAAHJzc7Fz505ERETg0aNH0NPTQ7NmzTB+/HjY2toqnKOHhwfGjh2LBg0aYMuWLbh79y5kMhl69OgBX19faGm9ewmcnJzE/Tw8PAAAZmZmCA8PV1nPXr16wczMDIGBgQrpBw4cwM6dO/H48WNUr14dAwYMgKGhoUKeN2/eYOjQocjIyMB//vMfVK5cWdy2ceNGBAUFYe7cuejdu/dH24uIiIiICCjlwCInJwcTJ05ETEwMWrVqhe7du0MqleLu3buIjIwUA4snT55gxIgRSE9Ph5eXFywtLRETE4OgoCDExsZi06ZN4gduuQULFkBLSwvDhg1DTk4O/vOf/8DPzw/79++Hubk5ACAiIgILFiwQP+Tr6Ojg2bNnOHv2LF69evXRwGLfvn1Yvnw5qlWrBk9PT5iZmeHp06f473//i2fPnomBxdy5c3HixAk4OzvD09MTSUlJCAsLg7e3N7Zs2aI0jens2bPYu3cvPD094eHhgd9//x0hISGQyWQYNWoUAMDf3x+RkZGIjIzE1KlTYWJiAn19/WK1/65du7BmzRrY29vD19cXmZmZ2Llzp9J5GxgYYOnSpRg9ejQWLFiAgIAASCQSXLhwAdu3b0eXLl0YVBARERFRsZRqYLFr1y7ExMTA29sbvr6+Ctvy8/PF/2/cuBGvX7/GunXrxBGH/v37IyAgACEhITh8+DD69OmjsL+JiQnWrl0r3lfg5OSEESNGYP/+/Zg4cSIAICoqCgYGBvjxxx8VApPx48d/tO7Pnj3DqlWrYG1tjW3btkEmk4nbfHx8xPqfP38eJ06cgJubG5YuXSrWx83NDV9++SVWrVqFn3/+WaHs+Ph47NmzRwyAPD09MXDgQOzevVsMLHr06IEHDx4gMjIS7du3F/MWVVpaGjZt2gQbGxts27ZNHPXp1asXvLy8lPI3aNAAvr6+WLduHXbu3ImePXti7ty5MDMzw6xZs4p1bCIiIiKiUl1u9ujRozAyMsKYMWOUD6Tx7lD5+fk4ffo06tatqzSNaeTIkdDQ0EBUVJTS/oMGDVK4WdnBwQH6+vq4f/++mGZoaIjMzEycOXMGgiAUq+4nT55ETk4Oxo4dqxBUfFh/ed1GjRqlUB97e3t88cUXuHLlCl6/fq2w74eBgkQigZOTE5KSkvD27dti1bMg58+fR2ZmJvr3768wlax69ero1q2byn2GDh0KFxcXbNy4EVOmTEFKSgqWLFmiNHWKiIiIiOhjSjWwuH//PqytraGjo1NgntevX+Pt27eoXbu20jZjY2NUrVoVjx49UtpmYWGhMn9KSor4u7e3N2rUqAE/Pz907twZ06dPx8GDB/HmzZuP1v3BgwcAgLp16xaa7/Hjx9DQ0FC5DKz8nD6sf82aNVXWHYBC/dUhP6a1tXWB9fqQRCLBwoULoaenhxs3bmDcuHFo2LBhqdSHiIiIiCqWf8wD8uQjBh96f2TC0tISYWFhWLduHdzd3fH06VMsXrwYXl5eePjw4aeqqpKC6g6g2CMrpe3y5ctIS0sDANy5c6dc60JERERE/1ylGlhYWVkhMTER2dnZBeapVKkSDAwMEB8fr7QtNTUVL1++VPkNf1FJpVK4urrim2++QUhICNatW4cXL14gNDS00P0sLS0BfPzDdc2aNZGfn4+EhASlbfI0depfUvJjJiYmKm1T1dYAxMCrTp06GDp0KE6ePIkDBw6UZTWJiIiI6F+qVAOLbt26ITU1FVu3blXaJv9mXkNDA1988QVu376NP/74QyFPcHAw8vPz0b59+xIdPzk5WSlNvkLT+1OOkpOTkZiYiPT0dDGtU6dO0NbWxpYtWxTSP6x/u3btAABBQUEKow13797F6dOn0bRp0yIta1tUquqqirOzM3R0dBAWFobMzEwx/dmzZzh27JhS/ry8PMyePRtZWVlYtmwZvv76azRu3BirV69WGTQRERERERWmVFeFGjx4MP773/9i69atuHHjhvhhNz4+Hvfu3cOmTZsAAL6+voiOjoafnx+8vLxQq1YtXL58GSdOnICjoyPc3d1LdHxfX1/IZDI0a9YM1atXR1paGsLDwyGRSNCjRw8x3+7du7FlyxbMnz8fvXr1AvDuJudp06bh+++/x6BBg9CzZ0+YmZnh+fPn+P333zFv3jzUrVsXrVq1gpubG44fP460tDS4urqKy81KpVL4+fmp35DvUVVXVYyMjODj44N169Zh1KhR6NGjBzIzM7F//37UqlULt2/fVsgfGBiI2NhYzJ49W7wHY/HixRgyZAhmzZqF7du38yF8RERERFRkpRpYaGtrY8OGDdi5cyeOHTuGTZs2QSqVwtLSUuFDsZmZGYKDg/HTTz/hyJEjSEtLQ/Xq1eHt7Y3Ro0crPcOiqLy8vHDixAns378fKSkpMDY2Rt26dTFjxgyFB9AVtr+FhQV27NiBX375BTk5OTA1NUWLFi1QvXp1Md+iRYtQt25dHD58GOvWrYOenh4cHR3h4+Oj8IC8T23YsGHQ09NDaGgoNm7ciOrVq2PYsGEwNDSEv7+/mO/SpUsICgqCm5sb+vbtK6abm5tj9uzZmDlzJtatW4cZM2aUx2kQERER0T+QRCjvu4fpH0+yKrfE+wrTlZ+xUXDmgyU+DhERERGVrX/MqlBERERERPT5YmBBRERERERqY2BBRERERERqY2BBRERERERqY2BBRERERERqY2BBRERERERq43KzpLbAwEB4e3tDW1u7vKtCREREROWEIxZERERERKQ2BhZERERERKQ2BhZERERERKQ2BhZERERERKQ2BhZERERERKQ2BhZERERERKQ2BhZERERERKQ2BhZERERERKQ2BhZEREREREVgbW2NkSNHlnc1PlsMLIiIiIioQvv777/x1VdfoXbt2tDV1YWRkRFcXFwQEBCAjIyM8q7eR2VlZeHbb7+Fubk59PT04OzsjBMnTnzyemh98iMSEREREX0mIiIi0L9/f+jo6GD48OFo2LAhsrOzcebMGUyfPh3Xr19HYGBgeVezUCNHjsTevXsxZcoU2NnZITg4GD169EBkZCRcXV0/WT0YWBARERFRiUlW5ZZ3FSD4lewjbUJCAgYNGgQrKyucOnUKZmZm4jZfX1/cvXsXERERpVXNMnHhwgX88ssvWLlyJfz8/ABADJBmzJiBP/7445PVhVOhSG1fpY6CNEACyarcQn8g6fPuh4iIiOgzsGLFCqSnp2Pr1q0KQYWcra0tJk+eXOD+r169gp+fHxo1agRDQ0MYGRmhe/fuiI2NVcq7fv16ODg4QF9fH5UqVYKTkxN27dolbk9LS8OUKVNgbW0NHR0dVKtWDW5ubrh8+XKh57B3715oampi3LhxYpquri5Gjx6Nc+fO4cGDB0VpilLBEQsiIiIiqpDCw8NRu3ZttGnTpkT7x8fH4+DBg+jfvz9sbGzw7NkzbN68Ge3atcONGzdgbm4OANiyZQsmTZoELy8vTJ48GZmZmbh69Sqio6MxZMgQAMD48eOxd+9eTJw4EQ0aNEBSUhLOnDmDmzdvwtHRscA6/Pnnn7C3t4eRkZFCesuWLQEAV65cQa1atUp0fsXFwIKIiIiIKpzU1FQ8evQIvXv3LnEZjRo1wp07d6Ch8b9JQF9++SXq1auHrVu3Yu7cuQDe3cfh4OCAsLCwAsuKiIjA2LFjsXr1ajFtxowZH63DkydPVI62yNMeP35c5PNRF6dCEREREVGFk5qaCgCQyWQlLkNHR0cMKvLy8pCUlARDQ0PUrVtXYQqTiYkJHj58iIsXLxZYlomJCaKjo4sdCGRkZEBHR0cpXVdXV9z+qTCwICIiIqIKRz51KC0trcRl5OfnY+3atbCzs4OOjg6qVq0KU1NTXL16FSkpKWK+b7/9FoaGhmjZsiXs7Ozg6+uLs2fPKpS1YsUKxMXFoVatWmjZsiUWLFiA+Pj4j9ZBT08PWVlZSumZmZni9k+FgQURERERVThGRkYwNzdHXFxcictYunQppk6dirZt22Lnzp04duwYTpw4AQcHB+Tn54v56tevj9u3b+OXX36Bq6sr9u3bB1dXV8yfP1/MM2DAAMTHx2P9+vUwNzfHypUr4eDggCNHjhRaBzMzMzx58kQpXZ4mv8/jU2BgQUREREQVkru7O/7++2+cO3euRPvv3bsXHTp0wNatWzFo0CB06dIFnTt3RnJyslJeAwMDDBw4EEFBQbh//z569uyJJUuWiCMLwLsgYcKECTh48CASEhJQpUoVLFmypNA6NG3aFHfu3BGndslFR0eL2z8VBhafmV69eiksF0ZEREREZWPGjBkwMDDAmDFj8OzZM6Xtf//9NwICAgrcX1NTE4IgKKSFhYXh0aNHCmlJSUkKv0ulUjRo0ACCICAnJwd5eXkKU6cAoFq1ajA3N1c5zel9Xl5eyMvLU3iIX1ZWFoKCguDs7PzJVoQCuCoUEREREVVQderUwa5duzBw4EDUr19f4cnbf/zxB8LCwjBy5MgC93d3d4e/vz+8vb3Rpk0bXLt2DaGhoahdu7ZCvi5duqBGjRpwcXFB9erVcfPmTWzYsAE9e/aETCZDcnIyLCws4OXlhSZNmsDQ0BAnT57ExYsXFVaJUsXZ2Rn9+/fHzJkz8fz5c9ja2mL79u1ITEzE1q1bS6OZioyBxWdm3759kEgk5V0NIiIiogrBw8MDV69excqVK3Ho0CH8+OOP0NHRQePGjbF69WqMHTu2wH1nzZqFN2/eYNeuXdi9ezccHR0RERGB7777TiHfV199hdDQUKxZswbp6emwsLDApEmTMGfOHACAvr4+JkyYgOPHj2P//v3Iz8+Hra0tNm3aBB8fn4+ew44dOzB37lyEhITg9evXaNy4MQ4fPoy2bduq1zjFJBE+HL+hQr158wYGBgblXY3PimRVbpHyCdO9/v9/DpZdZYiIiIioXFS4eyzCw8Ph5OSE6OhobN68Ge7u7mjdujUGDRqEY8eOKeSV3+9w69YtTJw4Ee3atcPgwYPF7ffv38fcuXPRtWtXtGrVCr169UJAQIDCesE//PADnJyc8NdffynVJT09HS4uLpg2bZrSMT8UFRWFUaNGwdXVFV988QVGjRqFqKgopXxOTk5YsGBBged96dIlMS0lJQWrV69G79690aZNG3Tq1AnDhg3Djh07Cm1DIiIiIqIPVdipUOvXr0dGRga8vN59ix4eHo7Zs2cjOzsbvXr1EvM9e/YMPj4+6Ny5Mzp27Ii3b98CAG7evInx48dDJpOhX79+qFatGu7cuYNffvkFsbGxCAwMhJaWFtzd3bFjxw5ERERgypQpCnU4ceIEsrKy4O7uXmhdw8LC8P3338Pa2hpjxowBABw+fBh+fn6YNWsW+vXrV6I2+O6773D58mV4enrCzs4OWVlZSEhIQExMDIYPH16iMomIiIioYqqwgUVycjJ++eUXGBoaAnh3R/2gQYOwdu1auLm5iU8rfPToEebMmYM+ffoo7O/v74+qVatix44dClOjWrZsienTp+PIkSPo1asXateujQYNGuDo0aP4+uuvoampKeaNiIiAsbExXF1dC6xnamoqfvjhB1hYWCA4OFihvkOHDsW6devg5uZW7KdGpqen4+LFi/Dy8irS4+KJiIiIiApT4aZCyXl5eYkf0gHA0NAQnp6eSE1NRUxMjJhubGysMIIBAHfv3sVff/2Fbt26IScnB8nJyeJP06ZNoaenh/Pnz4v5e/bsiZcvX4rrCQPvApbY2Fh07doV2traBdYzOjoaGRkZGDRokFJ9Bw0ahLdv3yqUW1Q6OjqQSqWIi4sr9qPjiYiIiIg+VGFHLKytrZXSbGxsAEBh7eGaNWsqjDIAQEJCAgBg8+bN2Lx5s8ryX716Jf6/a9euWLduHSIiItCmTRsA70YrBEFAz549C62nvC4fLlv2ftqHayUXhba2NqZOnYrVq1fDw8MDtWvXhpOTE9q3b4+WLVsWuzwiIiIiqtgqbGBRVPIpUe+TL6Q1bNgwtG7dWuV+RkZG4v9NTEzg4uKCqKgocVWpX3/9FTY2NnBwcCibin8gLy9PKc3Lywvt27fHmTNnEBMTg99++w179uyBm5sbli1b9knqRURERET/DhU2sEhMTFRKk49E1KxZs9B9LS0tAQAaGhpwdnYu0vHc3d0RFRWFkydPwsrKCg8fPsTEiRM/up+FhQUAID4+XmkkQVV9jY2NlZ7cCBQ8qlG1alX06dMHffr0QV5eHubNm4djx45h2LBhnyzoISIiIqJ/vgp7j8XevXuRnp4u/p6eno59+/ZBJpOhefPmhe5bt25d1KlTB/v27cPDhw+Vtufm5ip9uHd1dYWJiQkiIiIQEREBDQ0N9OjR46P1dHZ2hp6eHnbv3o03b96I6W/evMHu3buhr6+PVq1aiemWlpa4du0aMjMzxbTU1FT83//9n0K5mZmZCnmAd4+lt7OzE/chIiIiIiqqCjtiYWJighEjRog3ZoeHh+Pp06eYM2eOyulP75NIJPD394ePjw8GDx4s3qOQmZmJhw8f4tSpU5g4caLCTd9aWlro2rUr9uzZg1u3bqFly5aoVq3aR+spk8kwadIkfP/99xg5cqS4NO3hw4fx4MEDzJo1S+Gm7gEDBmDu3LkYP348evTogbS0NBw8eBBmZmZISkoS8927dw/jxo1Dhw4dUKdOHchkMiQmJmLv3r2oWbMmmjVrVqz2JCIiIqKKrcIGFl9//TWuXLmCsLAwvHr1CpaWlli8eDG6detWpP3r1q2L0NBQBAUF4fTp09i3bx8MDAxgZmaGXr16oUWLFkr7uLu7Y/fu3Xj79u1Hb9p+X//+/VG1alWEhIRgy5YtAAB7e3usWrUK7du3V8jbvXt3vHjxAnv27MHatWtRs2ZNjBkzBhoaGoiLixPzVa9eHR4eHoiJiUFUVBRycnJgamqKvn37YsSIER8NroiIiIiI3icR5HciVxDh4eFYuHAhfvrpJzg5OZV3df4VJKtyi5RPmO71//9zsOwqQ0RERETlosLeY0FERERERKWHgQURERERURFYW1tj5MiR5V2NzxYDCyIiIiKq0P7++2989dVXqF27NnR1dWFkZAQXFxcEBAQgIyOjvKtXqPT0dMyfPx/dunVD5cqVIZFIEBwcXC51qXA3b/fq1UthtSYiIiIiUoOkT3nXQK37NyMiItC/f3/o6Ohg+PDhaNiwIbKzs3HmzBlMnz4d169fR2BgYOnVtZS9fPkS/v7+sLS0RJMmTRAVFVVudalwgQUREREREfDuYcODBg2ClZUVTp06BTMzM3Gbr68v7t69i4iIiHKs4ceZmZnhyZMnqFGjBi5duqRyZdJPhVOhiIiIiKhCWrFiBdLT07F161aFoELO1tYWkydPLnD/V69ewc/PD40aNYKhoSGMjIzQvXt3xMbGKuVdv349HBwcoK+vj0qVKsHJyQm7du0St6elpWHKlCmwtraGjo4OqlWrBjc3N1y+fLnQc9DR0UGNGjWKcdZlhyMWRERERFQhhYeHo3bt2mjTpk2J9o+Pj8fBgwfRv39/2NjY4NmzZ9i8eTPatWuHGzduwNzcHACwZcsWTJo0CV5eXpg8eTIyMzNx9epVREdHY8iQIQCA8ePHY+/evZg4cSIaNGiApKQknDlzBjdv3oSjo2OpnXNZYmBBattstA3e3t7Q1tYuPKPfwU9SHyIiIqKPSU1NxaNHj9C7d+8Sl9GoUSPcuXMHGhr/mwT05Zdfol69eti6dSvmzp0L4N19HA4ODggLCyuwrIiICIwdOxarV68W02bMmFHiupUHToUiIiIiogonNTUVACCTyUpcho6OjhhU5OXlISkpCYaGhqhbt67CFCYTExM8fPgQFy9eLLAsExMTREdH4/HjxyWuT3ljYEFEREREFY6RkRGAd/c2lFR+fj7Wrl0LOzs76OjooGrVqjA1NcXVq1eRkpIi5vv2229haGiIli1bws7ODr6+vjh79qxCWStWrEBcXBxq1aqFli1bYsGCBYiPjy9x3coDAwsiIiIiqnCMjIxgbm6OuLi4EpexdOlSTJ06FW3btsXOnTtx7NgxnDhxAg4ODsjPzxfz1a9fH7dv38Yvv/wCV1dX7Nu3D66urpg/f76YZ8CAAYiPj8f69ethbm6OlStXwsHBAUeOHFHrPD8lBhZEREREVCG5u7vj77//xrlz50q0/969e9GhQwds3boVgwYNQpcuXdC5c2ckJycr5TUwMMDAgQMRFBSE+/fvo2fPnliyZAkyMzPFPGZmZpgwYQIOHjyIhIQEVKlSBUuWLCnp6X1yDCyIiIiIqEKaMWMGDAwMMGbMGDx79kxp+99//42AgIAC99fU1IQgCAppYWFhePTokUJaUlKSwu9SqRQNGjSAIAjIyclBXl6ewtQpAKhWrRrMzc2RlZVV3NMqN1wVioiIiIgqpDp16mDXrl0YOHAg6tevr/Dk7T/++ANhYWEYOXJkgfu7u7vD398f3t7eaNOmDa5du4bQ0FDUrl1bIV+XLl1Qo0YNuLi4oHr16rh58yY2bNiAnj17QiaTITk5GRYWFvDy8kKTJk1gaGiIkydP4uLFiwqrRBVkw4YNSE5OFm/8Dg8Px8OHDwEAX3/9NYyNjUveSMUgET4Ms4iKKTAwsGjLzRIREdG/j6RPedcAEA6qtftff/2FlStX4sSJE3j8+DF0dHTQuHFjDBo0CGPHjoWOjg4AwNraGu3bt0dwcDAAICsrC7Nnz8auXbuQnJwMR0dHrFq1Ct999x0AICoqCsC7z0qhoaG4fv060tPTYWFhgX79+mHOnDkwMjJCdnY25syZg+PHjyM+Ph75+fmwtbXFV199BR8fn4/W39raGvfu3VO5LSEhAdbW1mq1T1ExsCC1MbAgIiIiIt5jQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREamNgQUREREREatMq7wrQP5sgCMjIyEBqaiq0tbXLuzpEREREVAZkMhkkEkmheSSCIAifqD70L/Ty5UuYmpqWdzWIiIiIqAylpKTAyMio0DwcsSC16OjooGnTpoiIiIChoWF5V+dfJz09HT179mT7lhG2b9lh25Yttm/ZYvuWLbZv2Sqr9pXJZB/Nw8CC1CKRSKCpqQkjIyNeHMqAhoYG27cMsX3LDtu2bLF9yxbbt2yxfctWebYvb94mIiIiIiK1MbAgIiIiIiK1MbAgtUilUowdOxZSqbS8q/KvxPYtW2zfssO2LVts37LF9i1bbN+yVZ7ty1WhiIiIiIhIbRyxICIiIiIitTGwICIiIiIitXG5WQIAJCYmYsWKFbh69SoMDAzQo0cPTJgw4aNP0xYEAdu3b0dYWBiSk5Nhb2+PqVOnolGjRgr5Xrx4gRUrViA6OhpaWlro0KEDvvnmmwqzzFxZtu+lS5cwfvx4pX3d3NywbNmyUj+Xz1FJ2zcsLAxnz55FXFwckpOTsXz5cnTu3FkpH/tv2bVvRe+/JWnbly9fIjQ0FNHR0Xj48CEMDQ3RrFkzTJw4EWZmZgp52XfLrn0ret8FSn5tmDt3LuLi4vDixQtoa2vD1tYWo0ePRqtWrRTypaenY82aNYiKikJubi5atWqFGTNmoGrVqmV5Wp+Nsmzfx48fw8PDQ2nfhg0bIjg4uMR1ZmBBSE1Nxfjx42FpaYmVK1fi+fPnWLt2LTIzM/Htt98Wuu/27duxefNmTJw4EXZ2dggLC8PEiRMRGhoKCwsLAEBubi4mTpwIAFi8eDEyMzMREBCAOXPmYN26dWV9euWurNtXbv78+bC2thZ/NzExKYOz+fyo074REREAABcXF/H/H2L/Ldv2lauI/bekbXvz5k1ERkbCw8MDjRo1QnJyMn7++WeMGDECu3fvRqVKlQCw75Z1+8pVxL4LqHdtyMnJwdChQ1GrVi1kZ2fj0KFDmDx5Mn766Sc0a9ZMzDdz5kzEx8dj5syZkEql2LRpEyZNmoQdO3ZAS+vf/RH2U7QvAPj6+sLJyUn8XV9fX72KC1Thbdu2TXB1dRWSk5PFtH379gktW7YUnj9/XuB+mZmZQtu2bYUNGzaIadnZ2YK7u7uwbNkyMe3IkSOCk5OTkJCQIKadO3dOaN68uXDt2rXSPZnPUFm378WLF4XmzZsL169fL5sT+MyVtH0FQRDy8vIEQRCER48eCc2bNxdOnDihlIf9t2zbtyL335K2bWpqqpCTk6OQ9vTpU8HJyUkICQkR09h3y7Z9K3LfFQT1rg0fys3NFXr06CEsXrxYTIuNjRWaN28unDt3TkxLSEgQnJychOPHj6t/Ap+5sm7fwq7L6uA9FoQ//vgDLVu2hLGxsZjm5uaG/Px8nD9/vsD9rl69ijdv3ihMbdDW1kaHDh1w9uxZhfLt7OwUvtFxdnaGsbGxQr5/q7Ju34qupO0LvHs6aVHKZ/8tu/atyEratjKZTOnb2urVq6NSpUp48eKFQvnsu2XXvhWdOteGD2lqakImkyEnJ0ehfJlMBmdnZzHN2toa9vb27L+l0L5lhVd9QmJiosIfHuDdhbVq1apITEwsdD8ASvva2Njg6dOnyMzMFPNZWVkp5JFIJLCysiq0/H+Lsm5fucmTJ6Nly5bo0aMHAgIClLb/W5W0fYtTPvuvtUJaabavXEXsv6XZtvfu3cOrV69gY2OjUD77rrVCWmm2r1xF7LuA+u0rCAJyc3ORnJyMkJAQPHjwAP369VMo38rKChKJRGE/Gxsb9t9SaF+55cuXo2XLlnBzc8PixYuRkpKiVr3/3RPUqEhSU1Mhk8mU0mUyGVJTUwvdTyqVQkdHR2k/QRCQlpYGXV1dpKWlqSzfyMio0PL/Lcq6fQ0NDTF8+HA4OjpCR0cHFy9exM6dO5GQkFBh5lGXpH2Liv23bNu3Ivff0mpbQRCwatUqmJqaomvXrmI6+27Ztm9F7ruA+u176NAhLF68GMC7ef1Lly5F48aNS638f7qybl+pVAovLy+0atUKMpkMcXFx2LZtG27cuKHWPSwMLIj+4erVq4d69eqJv7do0QJVq1bFihUrEBcXh4YNG5Zj7YgKx/6rvsDAQFy4cAHr16+Hnp5eeVfnX6eg9mXfVU/79u1hb2+P5ORknDx5EjNnzsTKlSvh4uJS3lX7V/hY+1atWhXfffedmL958+aoU6cOpkyZgsjISLi5uZXouJwKRTAyMkJ6erpSelpaGoyMjArdLzs7G1lZWUr7SSQSMdKWyWQqy09NTS20/H+Lsm5fVeQXhFu3bpWw1v8cJW3fomL/Ldv2VaWi9N/SaNsDBw5gy5YtmDVrFlq2bKmwjX23bNtXlYrSdwH129fExAQNGjRAmzZtMG/ePLRp0wYBAQGlVv4/XVm3ryouLi7Q09PDzZs3S1xvBhYEa2trpfl66enpePnypdL8vg/3A97NPX1fYmIiatSoAV1d3QLLFwQB9+7dK7T8f4uybt+KrqTtq0757L+l174VmbptGxkZieXLl2P8+PHo3bt3kcpn3y299q3oSvvaUK9ePTx8+FCh/Hv37kEQBIV8qu49+Dcq6/YtKwwsCG3atMGFCxeQlpYmpp08eRIaGhpKD6t5X+PGjWFgYICTJ0+Kabm5uYiMjFQYymzTpg3++usv3L9/X0y7cOECUlJSKsSQZ1m3ryrHjh0DADRo0EDN2n/+Stq+xSmf/bfs2leVitJ/1WnbS5cuYfbs2ejTpw/GjBlTYPnsu2XXvqpUlL4LlP61ITY2FjVr1lQoPzU1FRcuXBDT7t27h9u3b7P/lkL7qvLf//4XGRkZavVf3mNB8PT0xO7duzFt2jSMGjUKz58/R0BAAPr16wdTU1Mxn4+PD548eYKDBw8CAHR0dODt7Y3AwEBUqlQJtra2CAsLQ0pKCoYNGybu17lzZwQFBWHGjBnw9fVFZmYm1q1bB1dX1woxB7Ws23fu3LmwsLBAvXr1xBsId+3ahfbt21eIP24lbV8AuHHjBh4/fozk5GQAQFxcHACgUqVKaN68OQD237Ju34rcf0vatgkJCfDz80OtWrXQo0cPXLt2TcxbqVIl8eGZ7Ltl274Vue8CJW/fM2fOICIiAq6urqhevTpSU1Nx9OhRnDt3DkuWLBH3a9y4MVq3bg1/f39888034gPy7Ozs0KFDh099up9cWbfv2rVroaGhgYYNG0Imk+H69esIDg5GgwYN0L59+xLXWyJ8OMZEFVJCQgJWrlyJ2NhYGBgYoGfPnkqPjR83bhyePHmC8PBwMU0QBAQHB2Pv3r14/fo17O3tMXXqVIWVBwDg+fPnWLlyJaKjo6GpqYkOHTpg6tSpMDQ0/GTnWJ7Ksn2DgoJw5MgRPH36FNnZ2TA3N0e3bt3g7e2tUP6/WUnbd8GCBTh8+LBSeY6OjggMDBR/Z/8tu/at6P23JG0bHh6OhQsXqizP3d0dCxYsEH9n3y279q3ofRcoWfsmJiZi/fr1uHHjBpKTk2FiYgI7OzuMGDFC/MJBLj09HWvWrEFkZCTy8vLg7OyMGTNmKHyw/jcry/Y9ePAg9u7diwcPHiAzMxPVqlVD+/bt8dVXX6l1fWBgQUREREREauM9FkREREREpDYGFkREREREpDYGFkREREREpDYGFkREREREpDYGFkREREREpDYGFkREREREpDYGFkREREREpDYGFkREREREpDYGFkRl6Pnz5zA2NsaWLVsU0keOHAlra+vyqdS/xIIFCyCRSJCYmPhJjhccHKx0vIyMDJibmxf4lN7CFNQ3qOTkr1FUVFR5V4XKmbrXB/aliisxMRESiUThCfafQlRUFCQSCYKDg0u0/5UrV6ChoYHff/+9dCtWTAwsiMrQnDlzYGpqCm9v7yLlf/r0Kfz8/NCwYUPIZDIYGRnBzs4OgwYNwv79+xXytm/fHoaGhgWWJf/DeunSJZXbX79+DT09PUgkEoSEhBRYjrW1NSQSifgjlUphbW2NMWPG4MGDB0U6r38rPT09fPfdd1i5ciWePHlSrH2L2zeoYrty5QoWLFjwyQJpKn+JiYlYsGABrly58kmPy76mLDk5GQsWLPisA82mTZuiT58+mDZtGgRBKLd6MLAgKiMPHz7Etm3b8PXXX0NLS+uj+e/du4cmTZpg48aNaNWqFZYvX45ly5bB3d0dt27dQlBQUKnWLzQ0FFlZWbCxscG2bdsKzWthYYGQkBCEhIQgICAAzs7O2LZtG5ydnfHy5ctSrdc/zejRoyGRSLBmzZoi71PcvkFF8+WXXyIjIwNt27Yt76qUuitXrmDhwoX8sFeBJCYmYuHCheUSWFTkvmZlZYWMjAzMmTNHTEtOTsbChQs/68ACAKZMmYKYmBj8+uuv5VYH/kUjKiObN2+GRCLB4MGDi5R/1apVeP78OQ4ePIjevXsrbX/69Gmp1m/r1q3o0KEDevfujSlTpiA+Ph61a9dWmdfY2BjDhg0Tf/fx8UG1atWwYcMGBAUFYfr06aVat38SAwMD9OvXD8HBwVi8eDF0dHQ+uk9x+0Z5y8vLQ1ZWFvT19cu7KoXS1NSEpqZmeVeDiP7BJBIJdHV1y7saJfLFF1/A2toaP/30E3r27FkudeCIBX025HNaf/vtN/j7+8PKygp6enpwdnbG+fPnAQC///47XF1dYWBgADMzMyxatEhlWZcuXULfvn1RtWpV6OjooG7duliyZAlyc3MV8l24cAEjR46Evb099PX1IZPJ4OLiggMHDiiVOXLkSEgkEqSkpIgfrHV1deHi4oLo6Gil/GFhYXByckK1atWKdP5//fUXAKBTp04qt9eoUaNI5RTF5cuXceXKFYwYMQJDhgyBlpbWR0ctPtS1a1cAwN27dwvMc+TIEUgkEvzwww8qt7du3RqmpqbIyckBULzXQxX5a6SKRCLByJEjldJ3794NV1dXyGQy6Ovrw9nZGXv37i3S8eS6d++Oly9fIjIyskj5C+ob+fn5WLJkCdq2bYsaNWpAKpXC0tISPj4+SEpKEvMlJydDV1cX/fr1U1n+zJkzIZFIFL7pTElJwbfffgtbW1vo6OjA1NQUgwcPRnx8vMK+8vfhyZMnsWjRItSpUwe6urrYs2cPAOD48eMYOHAgateuDT09PZiYmKBLly4Fzuvdt28fmjRpAl1dXVhaWmLhwoU4efKkyrnEWVlZWLp0KRwcHKCrqwsTExP06tULf/75Z5HaVdW8+NK6rlhbW6N9+/a4fPkyOnbsCENDQ1SuXBkjRozA8+fPFfKmpaVhzpw5cHZ2Fq9Btra2+O677/D27VulsgVBwJYtW+Ds7AxDQ0MYGhqiUaNGmDdvHoB30xrlU+Y6dOggTktU1Z8/dPXqVfTt2xdVqlSBrq4uGjRogBUrViAvL08hX3Gvb6rIp1/euHEDU6ZMgZmZGfT19dGpUyfcvn0bALB//344OjpCT08P1tbWCAwMVFnWzz//LOYzNjZGly5dcObMGaV8+fn5WLZsGWxsbKCrq4uGDRsiNDS0wDo+efIEPj4+sLS0hFQqhbm5OcaNG6f0GhZXUdu5ffv2Ku+v+3Bef3BwMDp06AAA8Pb2Fl/z9u3bA1Ccj79+/XrY29tDV1cX9vb2WL9+vVL58v77oQ/n9Ze0r8n7T1JSEkaOHImqVatCJpOhT58+4pdigYGBqF+/PnR1dVGvXj0cOnRIqZxNmzahS5cuqFmzJqRSKczMzDBs2DCVoyd5eXlYtGgRrKysoKuri8aNG2P37t0q768pTv/+8LWIioqCjY0NAGDhwoVim8hfx8LujSjob9KhQ4fQrFkz6OrqolatWpg7d674d/BDxbkuSiQSdO3aFUePHkV6errK8soaRyzos/Pdd98hLy8PkydPRnZ2NlavXo0uXbpgx44dGD16NMaNG4ehQ4diz549mDdvHmxsbBS+TY+IiEC/fv1ga2uLadOmoXLlyjh37hzmzZuHK1euICwsTMx74MAB3Lp1CwMGDICVlRWSkpKwfft29OvXD6GhoRgyZIhS/bp27QpTU1PMmzcPSUlJWLNmDXr27ImEhATIZDIAwLNnz3D79m1MmjSpyOddp04dAMCWLVswZcqUAj8gf6igqUiqPsDIbd26FYaGhvD09ISBgQHc3d2xfft2+Pv7Q0OjaN83yAOhqlWrFpinS5cuqFGjBnbs2KHUFn/99RfOnz+PSZMmQVtbG0DJXg91zJkzB0uWLEG3bt2waNEiaGho4MCBA+jfvz82bNgAX1/fIpXTunVrAO/+wHTr1q3QvIX1jezsbKxcuRKenp7o3bs3DAwMcPHiRWzduhVnzpxBTEwMpFIpTExM4OHhgUOHDuHVq1eoXLmyWEZ+fj5CQ0PRuHFjNG3aFMC7oKJNmza4f/8+Ro0aBQcHBzx58gSbNm2Cs7MzLl26BCsrK4W6+Pn5IScnB2PHjoWRkRHq1q0L4N0HnlevXmH48OGwsLDAo0eP8PPPP6NTp06IjIzEF198IZaxe/duDB48GHXq1MH8+fOhpaWF7du3Izw8XOncc3Jy0K1bN/zxxx/48ssvMXHiRKSkpGDLli1wcXHB6dOn4eTkVKTXQxV1ryvAuylsnTp1gqenJ7y8vHD58mVs27YNly5dwsWLF8URHXmbeHp6ioH777//jhUrVuDPP//EsWPHFMr98ssvERoaCmdnZ8yePRsmJia4desW9u7dC39/f/Tr1w9PnjxBYGAgZs2ahfr16wP43zWjIJcuXUK7du2gra0NX19f1KhRA+Hh4fj2228RGxur8gN4Ua5vHzNixAgYGhpi1qxZePHiBVavXo2uXbti0aJFmDFjBnx8fDBq1Chs3boVX331FRo0aABXV1dx/2+//RYrVqxAy5YtsXTpUqSlpSEwMBAdOnTAoUOH0KNHDzHv1KlTERAQgLZt2+Kbb77B8+fP4evrq3L09f79+2jdujWys7MxevRo1KlTB3fv3sWPP/6IyMhIXLp0CcbGxkU6R3Xb+WPatm2LWbNmYenSpRg3bpz4vqpevbpCvvXr1+Pp06f46quvIJPJ8J///AeTJk3Cq1evMH/+/GIft6R9Ta5bt26wsLCAv78/7t69ix9++AF9+/ZFv379EBgYiNGjR0NXVxc//PADvLy8cOfOHfFDO/Bu5L5Vq1aYNGkSKleujLi4OPz88884deoUrl27hipVqoh5J06ciJ9++gkdOnSAn58fXrx4gQkTJiiU96GS9O/69etj7dq1+Oabb8RzAVDoPY6FOXDgADw9PWFtbY158+ZBS0sLQUFBiIiIUMpbkuti69atsXnzZpw5c+ajf4/KhED0mQgKChIACM2aNROysrLE9EOHDgkABC0tLeHixYtielZWllCjRg2hVatWYlpGRoZQvXp14YsvvhBycnIUyl+zZo0AQIiMjBTT0tPTlerx5s0bwd7eXqhfv75C+ogRIwQAgo+Pj0L6nj17BADCTz/9JKadOnVKACAEBASoPNcRI0YIVlZWCml///23YGRkJAAQatWqJQwZMkRYu3atcOnSJZVltGvXTgDw0Z/320zeRiYmJsKIESPEtIMHDwoAhF9//VXpOFZWVkK9evWEFy9eCC9evBDi4+OFbdu2CcbGxoKWlpZw7do1lfWT8/PzEwAI169fV0ifM2eOAECIiYkR04rzesyfP18AICQkJIhp8tdIFQAK5xwTEyMAEGbOnKmUt3fv3oJMJhNSU1PFNHn/fP9479PS0hLc3d1VbntfYX0jPz9fePv2rVL6zz//LAAQdu/eLaYdPnxYACBs3LhRIe/JkycFAMLq1avFtEmTJgm6urrClStXFPImJiYKMplMoV3k52lvby+8efNGqS6qXqOnT58KVapUEbp37y6m5eTkCObm5kK1atWEV69eielpaWmCjY2NAEAICgoS0+Xvz6NHjyqUnZKSItSqVUto166d0nE/JK/7++/x0riuCMK79wEAYe3atQrp8novW7ZMoYzs7Gyl+sn7fHR0tJi2e/duAYAwbNgwIS8vTyH/+7+rOrePadOmjaCpqSnExsaKafn5+UL//v0FAMLJkyfF9OJc3woif0+6u7sL+fn5YnpAQIAAQJDJZML9+/fF9OfPnws6OjrCoEGDxLRbt24JEolEcHFxUXi9Hj16JBgbGwtWVlZCbm6uQt6OHTuKaYLw7r0tkUiU3q8eHh6Cqamp8ODBA4V6X7x4UdDU1BTmz58vphWnvYvTzu3atVO69guCICQkJAgAFOoQGRmp9D75cJuhoaHC+WRlZQktWrQQtLS0FNKtrKxUvodUHaMkfU3efyZMmKCQ/s0334h/01JSUsT02NhYAYDw3XffKeRXdX2RX9O+//57MS0uLk4AIHTt2lXhfXL16lVBQ0OjwL8NRenfql4LVWlyhb1OH/5Nys3NFWrVqiVUqVJFePHihZienJwsWFpalsp18b///a8AQFi1apXStk+BU6Hos+Pj4wOpVCr+Lv+mxtnZWSEyl0qlaNmypfjNOQCcOHECz549g7e3N5KTk/Hy5UvxR/4t1/Hjx8X8BgYG4v/fvn2LpKQkvH37Fh07dsTNmzeRmpqqVL9vvvlG4feOHTsCgEI9Xrx4AQAK3yR/TO3atREbGyt+S75r1y588803cHJyQuPGjRETE6O0j66uLk6cOKHy58svv1R5nP379yM5ORkjRowQ03r06AFTU9MCp0PdunULpqamMDU1Re3atTFq1ChUrVoVhw4dQsOGDQs9L/lxduzYIaYJgoCdO3eiYcOGcHR0FNNL8nqUVGhoKCQSCUaMGKHQT16+fAkPDw+kpaXh3LlzRS6vcuXKRZpOUVjfkEgk0NPTA/BumF/eh+V97P0h+65du6J69eoK7Qq8a2ctLS0MHToUwLu2Dg0NRdu2bVGzZk2F8zQwMECrVq0U3hNyPj4+Ku+peP81Sk9PR1JSEjQ1NeHs7KxQv5iYGDx+/BgjR45EpUqVxHRDQ0OMHz9eqdydO3eiXr16aN68uUIds7Oz4ebmhjNnziAjI0NFixaNOtcVOSMjI0yYMEEhbcKECTAyMlKYrieVSsVRuNzcXLx+/RovX75E586dASi+jvJvs1etWqU0WljU0UNVnj9/jj/++AMeHh5o3LixmC6RSDB79mwAUDnFsCjXt4+ZNGmSwoirvK09PDxQq1YtMd3U1BR169ZVKPvQoUMQBAEzZsxQeL3Mzc3h7e2Ne/fuiVNA5HmnTp2qcG+No6Mj3NzcFOqUkpKCw4cPw8PDA7q6ugp9zNraGra2tirfBx9T0nYuLUOHDoWFhYX4u1QqxTfffIPc3FyVI4NlbcqUKQq/y1/74cOHw8jISExv3LgxjIyMlPqV/PqSn5+PlJQUvHz5Ek2aNIGxsbHC++bw4cMAgMmTJyu8Txo1aiRO01WlNPq3OmJiYvDgwQN4e3srjPYbGxuX2nVRPqqj7vS+kuJUKPrsfDiELf9Qomp4s1KlSgpzz2/evAkAGDVqVIHlP3v2TPz/8+fPMWfOHBw6dEjlmzA5OVnhYqiqfvI38fv1kP9RFYq55Ju1tTU2bNiADRs24MmTJzhz5gxCQkIQHh4Od3d3XL9+XeEDqaampvhh5UOq5iMD76ZBmZqawsLCQuH+iC5duiAsLAwvX75Umt5kbW0tPm9BPi/Z1ta2SOckDx5CQ0OxdOlSaGho4PTp00hMTMSKFSsU8pbk9SipmzdvQhAE1KtXr8A87/eVjxEEoUjT1z7WN/bs2YPVq1fjzz//VJpz+/r1a/H/8uBhzZo1uHPnDuzt7fHmzRvs378fXbp0EadMvHjxAklJSTh+/DhMTU1VHlPVB1h7e3uVef/++2/Mnj0bx44dQ3JysspzA4CEhAQAEKdQvU9V2s2bN5GRkVFgHYF30/7e/2BaHOpcV94v4/0PuwCgo6OD2rVrK92rsmnTJvz000+4fv068vPzFba9/zr+9ddfMDMzU5rioi55+zs4OChtq1+/PjQ0NJTqDBTt+vYxxW3re/fuFane8rT4+Hg4OTmJ9Vf1Hm7QoIFCoHD79m3k5+dj69at2Lp1a5HqXRQlbefSIp+q9L4GDRoAQJketyDqvs9OnToFf39/REdHIzMzU2Hb+++bj11fjhw5UqT6laR/q+NjffZDJbkuyv+2FHU6dWljYEGfnYJWdSnKai/yN9TKlSvF+eUfMjc3F/N26dIFN2/exOTJk+Hk5ARjY2NoamoiKCgIu3btUvpAUFg93v+gKL8IvHr16qN1LoiZmRn69++P/v37Y+jQodi1axd+/fVXpXnfxZGQkIDIyEgIglDgB8edO3cqfetkYGBQYABTFMOHD8eUKVNw6tQpdO7cGTt27ICmpqbCuZT09XhfQRfSD2/alx9PIpHgyJEjBb6mqj4sFOT169eFXvzlCusb+/fvx8CBA9GyZUsEBASgVq1a0NXVRV5eHrp166Z0/sOHD8eaNWuwY8cOLF68GPv370d6errCaJS8X3bu3Bnffvttkc9H1WhFeno62rZtizdv3mDKlClo1KgRZDIZNDQ0sGzZMpw6darI5X9IEAQ0atSo0GV7i9K+BVHnulJca9aswbRp09ClSxdMmjQJ5ubmkEqlePToEUaOHPnRflyeinJ9K2kZpVF2ScmPMWzYMIX3x/vko4VlqTjXqH/icdV57S9evIguXbrA1tYWy5cvh42NjfispUGDBpXK+6Ys+mBhH+DVbd+SXBflf1vUuV6qg4EF/avY2dkBKNoH4atXryI2Nhbz5s1TenLyzz//rFY95B9IS2t4tVWrVti1axcePXqkVjlBQUHiCjQmJiZK2+fMmYNt27YpBRbqGjJkCKZPn44dO3bAxcUFe/fuhZubG8zMzMQ8pfF6yEdzPryhWdU3d3Z2djh69CgsLS1VfutXHImJicjNzf3otDCg8L4REhICXV1dREZGKnywv3XrlsqymjRpgiZNmmDnzp1YtGgRduzYId7YLWdqagoTExOkpqaqFRwCwG+//YbHjx9j27ZtSg/2e3/NdwDiiiny1YDepyrNzs4OL168QMeOHdWaAlSW4uPjkZ2drTBqkZWVhfj4eIVvIENCQmBtbY0jR44onMvRo0eVyrS3t8ehQ4fw7NmzQkctivvto/wb4uvXryttu3XrFvLz80v0DX1Zk9fp+vXrSjcM37hxQyGP/N9bt24VmFfO1tYWEokE2dnZar8P3lfcdq5cubLKaa2qrlFFec3lo/Tv+7Cd5MdV9WVGSY9bFnbt2oW8vDwcOXJEYYTjzZs3CqMVgOL15cN+rOr6oq7C2uT9vzsf+rB93++zH/qwzwIluy7KZyIU5e9RWfg8r95EJdS1a1dUq1YNy5cvV/kmz8jIQFpaGoD/fXPx4TcVcXFxas+JNTU1hYODg7icZVFERUWpnEOen58vzpVVNVRaVPn5+QgODkajRo0wZswYeHl5Kf0MHjwY165dw8WLF0t8HFVMTU3RvXt37N+/H6GhoUhNTVX61rA0Xg/5KMzJkycV0levXq2UV34PyqxZs5SWhASKNw1K/jq3a9fuo3kL6xuampqQSCQK38wJgoDFixcXWN6IESNw79497Nq1C6dOncLAgQMV1mDX0NDA0KFDceHChQKX0S3qXNyCXqPjx48rLdno5OQEMzMzBAcHK3woSE9Px08//aRU9vDhw/H06dMCv5krzutRVlJTU7Fp0yaFtE2bNiE1NRV9+vQR0+Sv4/vtlJubi+XLlyuVKb8XZsaMGUrfyL6/v3wFmqKOglarVg1t2rRBeHg44uLiFMpctmwZAKBv375FKutT8vDwgEQiwcqVKxWmAj558gRBQUGwsrJCs2bNFPKuWbNG4T18+fJlpWtAlSpV0KNHD+zfv1/le08QBPH+p+Iobjvb29sjLS0NFy5cENPy8/Oxdu1apbKL8pqHhobi4cOH4u/Z2dlYu3YtNDU14e7urnDcW7duKXw5lZWVhY0bN5bouGWhoOvL0qVLld4bvXr1AgAEBAQobLt27ZrSqmulobA2sbGxgZaWllKf++OPP5T6WvPmzWFhYYGgoCCFFR1TU1NL7bp4/vx5aGlpwcXF5eMnVgY4YkH/KgYGBtixYwf69OmDunXrYtSoUbC1tUVycjJu3bqF/fv348CBA2jfvj3q168PBwcHrFixAm/fvkXdunVx584dbN68GY0aNVL5rVJx9O/fH4sWLcKTJ08UvpkvyKpVq3D27Fn06tULjo6OMDY2xtOnT7Fv3z7ExMSgQ4cOaj3w5vjx43jw4AFGjx5dYB5PT08sWLAAW7duRYsWLUp8LFVGjBiB//u//8O0adNgbGys8EEMQKm8HoMHD8asWbMwbtw43Lp1C5UrV8bRo0dVLsnbokULLFiwAAsWLEDTpk3Rv39/mJub48mTJ+KTS7Ozs4t0br/++iuqVq0qrjv/MQX1DS8vL+zbtw8dO3bE8OHDkZOTg4MHDxa6dPDQoUMxY8YMTJgwAfn5+SqneSxZsgRnz57FgAEDMGDAALRq1QpSqRT37t3Dr7/+iubNm6tcg/1Drq6uqFGjBqZNm4bExERYWFjgypUrCAkJQaNGjXDt2jUxr5aWFlatWoWhQ4eiZcuWGD16NLS0tBAcHIwqVaogISFB4VvAyZMn48SJE5g+fTpOnTqFjh07wsjICPfv38dvv/0mjuSUpzp16mDhwoWIi4tD8+bNERMTg23btqFevXoKywd7eXlh5syZ6N69O/r164fU1FTs2rVLvKH7ff3798fAgQOxY8cO/PXXX/Dw8EClSpVw584dHDt2TPyw2qJFC2hoaGDJkiV4/fo1DAwMYGNjA2dn5wLrGxAQgHbt2uGLL74Ql0E9fPgwjh07hiFDhhT4zJzyVLduXUyfPh0rVqxA27ZtMXDgQHG52fT0dISGhoofQOvVqwdfX19s2LABHTt2hKenJ54/f44NGzagSZMmSuv8//jjj3B1dUXbtm0xfPhwNGvWDPn5+YiPj8ehQ4cwfPhw8dkFxVGcdh43bhxWr16Nvn37YvLkyZBKpdi7d6/KKTMNGjSATCbDpk2boK+vDxMTE1SrVk284Rh4FzA4Oztj/PjxkMlk2LVrFy5evIi5c+cqzLufOHEifvnlF3Tu3Bnjx49HdnY2QkJCVE55LElfKw19+/bF2rVr0aNHD4wbNw5SqRQnTpzA1atXle77c3BwwLhx4xAYGIjOnTujb9++ePHiBTZu3IhmzZohJiamVEdeqlSpAltbW/zyyy+oU6cOqlevDgMDA/Tq1QuGhoYYOXIkfv75ZwwePBjt27fHX3/9haCgIDRu3BixsbFiOZqamli7di0GDBiAli1bYuzYseJzpKpUqYL79+8rHLe410VBEHD06FF069atxMvhqq2MV50iKrLClrjDB0uFyhW0vOi1a9eEoUOHCubm5oK2trZQrVo1oXXr1oK/v7+QlJQk5ktMTBS8vLyEqlWrCnp6ekKLFi2E/fv3q72UqSC8Wx5RS0tL5ZJvqpabPXfunDB16lTByclJqFatmqClpSUYGxsLrVq1ElavXi1kZmYq5G/Xrp1gYGCgsj6C8L+lH+VLaXp5eQkAhKtXrxa4jyAIgr29vWBsbCwue2plZSU4ODgUuk9RZGVlCZUrVxYACGPGjFGZpzivh6o0QRCE8+fPC23atBF0dHSEKlWqCGPHjhVev35dYB86fPiw0KVLF6FSpUqCVCoVLCwshG7dugk//vijQr6ClptNT08XDAwMBD8/vyK3RWF9IzAwUKhfv76go6Mj1KhRQxg7dqyQlJRUYP0FQRDc3d0FAIKdnV2Bx3zz5o3g7+8vNGzYUNDV1RUMDQ2FevXqCWPGjBHOnz+vdJ4FLTUZGxsrdO3aVTAxMREMDQ2Fdu3aCadPny7w/bFnzx6hUaNGglQqFWrVqiUsWLBA2L9/v9LyuYLwbonagIAAwcnJSdDX1xf09fUFW1tbYciQIcKxY8cKPLfC6l5a1xX5cp0xMTFChw4dBH19fcHExEQYNmyY8PTpU4W8ubm5wtKlS4U6deoIUqlUsLS0FKZPny7cuHFD5ZKVeXl5woYNG4RmzZoJenp6gqGhodCoUSNhwYIFCvmCg4OF+vXrC9ra2oX2h/dduXJF6N27t9i/69WrJ3z//fcKy7MWdM4fa6cPFfSeLGypzoKWXw0MDBSaNm0q6OjoCDKZTOjcubNw+vRppXx5eXnC4sWLBUtLS0EqlQoODg7Czp07C6zLixcvBD8/P8HOzk7Q0dERjI2NhYYNGwqTJk1SWBK7uEuuFrWdBUEQIiIihCZNmghSqVQwMzMTZsyYIdy6dUtlG0VERAjNmjUTdHR0BADi8qLvL3EaEBAg2NraClKpVLC1tRXWrVunso7BwcGCvb29oK2tLVhbWwvff/+98Ntvv6lcKrW4fa2g/lPYUqyqlsA9cOCA4OjoKOjr6wtVqlQRBg4cKNy7d09l3tzcXGHBggVCrVq1BKlUKjRq1EjYvXu3MG3aNAGA8OzZs4/WTxCU+3dB/TU6Olpo06aNoK+vLwBQ6LdpaWnC6NGjhcqVKwt6enqCq6urcPbs2QKPu2/fPrEPWFhYCHPmzBGOHz+usq2Kc12MiooSAAiHDx9Wea6fgkQQPsFdU0QV1Pjx43H8+HHcvn1b4dvKkSNHIioqSuXTROnzFBwcDG9vbyQkJCg8OTcgIACzZ88WV/cpqoL6RkWwevVq+Pn54dy5c2jVqlV5V6dIrK2tYW1trfBUb6LyEhUVhQ4dOiAoKKhIT2CvSHr16oVTp04hNTW1TBZn+Jz17dsXDx48wMWLF8vtXhneY0FUhvz9/ZGUlISgoKDyrgqVgYyMDCxfvhzTp08vVlABVIy+kZ2drXT/Snp6OjZu3IgqVaooPMOEiKg4VN2TePXqVRw5cgQdO3ascEHFn3/+iUOHDmH16tXlFlQAvMeCqExVq1YNKSkp5V0NKiN6enp48uRJifatCH0jPj4e3bt3x6BBg2BjY4MnT55g+/btSEhIwI8//qj0TAgioqLavn07duzYgZ49e8LU1BS3bt1CYGAgpFIp/P39y7t6n5z8nqHyxsCCiIjKhKmpKVq1aoXQ0FA8f/4cWlpaaNSoEZYvX44BAwaUd/WI6B/M0dERBw4cwA8//IBXr15BJpOhY8eOmD9/vrhyGH16vMeCiIiIiIjUxnssiIiIiIhIbQwsiIiIiIhIbQwsiIiIiIhIbQwsiIiIiIhIbQwsiIiIiIhIbQwsiIiIiIhIbQwsiIiIiIhIbQwsiIiIiIhIbQwsiIiIiIhIbf8PZQydr8zWKj0AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}